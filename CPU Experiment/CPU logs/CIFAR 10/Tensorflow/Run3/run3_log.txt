Training
Iteration 0: with minibatch training loss = 2.73 and accuracy of 0.13
Iteration 128: with minibatch training loss = 1.82 and accuracy of 0.34
Iteration 256: with minibatch training loss = 1.84 and accuracy of 0.3
Epoch 1, Overall loss = 1.92 and accuracy of 0.306
Iteration 384: with minibatch training loss = 1.67 and accuracy of 0.45
Iteration 512: with minibatch training loss = 1.68 and accuracy of 0.37
Iteration 640: with minibatch training loss = 1.65 and accuracy of 0.44
Epoch 2, Overall loss = 1.64 and accuracy of 0.415
Iteration 768: with minibatch training loss = 1.56 and accuracy of 0.44
Iteration 896: with minibatch training loss = 1.44 and accuracy of 0.49
Iteration 1024: with minibatch training loss = 1.32 and accuracy of 0.59
Epoch 3, Overall loss = 1.48 and accuracy of 0.483
Iteration 1152: with minibatch training loss = 1.47 and accuracy of 0.48
Iteration 1280: with minibatch training loss = 1.28 and accuracy of 0.55
Iteration 1408: with minibatch training loss = 1.47 and accuracy of 0.48
Epoch 4, Overall loss = 1.36 and accuracy of 0.527
Iteration 1536: with minibatch training loss = 1.46 and accuracy of 0.5
Iteration 1664: with minibatch training loss = 1.22 and accuracy of 0.58
Iteration 1792: with minibatch training loss = 1.13 and accuracy of 0.63
Epoch 5, Overall loss = 1.28 and accuracy of 0.558
Iteration 1920: with minibatch training loss = 1.23 and accuracy of 0.6
Iteration 2048: with minibatch training loss = 1.22 and accuracy of 0.52
Iteration 2176: with minibatch training loss = 1.18 and accuracy of 0.6
Epoch 6, Overall loss = 1.21 and accuracy of 0.581
Iteration 2304: with minibatch training loss = 1.19 and accuracy of 0.58
Iteration 2432: with minibatch training loss = 1.17 and accuracy of 0.6
Iteration 2560: with minibatch training loss = 1.27 and accuracy of 0.53
Epoch 7, Overall loss = 1.16 and accuracy of 0.601
Iteration 2688: with minibatch training loss = 1.15 and accuracy of 0.59
Iteration 2816: with minibatch training loss = 1.04 and accuracy of 0.66
Iteration 2944: with minibatch training loss = 1.13 and accuracy of 0.55
Epoch 8, Overall loss = 1.11 and accuracy of 0.617
Iteration 3072: with minibatch training loss = 1.2 and accuracy of 0.64
Iteration 3200: with minibatch training loss = 1.08 and accuracy of 0.63
Iteration 3328: with minibatch training loss = 1.08 and accuracy of 0.61
Epoch 9, Overall loss = 1.07 and accuracy of 0.631
Iteration 3456: with minibatch training loss = 0.893 and accuracy of 0.68
Iteration 3584: with minibatch training loss = 1.04 and accuracy of 0.62
Iteration 3712: with minibatch training loss = 0.934 and accuracy of 0.69
Epoch 10, Overall loss = 1.03 and accuracy of 0.645
Iteration 3840: with minibatch training loss = 0.926 and accuracy of 0.73
Iteration 3968: with minibatch training loss = 0.961 and accuracy of 0.64
Iteration 4096: with minibatch training loss = 0.97 and accuracy of 0.66
Epoch 11, Overall loss = 1.01 and accuracy of 0.655
Iteration 4224: with minibatch training loss = 0.997 and accuracy of 0.67
Iteration 4352: with minibatch training loss = 1.16 and accuracy of 0.59
Iteration 4480: with minibatch training loss = 0.723 and accuracy of 0.79
Epoch 12, Overall loss = 0.979 and accuracy of 0.667
Iteration 4608: with minibatch training loss = 1.11 and accuracy of 0.62
Iteration 4736: with minibatch training loss = 0.968 and accuracy of 0.63
Iteration 4864: with minibatch training loss = 0.963 and accuracy of 0.66
Epoch 13, Overall loss = 0.957 and accuracy of 0.671
Iteration 4992: with minibatch training loss = 0.932 and accuracy of 0.7
Iteration 5120: with minibatch training loss = 0.846 and accuracy of 0.72
Iteration 5248: with minibatch training loss = 0.938 and accuracy of 0.71
Epoch 14, Overall loss = 0.933 and accuracy of 0.681
Iteration 5376: with minibatch training loss = 0.798 and accuracy of 0.75
Iteration 5504: with minibatch training loss = 0.951 and accuracy of 0.69
Iteration 5632: with minibatch training loss = 0.882 and accuracy of 0.69
Epoch 15, Overall loss = 0.908 and accuracy of 0.689
Iteration 5760: with minibatch training loss = 0.866 and accuracy of 0.7
Iteration 5888: with minibatch training loss = 1.01 and accuracy of 0.65
Iteration 6016: with minibatch training loss = 0.788 and accuracy of 0.73
Epoch 16, Overall loss = 0.89 and accuracy of 0.695
Iteration 6144: with minibatch training loss = 0.938 and accuracy of 0.69
Iteration 6272: with minibatch training loss = 0.977 and accuracy of 0.65
Iteration 6400: with minibatch training loss = 0.827 and accuracy of 0.71
Epoch 17, Overall loss = 0.875 and accuracy of 0.7
Iteration 6528: with minibatch training loss = 0.787 and accuracy of 0.75
Iteration 6656: with minibatch training loss = 0.835 and accuracy of 0.7
Iteration 6784: with minibatch training loss = 0.896 and accuracy of 0.7
Epoch 18, Overall loss = 0.855 and accuracy of 0.709
Iteration 6912: with minibatch training loss = 0.788 and accuracy of 0.77
Iteration 7040: with minibatch training loss = 0.843 and accuracy of 0.75
Iteration 7168: with minibatch training loss = 0.75 and accuracy of 0.73
Epoch 19, Overall loss = 0.846 and accuracy of 0.712
Iteration 7296: with minibatch training loss = 0.753 and accuracy of 0.74
Iteration 7424: with minibatch training loss = 0.791 and accuracy of 0.73
Iteration 7552: with minibatch training loss = 0.771 and accuracy of 0.76
Epoch 20, Overall loss = 0.826 and accuracy of 0.716
Iteration 7680: with minibatch training loss = 0.794 and accuracy of 0.77
Iteration 7808: with minibatch training loss = 0.883 and accuracy of 0.69
Iteration 7936: with minibatch training loss = 0.821 and accuracy of 0.75
Epoch 21, Overall loss = 0.812 and accuracy of 0.722
Iteration 8064: with minibatch training loss = 0.793 and accuracy of 0.72
Iteration 8192: with minibatch training loss = 0.811 and accuracy of 0.69
Iteration 8320: with minibatch training loss = 0.782 and accuracy of 0.7
Epoch 22, Overall loss = 0.798 and accuracy of 0.724
Iteration 8448: with minibatch training loss = 0.756 and accuracy of 0.74
Iteration 8576: with minibatch training loss = 0.764 and accuracy of 0.73
Iteration 8704: with minibatch training loss = 0.598 and accuracy of 0.8
Epoch 23, Overall loss = 0.784 and accuracy of 0.73
Iteration 8832: with minibatch training loss = 0.725 and accuracy of 0.75
Iteration 8960: with minibatch training loss = 0.706 and accuracy of 0.73
Iteration 9088: with minibatch training loss = 0.687 and accuracy of 0.75
Epoch 24, Overall loss = 0.772 and accuracy of 0.737
Iteration 9216: with minibatch training loss = 0.743 and accuracy of 0.73
Iteration 9344: with minibatch training loss = 0.714 and accuracy of 0.73
Iteration 9472: with minibatch training loss = 0.829 and accuracy of 0.73
Epoch 25, Overall loss = 0.76 and accuracy of 0.74
Iteration 9600: with minibatch training loss = 0.668 and accuracy of 0.8
Iteration 9728: with minibatch training loss = 0.771 and accuracy of 0.75
Iteration 9856: with minibatch training loss = 0.809 and accuracy of 0.72
Epoch 26, Overall loss = 0.751 and accuracy of 0.743
Iteration 9984: with minibatch training loss = 0.627 and accuracy of 0.77
Iteration 10112: with minibatch training loss = 0.652 and accuracy of 0.78
Iteration 10240: with minibatch training loss = 0.834 and accuracy of 0.7
Epoch 27, Overall loss = 0.738 and accuracy of 0.747
Iteration 10368: with minibatch training loss = 0.754 and accuracy of 0.76
Iteration 10496: with minibatch training loss = 0.729 and accuracy of 0.77
Iteration 10624: with minibatch training loss = 0.754 and accuracy of 0.77
Epoch 28, Overall loss = 0.723 and accuracy of 0.75
Iteration 10752: with minibatch training loss = 0.833 and accuracy of 0.74
Iteration 10880: with minibatch training loss = 0.851 and accuracy of 0.71
Iteration 11008: with minibatch training loss = 0.558 and accuracy of 0.8
Epoch 29, Overall loss = 0.719 and accuracy of 0.754
Iteration 11136: with minibatch training loss = 0.777 and accuracy of 0.73
Iteration 11264: with minibatch training loss = 0.735 and accuracy of 0.73
Iteration 11392: with minibatch training loss = 0.775 and accuracy of 0.74
Epoch 30, Overall loss = 0.709 and accuracy of 0.758
Iteration 11520: with minibatch training loss = 0.747 and accuracy of 0.74
Iteration 11648: with minibatch training loss = 0.72 and accuracy of 0.75
Iteration 11776: with minibatch training loss = 0.67 and accuracy of 0.8
Epoch 31, Overall loss = 0.697 and accuracy of 0.761
Iteration 11904: with minibatch training loss = 0.72 and accuracy of 0.77
Iteration 12032: with minibatch training loss = 0.727 and accuracy of 0.72
Iteration 12160: with minibatch training loss = 0.867 and accuracy of 0.73
Epoch 32, Overall loss = 0.684 and accuracy of 0.766
Iteration 12288: with minibatch training loss = 0.592 and accuracy of 0.77
Iteration 12416: with minibatch training loss = 0.671 and accuracy of 0.71
Iteration 12544: with minibatch training loss = 0.595 and accuracy of 0.79
Epoch 33, Overall loss = 0.677 and accuracy of 0.77
Iteration 12672: with minibatch training loss = 0.489 and accuracy of 0.88
Iteration 12800: with minibatch training loss = 0.817 and accuracy of 0.72
Iteration 12928: with minibatch training loss = 0.76 and accuracy of 0.73
Epoch 34, Overall loss = 0.671 and accuracy of 0.768
Iteration 13056: with minibatch training loss = 0.534 and accuracy of 0.83
Iteration 13184: with minibatch training loss = 0.682 and accuracy of 0.73
Iteration 13312: with minibatch training loss = 0.602 and accuracy of 0.81
Epoch 35, Overall loss = 0.663 and accuracy of 0.773
Iteration 13440: with minibatch training loss = 0.555 and accuracy of 0.8
Iteration 13568: with minibatch training loss = 0.631 and accuracy of 0.75
Iteration 13696: with minibatch training loss = 0.636 and accuracy of 0.77
Epoch 36, Overall loss = 0.65 and accuracy of 0.779
Iteration 13824: with minibatch training loss = 0.743 and accuracy of 0.74
Iteration 13952: with minibatch training loss = 0.625 and accuracy of 0.76
Iteration 14080: with minibatch training loss = 0.623 and accuracy of 0.79
Epoch 37, Overall loss = 0.643 and accuracy of 0.78
Iteration 14208: with minibatch training loss = 0.63 and accuracy of 0.78
Iteration 14336: with minibatch training loss = 0.657 and accuracy of 0.79
Iteration 14464: with minibatch training loss = 0.68 and accuracy of 0.76
Epoch 38, Overall loss = 0.635 and accuracy of 0.782
Iteration 14592: with minibatch training loss = 0.485 and accuracy of 0.84
Iteration 14720: with minibatch training loss = 0.73 and accuracy of 0.76
Iteration 14848: with minibatch training loss = 0.675 and accuracy of 0.79
Epoch 39, Overall loss = 0.63 and accuracy of 0.785
Iteration 14976: with minibatch training loss = 0.576 and accuracy of 0.82
Iteration 15104: with minibatch training loss = 0.648 and accuracy of 0.74
Iteration 15232: with minibatch training loss = 0.659 and accuracy of 0.8
Epoch 40, Overall loss = 0.617 and accuracy of 0.788
Iteration 15360: with minibatch training loss = 0.639 and accuracy of 0.78
Iteration 15488: with minibatch training loss = 0.472 and accuracy of 0.8
Iteration 15616: with minibatch training loss = 0.505 and accuracy of 0.84
Epoch 41, Overall loss = 0.616 and accuracy of 0.788
Iteration 15744: with minibatch training loss = 0.599 and accuracy of 0.78
Iteration 15872: with minibatch training loss = 0.528 and accuracy of 0.82
Iteration 16000: with minibatch training loss = 0.505 and accuracy of 0.84
Epoch 42, Overall loss = 0.609 and accuracy of 0.79
Iteration 16128: with minibatch training loss = 0.605 and accuracy of 0.8
Iteration 16256: with minibatch training loss = 0.528 and accuracy of 0.83
Iteration 16384: with minibatch training loss = 0.443 and accuracy of 0.84
Epoch 43, Overall loss = 0.601 and accuracy of 0.794
Iteration 16512: with minibatch training loss = 0.59 and accuracy of 0.79
Iteration 16640: with minibatch training loss = 0.697 and accuracy of 0.73
Iteration 16768: with minibatch training loss = 0.558 and accuracy of 0.8
Epoch 44, Overall loss = 0.593 and accuracy of 0.797
Iteration 16896: with minibatch training loss = 0.583 and accuracy of 0.79
Iteration 17024: with minibatch training loss = 0.552 and accuracy of 0.85
Iteration 17152: with minibatch training loss = 0.644 and accuracy of 0.77
Epoch 45, Overall loss = 0.584 and accuracy of 0.8
Iteration 17280: with minibatch training loss = 0.543 and accuracy of 0.8
Iteration 17408: with minibatch training loss = 0.553 and accuracy of 0.81
Iteration 17536: with minibatch training loss = 0.551 and accuracy of 0.82
Epoch 46, Overall loss = 0.583 and accuracy of 0.801
Iteration 17664: with minibatch training loss = 0.435 and accuracy of 0.85
Iteration 17792: with minibatch training loss = 0.481 and accuracy of 0.84
Iteration 17920: with minibatch training loss = 0.525 and accuracy of 0.85
Epoch 47, Overall loss = 0.573 and accuracy of 0.803
Iteration 18048: with minibatch training loss = 0.596 and accuracy of 0.8
Iteration 18176: with minibatch training loss = 0.557 and accuracy of 0.79
Iteration 18304: with minibatch training loss = 0.457 and accuracy of 0.82
Epoch 48, Overall loss = 0.565 and accuracy of 0.804
Iteration 18432: with minibatch training loss = 0.649 and accuracy of 0.8
Iteration 18560: with minibatch training loss = 0.499 and accuracy of 0.82
Iteration 18688: with minibatch training loss = 0.673 and accuracy of 0.74
Epoch 49, Overall loss = 0.563 and accuracy of 0.808
Iteration 18816: with minibatch training loss = 0.647 and accuracy of 0.79
Iteration 18944: with minibatch training loss = 0.608 and accuracy of 0.77
Iteration 19072: with minibatch training loss = 0.565 and accuracy of 0.82
Epoch 50, Overall loss = 0.555 and accuracy of 0.808
Iteration 19200: with minibatch training loss = 0.473 and accuracy of 0.84
Iteration 19328: with minibatch training loss = 0.626 and accuracy of 0.81
Iteration 19456: with minibatch training loss = 0.553 and accuracy of 0.8
Epoch 51, Overall loss = 0.546 and accuracy of 0.812
Iteration 19584: with minibatch training loss = 0.581 and accuracy of 0.8
Iteration 19712: with minibatch training loss = 0.537 and accuracy of 0.81
Iteration 19840: with minibatch training loss = 0.655 and accuracy of 0.75
Epoch 52, Overall loss = 0.54 and accuracy of 0.814
Iteration 19968: with minibatch training loss = 0.436 and accuracy of 0.85
Iteration 20096: with minibatch training loss = 0.552 and accuracy of 0.81
Iteration 20224: with minibatch training loss = 0.446 and accuracy of 0.87
Epoch 53, Overall loss = 0.537 and accuracy of 0.815
Iteration 20352: with minibatch training loss = 0.619 and accuracy of 0.76
Iteration 20480: with minibatch training loss = 0.492 and accuracy of 0.84
Iteration 20608: with minibatch training loss = 0.637 and accuracy of 0.76
Epoch 54, Overall loss = 0.527 and accuracy of 0.819
Iteration 20736: with minibatch training loss = 0.546 and accuracy of 0.83
Iteration 20864: with minibatch training loss = 0.53 and accuracy of 0.78
Iteration 20992: with minibatch training loss = 0.593 and accuracy of 0.8
Epoch 55, Overall loss = 0.527 and accuracy of 0.818
Iteration 21120: with minibatch training loss = 0.492 and accuracy of 0.84
Iteration 21248: with minibatch training loss = 0.625 and accuracy of 0.79
Iteration 21376: with minibatch training loss = 0.478 and accuracy of 0.83
Epoch 56, Overall loss = 0.521 and accuracy of 0.821
Iteration 21504: with minibatch training loss = 0.61 and accuracy of 0.84
Iteration 21632: with minibatch training loss = 0.552 and accuracy of 0.8
Iteration 21760: with minibatch training loss = 0.514 and accuracy of 0.78
Epoch 57, Overall loss = 0.512 and accuracy of 0.824
Iteration 21888: with minibatch training loss = 0.573 and accuracy of 0.8
Iteration 22016: with minibatch training loss = 0.48 and accuracy of 0.87
Iteration 22144: with minibatch training loss = 0.433 and accuracy of 0.84
Epoch 58, Overall loss = 0.509 and accuracy of 0.824
Iteration 22272: with minibatch training loss = 0.496 and accuracy of 0.84
Iteration 22400: with minibatch training loss = 0.539 and accuracy of 0.83
Iteration 22528: with minibatch training loss = 0.475 and accuracy of 0.84
Epoch 59, Overall loss = 0.503 and accuracy of 0.826
Iteration 22656: with minibatch training loss = 0.538 and accuracy of 0.82
Iteration 22784: with minibatch training loss = 0.504 and accuracy of 0.83
Iteration 22912: with minibatch training loss = 0.539 and accuracy of 0.8
Epoch 60, Overall loss = 0.502 and accuracy of 0.829
Iteration 23040: with minibatch training loss = 0.422 and accuracy of 0.85
Iteration 23168: with minibatch training loss = 0.436 and accuracy of 0.88
Iteration 23296: with minibatch training loss = 0.531 and accuracy of 0.85
Epoch 61, Overall loss = 0.493 and accuracy of 0.83
Iteration 23424: with minibatch training loss = 0.446 and accuracy of 0.88
Iteration 23552: with minibatch training loss = 0.392 and accuracy of 0.86
Iteration 23680: with minibatch training loss = 0.495 and accuracy of 0.83
Epoch 62, Overall loss = 0.489 and accuracy of 0.831
Iteration 23808: with minibatch training loss = 0.498 and accuracy of 0.83
Iteration 23936: with minibatch training loss = 0.461 and accuracy of 0.8
Iteration 24064: with minibatch training loss = 0.609 and accuracy of 0.8
Epoch 63, Overall loss = 0.483 and accuracy of 0.834
Iteration 24192: with minibatch training loss = 0.559 and accuracy of 0.8
Iteration 24320: with minibatch training loss = 0.333 and accuracy of 0.88
Iteration 24448: with minibatch training loss = 0.413 and accuracy of 0.87
Epoch 64, Overall loss = 0.482 and accuracy of 0.832
Iteration 24576: with minibatch training loss = 0.414 and accuracy of 0.87
Iteration 24704: with minibatch training loss = 0.516 and accuracy of 0.8
Iteration 24832: with minibatch training loss = 0.451 and accuracy of 0.84
Epoch 65, Overall loss = 0.478 and accuracy of 0.833
Iteration 24960: with minibatch training loss = 0.348 and accuracy of 0.86
Iteration 25088: with minibatch training loss = 0.468 and accuracy of 0.8
Iteration 25216: with minibatch training loss = 0.426 and accuracy of 0.85
Epoch 66, Overall loss = 0.47 and accuracy of 0.84
Iteration 25344: with minibatch training loss = 0.509 and accuracy of 0.82
Iteration 25472: with minibatch training loss = 0.487 and accuracy of 0.79
Iteration 25600: with minibatch training loss = 0.467 and accuracy of 0.82
Epoch 67, Overall loss = 0.463 and accuracy of 0.84
Iteration 25728: with minibatch training loss = 0.453 and accuracy of 0.82
Iteration 25856: with minibatch training loss = 0.437 and accuracy of 0.84
Iteration 25984: with minibatch training loss = 0.529 and accuracy of 0.8
Epoch 68, Overall loss = 0.457 and accuracy of 0.842
Iteration 26112: with minibatch training loss = 0.414 and accuracy of 0.87
Iteration 26240: with minibatch training loss = 0.44 and accuracy of 0.83
Iteration 26368: with minibatch training loss = 0.361 and accuracy of 0.88
Epoch 69, Overall loss = 0.457 and accuracy of 0.841
Iteration 26496: with minibatch training loss = 0.479 and accuracy of 0.85
Iteration 26624: with minibatch training loss = 0.471 and accuracy of 0.87
Iteration 26752: with minibatch training loss = 0.385 and accuracy of 0.88
Epoch 70, Overall loss = 0.452 and accuracy of 0.845
Iteration 26880: with minibatch training loss = 0.343 and accuracy of 0.9
Iteration 27008: with minibatch training loss = 0.507 and accuracy of 0.84
Iteration 27136: with minibatch training loss = 0.572 and accuracy of 0.79
Epoch 71, Overall loss = 0.447 and accuracy of 0.846
Iteration 27264: with minibatch training loss = 0.396 and accuracy of 0.84
Iteration 27392: with minibatch training loss = 0.369 and accuracy of 0.86
Iteration 27520: with minibatch training loss = 0.515 and accuracy of 0.81
Epoch 72, Overall loss = 0.443 and accuracy of 0.847
Iteration 27648: with minibatch training loss = 0.497 and accuracy of 0.81
Iteration 27776: with minibatch training loss = 0.5 and accuracy of 0.77
Iteration 27904: with minibatch training loss = 0.365 and accuracy of 0.89
Epoch 73, Overall loss = 0.443 and accuracy of 0.847
Iteration 28032: with minibatch training loss = 0.376 and accuracy of 0.88
Iteration 28160: with minibatch training loss = 0.475 and accuracy of 0.84
Iteration 28288: with minibatch training loss = 0.463 and accuracy of 0.84
Epoch 74, Overall loss = 0.433 and accuracy of 0.853
Iteration 28416: with minibatch training loss = 0.372 and accuracy of 0.84
Iteration 28544: with minibatch training loss = 0.485 and accuracy of 0.84
Iteration 28672: with minibatch training loss = 0.557 and accuracy of 0.81
Epoch 75, Overall loss = 0.429 and accuracy of 0.851
Iteration 28800: with minibatch training loss = 0.381 and accuracy of 0.88
Iteration 28928: with minibatch training loss = 0.411 and accuracy of 0.86
Iteration 29056: with minibatch training loss = 0.394 and accuracy of 0.89
Epoch 76, Overall loss = 0.429 and accuracy of 0.853
Iteration 29184: with minibatch training loss = 0.328 and accuracy of 0.91
Iteration 29312: with minibatch training loss = 0.487 and accuracy of 0.84
Iteration 29440: with minibatch training loss = 0.508 and accuracy of 0.84
Epoch 77, Overall loss = 0.425 and accuracy of 0.853
Iteration 29568: with minibatch training loss = 0.571 and accuracy of 0.79
Iteration 29696: with minibatch training loss = 0.476 and accuracy of 0.81
Iteration 29824: with minibatch training loss = 0.445 and accuracy of 0.85
Epoch 78, Overall loss = 0.411 and accuracy of 0.858
Iteration 29952: with minibatch training loss = 0.455 and accuracy of 0.83
Iteration 30080: with minibatch training loss = 0.376 and accuracy of 0.87
Iteration 30208: with minibatch training loss = 0.461 and accuracy of 0.83
Epoch 79, Overall loss = 0.412 and accuracy of 0.858
Iteration 30336: with minibatch training loss = 0.458 and accuracy of 0.84
Iteration 30464: with minibatch training loss = 0.354 and accuracy of 0.88
Iteration 30592: with minibatch training loss = 0.466 and accuracy of 0.84
Epoch 80, Overall loss = 0.41 and accuracy of 0.857
Iteration 30720: with minibatch training loss = 0.398 and accuracy of 0.84
Iteration 30848: with minibatch training loss = 0.379 and accuracy of 0.84
Iteration 30976: with minibatch training loss = 0.363 and accuracy of 0.88
Epoch 81, Overall loss = 0.411 and accuracy of 0.856
Iteration 31104: with minibatch training loss = 0.439 and accuracy of 0.83
Iteration 31232: with minibatch training loss = 0.39 and accuracy of 0.88
Iteration 31360: with minibatch training loss = 0.36 and accuracy of 0.86
Epoch 82, Overall loss = 0.402 and accuracy of 0.862
Iteration 31488: with minibatch training loss = 0.37 and accuracy of 0.87
Iteration 31616: with minibatch training loss = 0.363 and accuracy of 0.87
Iteration 31744: with minibatch training loss = 0.351 and accuracy of 0.88
Epoch 83, Overall loss = 0.396 and accuracy of 0.864
Iteration 31872: with minibatch training loss = 0.449 and accuracy of 0.84
Iteration 32000: with minibatch training loss = 0.344 and accuracy of 0.88
Iteration 32128: with minibatch training loss = 0.31 and accuracy of 0.86
Epoch 84, Overall loss = 0.395 and accuracy of 0.864
Iteration 32256: with minibatch training loss = 0.389 and accuracy of 0.88
Iteration 32384: with minibatch training loss = 0.373 and accuracy of 0.85
Iteration 32512: with minibatch training loss = 0.359 and accuracy of 0.88
Epoch 85, Overall loss = 0.392 and accuracy of 0.865
Iteration 32640: with minibatch training loss = 0.331 and accuracy of 0.86
Iteration 32768: with minibatch training loss = 0.29 and accuracy of 0.9
Iteration 32896: with minibatch training loss = 0.339 and accuracy of 0.88
Epoch 86, Overall loss = 0.389 and accuracy of 0.866
Iteration 33024: with minibatch training loss = 0.369 and accuracy of 0.88
Iteration 33152: with minibatch training loss = 0.379 and accuracy of 0.86
Iteration 33280: with minibatch training loss = 0.33 and accuracy of 0.89
Epoch 87, Overall loss = 0.384 and accuracy of 0.866
Iteration 33408: with minibatch training loss = 0.408 and accuracy of 0.85
Iteration 33536: with minibatch training loss = 0.326 and accuracy of 0.91
Iteration 33664: with minibatch training loss = 0.307 and accuracy of 0.91
Epoch 88, Overall loss = 0.377 and accuracy of 0.87
Iteration 33792: with minibatch training loss = 0.513 and accuracy of 0.84
Iteration 33920: with minibatch training loss = 0.366 and accuracy of 0.9
Iteration 34048: with minibatch training loss = 0.392 and accuracy of 0.88
Epoch 89, Overall loss = 0.379 and accuracy of 0.869
Iteration 34176: with minibatch training loss = 0.383 and accuracy of 0.91
Iteration 34304: with minibatch training loss = 0.465 and accuracy of 0.84
Iteration 34432: with minibatch training loss = 0.311 and accuracy of 0.89
Epoch 90, Overall loss = 0.372 and accuracy of 0.87
Iteration 34560: with minibatch training loss = 0.401 and accuracy of 0.85
Iteration 34688: with minibatch training loss = 0.361 and accuracy of 0.84
Iteration 34816: with minibatch training loss = 0.33 and accuracy of 0.9
Epoch 91, Overall loss = 0.372 and accuracy of 0.872
Iteration 34944: with minibatch training loss = 0.295 and accuracy of 0.89
Iteration 35072: with minibatch training loss = 0.334 and accuracy of 0.88
Iteration 35200: with minibatch training loss = 0.461 and accuracy of 0.82
Epoch 92, Overall loss = 0.367 and accuracy of 0.874
Iteration 35328: with minibatch training loss = 0.474 and accuracy of 0.87
Iteration 35456: with minibatch training loss = 0.379 and accuracy of 0.88
Iteration 35584: with minibatch training loss = 0.398 and accuracy of 0.84
Epoch 93, Overall loss = 0.363 and accuracy of 0.874
Iteration 35712: with minibatch training loss = 0.368 and accuracy of 0.86
Iteration 35840: with minibatch training loss = 0.441 and accuracy of 0.85
Iteration 35968: with minibatch training loss = 0.32 and accuracy of 0.89
Epoch 94, Overall loss = 0.362 and accuracy of 0.874
Iteration 36096: with minibatch training loss = 0.322 and accuracy of 0.9
Iteration 36224: with minibatch training loss = 0.344 and accuracy of 0.88
Iteration 36352: with minibatch training loss = 0.332 and accuracy of 0.87
Epoch 95, Overall loss = 0.357 and accuracy of 0.877
Iteration 36480: with minibatch training loss = 0.311 and accuracy of 0.9
Iteration 36608: with minibatch training loss = 0.22 and accuracy of 0.91
Iteration 36736: with minibatch training loss = 0.297 and accuracy of 0.9
Epoch 96, Overall loss = 0.355 and accuracy of 0.878
Iteration 36864: with minibatch training loss = 0.28 and accuracy of 0.91
Iteration 36992: with minibatch training loss = 0.282 and accuracy of 0.91
Iteration 37120: with minibatch training loss = 0.412 and accuracy of 0.83
Epoch 97, Overall loss = 0.355 and accuracy of 0.877
Iteration 37248: with minibatch training loss = 0.35 and accuracy of 0.88
Iteration 37376: with minibatch training loss = 0.299 and accuracy of 0.89
Iteration 37504: with minibatch training loss = 0.329 and accuracy of 0.88
Epoch 98, Overall loss = 0.35 and accuracy of 0.879
Iteration 37632: with minibatch training loss = 0.373 and accuracy of 0.87
Iteration 37760: with minibatch training loss = 0.332 and accuracy of 0.9
Iteration 37888: with minibatch training loss = 0.236 and accuracy of 0.91
Epoch 99, Overall loss = 0.343 and accuracy of 0.882
Iteration 38016: with minibatch training loss = 0.42 and accuracy of 0.85
Iteration 38144: with minibatch training loss = 0.363 and accuracy of 0.89
Iteration 38272: with minibatch training loss = 0.297 and accuracy of 0.91
Epoch 100, Overall loss = 0.342 and accuracy of 0.883
Validation
Epoch 1, Overall loss = 0.534 and accuracy of 0.827
Training
Epoch 1, Overall loss = 0.156 and accuracy of 0.964
Validation
Epoch 1, Overall loss = 0.534 and accuracy of 0.827
Test
Epoch 1, Overall loss = 0.565 and accuracy of 0.816
5:35:28.261155