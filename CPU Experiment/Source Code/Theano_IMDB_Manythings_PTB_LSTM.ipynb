{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import LoggerYN as YN\n",
    "from data import load_imdb \n",
    "import chainer as ch\n",
    "from data import load_ptb, load_ptb_vocab\n",
    "import tensorflow as tf\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import InputLayer, EmbeddingLayer, LSTMLayer, Gate, SliceLayer, DenseLayer, flatten\n",
    "from lasagne import init\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_imdb(n_epochs):\n",
    "\n",
    "    # save np.load\n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    def imdb_dataset(train, vocabulary_size, seq_len):\n",
    "        x, y = load_imdb(train, vocabulary_size, seq_len)\n",
    "        x = np.array([np.pad(xi, (0, seq_len - len(xi)), 'constant') for xi in x], dtype=np.int32)\n",
    "        mask = (x != 0).astype(np.bool)\n",
    "        y = np.array(y, dtype=np.int32)\n",
    "        return x, mask, y\n",
    "\n",
    "    def imdb_generator(data, batch_size, shuffle=False):\n",
    "        x, mask, y = data\n",
    "        indices = np.arange(len(y))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for idx in range(0, len(y) - batch_size + 1, batch_size):\n",
    "            batch_indices = indices[idx:idx + batch_size]\n",
    "            yield x[batch_indices], mask[batch_indices], y[batch_indices]\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    def imdb_lstm(input_var, mask_var, seq_len, vocabulary_size, embedding_size, hidden_size):\n",
    "        l_input = lasagne.layers.InputLayer(shape=(None, seq_len), input_var=input_var)\n",
    "        l_mask = lasagne.layers.InputLayer(shape=(None, seq_len), input_var=mask_var)\n",
    "        l_embed = EmbeddingLayer(l_input, vocabulary_size, embedding_size,\n",
    "                                 W=init.Uniform(1.0))\n",
    "        l_lstm = LSTMLayer(l_embed, hidden_size, mask_input=l_mask,\n",
    "                           ingate=Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal()),\n",
    "                           forgetgate=Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal(), b=init.Constant(1.0)),\n",
    "                           cell=Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal(), W_cell=None, \n",
    "                                     nonlinearity=lasagne.nonlinearities.tanh), \n",
    "                           outgate=Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal())) \n",
    "        l_lstm_last = SliceLayer(l_lstm, -1, axis=1)\n",
    "        l_out = DenseLayer(l_lstm_last, num_units=1, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "        return lasagne.layers.flatten(l_out, outdim=1)\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    def imdb_train(model, data_gen, func, epoch, print_every=50):\n",
    "        losses = []\n",
    "        for i, (inputs, mask, labels) in enumerate(data_gen()):\n",
    "            loss = func(inputs, mask, labels)\n",
    "\n",
    "            losses.append(loss)\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('[%d, %5d] train loss: %.3f' % (epoch, i + 1, np.mean(losses)))\n",
    "                sys.stdout.flush()\n",
    "                losses = []\n",
    "\n",
    "\n",
    "    def imdb_test(model, data_gen, func, epoch):\n",
    "        losses = []\n",
    "        correct, total = 0, 0\n",
    "        for i, (inputs, mask, labels) in enumerate(data_gen()):\n",
    "            loss, preds = func(inputs, mask, labels)\n",
    "\n",
    "            losses.append(loss)\n",
    "            correct += preds.sum().item()\n",
    "            total += preds.shape[0]\n",
    "        print('[%d] test loss: %.3f accuracy: %.3f' % (epoch, np.mean(losses), correct / total * 100))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def imdb_run(n_epochs, vocabulary_size, seq_len, batch_size, embedding_size, hidden_size):\n",
    "        np.random.seed(1)\n",
    "\n",
    "\n",
    "        train_data = imdb_dataset(True, vocabulary_size, seq_len)\n",
    "        test_data = imdb_dataset(False, vocabulary_size, seq_len)\n",
    "        train_gen = lambda: imdb_generator(train_data, batch_size, shuffle=True)\n",
    "        test_gen = lambda: imdb_generator(test_data, batch_size, shuffle=False)\n",
    "\n",
    "        input_var = T.imatrix('inputs')\n",
    "        mask_var = T.matrix('mask')\n",
    "        labels_var = T.ivector('labels')\n",
    "        variables = [input_var, mask_var, labels_var]\n",
    "        model = imdb_lstm(input_var, mask_var, seq_len, vocabulary_size, embedding_size, hidden_size)\n",
    "\n",
    "        preds = lasagne.layers.get_output(model)\n",
    "        loss = lasagne.objectives.binary_crossentropy(preds, labels_var).mean()\n",
    "        correct = T.eq(preds >= .5, labels_var)\n",
    "\n",
    "        params = lasagne.layers.get_all_params(model, trainable=True)\n",
    "        updates = lasagne.updates.adam(loss, params)\n",
    "        train_func = theano.function(variables, loss, updates=updates)\n",
    "        test_func = theano.function(variables, [loss, correct])\n",
    "\n",
    "\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"Theano\",\"IMDB\")   \n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "        epoch=1\n",
    "\n",
    "        while (time_consumed <= 86400 and epoch <= n_epochs):\n",
    "            imdb_train(model, train_gen, train_func, epoch)\n",
    "            imdb_test(model, test_gen, test_func, epoch)\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Time since beginning \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "            sys.stdout.flush()\n",
    "            epoch += 1\n",
    "        end = time.time()\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"\\n\\nTotal Time Consumed \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "        sys.stdout.flush()\n",
    "        f = open('Theano_CPU_IMDB_LSTM_model', 'wb')\n",
    "        cPickle.dump(model, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        f.close()    \n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    imdb_run(n_epochs, vocabulary_size=5000, seq_len=500, batch_size=64, embedding_size=32, hidden_size=100) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ptb(n_epochs):\n",
    "    \n",
    "    class PtbIterator:\n",
    "\n",
    "        def __init__(self, train, batch_size, seq_len, skip_step=5):\n",
    "            self.data = load_ptb(train)\n",
    "            self.batch_size = batch_size\n",
    "            self.seq_len = seq_len\n",
    "            self.skip_step = skip_step\n",
    "            self.reset()\n",
    "\n",
    "        def __iter__(self):\n",
    "            self.reset()\n",
    "            return self\n",
    "\n",
    "        def __next__(self):\n",
    "            x = np.zeros((self.batch_size, self.seq_len), dtype=np.int32)\n",
    "            y = np.zeros((self.batch_size, self.seq_len), dtype=np.int32)\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                if self.cur_idx + self.seq_len >= len(self.data):\n",
    "                    raise StopIteration\n",
    "                x[i, :] = self.data[self.cur_idx:self.cur_idx+self.seq_len]\n",
    "                y[i, :] = self.data[self.cur_idx+1:self.cur_idx+self.seq_len+1]\n",
    "                self.cur_idx += self.skip_step\n",
    "\n",
    "            return x, y.ravel()\n",
    "\n",
    "        def reset(self):\n",
    "            self.cur_idx = 0\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    def ptb_lstm(input_var, vocabulary_size, hidden_size, seq_len, num_layers, dropout, batch_size):\n",
    "        l_input = L.InputLayer(shape=(batch_size, seq_len), input_var=input_var)\n",
    "        l_embed = L.EmbeddingLayer(l_input, vocabulary_size, hidden_size,\n",
    "                                   W=init.Uniform(1.0))\n",
    "        l_lstms = []\n",
    "        for i in range(num_layers):\n",
    "            l_lstm = L.LSTMLayer(l_embed if i == 0 else l_lstms[-1], hidden_size,\n",
    "                                 ingate=L.Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal()),\n",
    "                                 forgetgate=L.Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal(), \n",
    "                                                   b=init.Constant(1.0)),\n",
    "                                 cell=L.Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal(), W_cell=None, \n",
    "                                             nonlinearity=lasagne.nonlinearities.tanh), \n",
    "                                 outgate=L.Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal()))\n",
    "            l_lstms.append(l_lstm)\n",
    "        l_drop = L.DropoutLayer(l_lstms[-1], dropout)\n",
    "        l_out = L.DenseLayer(l_drop, num_units=vocabulary_size, num_leading_axes=2)\n",
    "        l_out = L.ReshapeLayer(l_out, (l_out.output_shape[0] * l_out.output_shape[1], l_out.output_shape[2]))\n",
    "        l_out = L.NonlinearityLayer(l_out, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "        return l_out\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    def ptb_train(model, data_iter, func, epoch, print_every=50):\n",
    "        losses = []\n",
    "        for i, (inputs, labels) in enumerate(data_iter):\n",
    "            loss = func(inputs, labels)\n",
    "\n",
    "            losses.append(loss)\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('[%d, %5d] train loss: %.3f' % (epoch, i + 1, np.mean(losses)))\n",
    "                losses = []\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def ptb_test(model, data_iter, func, epoch):\n",
    "        losses = []\n",
    "        t_acc=0\n",
    "        count=0\n",
    "        for inputs, labels in data_iter:\n",
    "            loss,acc = func(inputs, labels)\n",
    "            losses.append(loss)\n",
    "            t_acc += acc\n",
    "            count += 1\n",
    "\n",
    "        loss = np.mean(losses)\n",
    "        perplexity = np.exp(loss)\n",
    "        print('[%d] test loss: %.3f accuracy: %.3f' % (epoch, np.mean(losses), t_acc/count))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def ptb_run(n_epochs,hidden_size, batch_size, seq_len, dropout, num_layers):\n",
    "        np.random.seed(1)\n",
    "\n",
    "        ptb_vocab = load_ptb_vocab()\n",
    "        vocabulary_size = len(ptb_vocab)\n",
    "\n",
    "        train_iter = PtbIterator(train=True, batch_size=batch_size, seq_len=seq_len)\n",
    "        test_iter = PtbIterator(train=True, batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "        input_var = T.imatrix('inputs')\n",
    "        labels_var = T.ivector('labels')\n",
    "        variables = [input_var, labels_var]\n",
    "        model = ptb_lstm(input_var, vocabulary_size, hidden_size, seq_len, num_layers, dropout, batch_size)\n",
    "\n",
    "        preds = lasagne.layers.get_output(model)\n",
    "        loss = lasagne.objectives.categorical_crossentropy(preds, labels_var).mean()\n",
    "        test_acc = T.mean(T.eq(T.argmax(preds, axis=1), labels_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "        params = lasagne.layers.get_all_params(model, trainable=True)\n",
    "        updates = lasagne.updates.adadelta(loss, params)\n",
    "        train_func = theano.function(variables, loss, updates=updates)\n",
    "        test_func = theano.function(variables, [loss,test_acc])\n",
    "\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"Theano_CPU\", \"PTB\")   \n",
    "\n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "        epoch=1\n",
    "\n",
    "        while(epoch <= n_epochs and time_consumed <= 86400 ):\n",
    "            ptb_train(model, train_iter, train_func, epoch)\n",
    "            ptb_test(model, test_iter, test_func, epoch)\n",
    "            epoch += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Time since beginning \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "        f = open('Theano_CPU_IMDB_LSTM_model', 'wb')\n",
    "        cPickle.dump(model, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "        end = time.time()\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"\\n\\nTotal Time Consumed \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    ptb_run(n_epochs, hidden_size = 200, batch_size = 20, seq_len = 30, dropout = 0.5, num_layers = 2)\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_manythings(n_epochs):\n",
    "\n",
    "    # Converts the unicode file to ascii\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "    def preprocess_sentence(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.rstrip().strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "\n",
    "    # 1. Remove the accents\n",
    "    # 2. Clean the sentences\n",
    "    # 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "    def create_dataset(path):\n",
    "        lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "        word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines]\n",
    "\n",
    "        return word_pairs\n",
    "\n",
    "        # This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "    # (e.g., 5 -> \"dad\") for each language,\n",
    "    class LanguageIndex():\n",
    "        def __init__(self, lang):\n",
    "            self.lang = lang\n",
    "            self.word2idx = {}\n",
    "            self.idx2word = {}\n",
    "            self.vocab = set()\n",
    "\n",
    "            self.create_index()\n",
    "\n",
    "        def create_index(self):\n",
    "            for phrase in self.lang:\n",
    "                self.vocab.update(phrase.split(' '))\n",
    "\n",
    "            self.vocab = sorted(self.vocab)\n",
    "\n",
    "            self.word2idx['<pad>'] = 0\n",
    "            for index, word in enumerate(self.vocab):\n",
    "                self.word2idx[word] = index + 1\n",
    "\n",
    "            for word, index in self.word2idx.items():\n",
    "                self.idx2word[index] = word\n",
    "    def max_length(tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "    def load_dataset(path):\n",
    "        # creating cleaned input, output pairs\n",
    "        pairs = create_dataset(path)\n",
    "\n",
    "        # index language using the class defined above    \n",
    "        inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "        targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "\n",
    "        # Vectorize the input and target languages\n",
    "\n",
    "        # Spanish sentences\n",
    "        input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # English sentences\n",
    "        target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # Calculate max_length of input and output tensor\n",
    "        # Here, we'll set those to the longest sentence in the dataset\n",
    "        max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "        # Padding the input and output tensor to the maximum length\n",
    "        input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                     maxlen=max_length_inp,\n",
    "                                                                     padding='post')\n",
    "\n",
    "        target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                      maxlen=max_length_tar, \n",
    "                                                                      padding='post')\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n",
    "    def acc_function(real,pred):\n",
    "        print(\"real.shape\",real.shape)\n",
    "        print(\"pred.shape\", pred.shape)\n",
    "        return\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    def create_db(path_to_file):\n",
    "        input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file)\n",
    "        # Creating training and validation sets using an 80-20 split\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2,random_state=42)\n",
    "        vocab_inp_size = len(inp_lang.word2idx)\n",
    "        vocab_tar_size = len(targ_lang.word2idx)\n",
    "        return input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ\n",
    "\n",
    "\n",
    "    # In[7]:\n",
    "\n",
    "\n",
    "    def model_run(vocab_size_enc,vocab_size_dec,embedding_dim,units,batch_sz,input_train,target_train,input_val, target_val,max_length_inp,max_length_targ,epochs):\n",
    "        theano.config.exception_verbosity = 'high'\n",
    "        theano.config.optimizer = 'fast_compile'\n",
    "        input_enc = T.imatrix(\"input_enc\")\n",
    "        l_in = lasagne.layers.InputLayer(shape=(None,max_length_inp), input_var=input_enc)\n",
    "        l_embed_enc = EmbeddingLayer(l_in, vocab_size_enc, output_size=embedding_dim,W=init.Uniform(1.0))\n",
    "        l_lstm_enc = LSTMLayer(l_embed_enc,num_units=units,ingate=Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal()),\n",
    "                           forgetgate=Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal(), b=init.Constant(1.0)),\n",
    "                           cell=Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal(), W_cell=None, \n",
    "                                     nonlinearity=lasagne.nonlinearities.tanh), \n",
    "                           outgate=Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal()))\n",
    "        hidden_enc = SliceLayer(l_lstm_enc,indices = -1, axis=1)\n",
    "        target_in = T.imatrix(\"traget_in\")\n",
    "        dec_in = lasagne.layers.InputLayer(shape=(None,max_length_targ), input_var=target_in)\n",
    "        l_embed_dec =  EmbeddingLayer(dec_in, vocab_size_dec, output_size = embedding_dim)\n",
    "        l_lstm_dec = LSTMLayer(l_embed_dec, num_units = units, hid_init = hidden_enc)\n",
    "        dec_out = DenseLayer(l_lstm_dec,num_units = vocab_size_dec,num_leading_axes=-1)\n",
    "        output = lasagne.layers.get_output(dec_out)\n",
    "        output = T.reshape(output,(-1,vocab_size_dec))\n",
    "        output = T.nnet.softmax(output)\n",
    "        output_reshaped = T.flatten(target_in)\n",
    "\n",
    "        loss = lasagne.objectives.categorical_crossentropy(output,output_reshaped).mean()\n",
    "\n",
    "        test_acc = T.mean(T.eq(T.argmax(output, axis=1), output_reshaped),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "        #accuracy = lasagne.objectives.categorical_accuracy(output,output_reshaped).mean()    \n",
    "        params = lasagne.layers.get_all_params(dec_out, trainable=True)\n",
    "        update = adam(loss,params,learning_rate=0.0001)\n",
    "\n",
    "        train = theano.function([input_enc,target_in],loss, updates=update ,allow_input_downcast=True)\n",
    "        test =theano.function([input_enc,target_in],[loss,test_acc],allow_input_downcast=True)\n",
    "\n",
    "\n",
    "        instances = len(input_train)\n",
    "        val_instances = len(input_val)\n",
    "        batches_val = int(val_instances/batch_sz)\n",
    "        batches = int(instances/batch_sz)\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"Theano_GPU\",\"Manythings\")\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "        epoch=1\n",
    "\n",
    "        while (epoch <= epochs):\n",
    "\n",
    "            loss_val = 0\n",
    "            t_acc=0\n",
    "            count=0\n",
    "            print(\"\\n\\nEpoch : \",epoch)\n",
    "\n",
    "            for batch in range(batches):\n",
    "                start = batch*batch_sz\n",
    "                if(start + batch_sz > instances):\n",
    "                    break\n",
    "                inpu = input_train[start:start+batch_sz]\n",
    "                targ = target_train[start:start +batch_sz]\n",
    "                loss_train = train(inpu,targ)\n",
    "                inpu = np.argmax(inpu,1) \n",
    "                targ = np.argmax(targ,1)\n",
    "\n",
    "                if(batch % 300 == 0):\n",
    "                    print(\"Batch {} Loss {}\".format(batch,loss_train))\n",
    "            loss_val = 0\n",
    "            t_acc=0\n",
    "            count=0\n",
    "            for batch in range(batches_val):\n",
    "                start = batch*batch_sz\n",
    "\n",
    "                if(start + batch_sz > instances):\n",
    "                    break\n",
    "                inpu = input_val[start:start+batch_sz]\n",
    "                targ = target_val[start:start +batch_sz]\n",
    "                loss,acc = test(inpu,targ)\n",
    "                loss_val +=loss\n",
    "                inpu = np.argmax(inpu,1) \n",
    "                targ = np.argmax(targ,1) \n",
    "                t_acc += acc\n",
    "                count += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Validation Perplexity :\",np.power(2,loss_val/batch)) \n",
    "            print(\"Validation        Acc :\",t_acc/count)\n",
    "            epoch += 1\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"\\nTotal Time Consumed \", str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "\n",
    "    def run(BATCH_SIZE, embedding_dim, units, epochs):\n",
    "        path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
    "        path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ = create_db(path_to_file)\n",
    "        BUFFER_SIZE = len(input_tensor_train)\n",
    "        N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "        train_samples = len(input_tensor_train)\n",
    "        val_samples = len(input_tensor_val)\n",
    "        input_tensor_train = np.array(input_tensor_train,dtype='int')\n",
    "        target_tensor_train = np.array(target_tensor_train,dtype='int')\n",
    "        model_run(vocab_inp_size,vocab_tar_size,embedding_dim,units,BATCH_SIZE,input_tensor_train,target_tensor_train,input_tensor_val,target_tensor_val,max_length_inp, max_length_targ,epochs)\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    run(BATCH_SIZE = 128, embedding_dim = 256, units = 256, epochs = 100)\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unicodedata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1a0ede1f9b5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#run_imdb(n_epochs=50)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#run_ptb(n_epochs=50)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrun_manythings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-72618ebdfa71>\u001b[0m in \u001b[0;36mrun_manythings\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-72618ebdfa71>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(BATCH_SIZE, embedding_dim, units, epochs)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mpath_to_zip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spa-eng.zip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http://download.tensorflow.org/data/spa-eng.zip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[0mpath_to_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_zip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/spa-eng/spa.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[0minput_tensor_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_inp_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_tar_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length_targ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m         \u001b[0mBUFFER_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mN_BATCH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBUFFER_SIZE\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-72618ebdfa71>\u001b[0m in \u001b[0;36mcreate_db\u001b[1;34m(path_to_file)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp_lang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg_lang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length_inp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length_targ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[1;31m# Creating training and validation sets using an 80-20 split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0minput_tensor_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-72618ebdfa71>\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# creating cleaned input, output pairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mpairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# index language using the class defined above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-72618ebdfa71>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mword_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mword_pairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-72618ebdfa71>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mword_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mword_pairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-72618ebdfa71>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mword_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mword_pairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-72618ebdfa71>\u001b[0m in \u001b[0;36mpreprocess_sentence\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0municode_to_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# creating a space between a word and the punctuation following it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-72618ebdfa71>\u001b[0m in \u001b[0;36municode_to_ascii\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Converts the unicode file to ascii\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0municode_to_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         return ''.join(c for c in unicodedata.normalize('NFD', s)\n\u001b[0m\u001b[0;32m      6\u001b[0m             if unicodedata.category(c) != 'Mn')\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unicodedata' is not defined"
     ]
    }
   ],
   "source": [
    "#run_imdb(n_epochs=50)\n",
    "#run_ptb(n_epochs=50)\n",
    "run_manythings(n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
