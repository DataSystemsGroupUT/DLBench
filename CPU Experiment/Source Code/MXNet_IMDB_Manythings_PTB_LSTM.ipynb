{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon as gl\n",
    "from mxnet import initializer as init\n",
    "from mxnet.gluon.data import ArrayDataset, DataLoader\n",
	
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from data import load_imdb\n",
    "import LoggerYN as YN\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "from data import load_ptb, load_ptb_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_imdb(n_epochs):\n",
    "    \n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "    \n",
    "    class ImdbDataset(ArrayDataset):\n",
    "\n",
    "        def __init__(self, train, vocabulary_size, seq_len):\n",
    "            x, y = load_imdb(train, vocabulary_size, seq_len)\n",
    "\n",
    "            lens = mx.nd.array([len(xi) for xi in x])\n",
    "            x = mx.nd.array([np.pad(xi, (0, seq_len - len(xi)), 'constant') for xi in x], dtype=int)\n",
    "            y = mx.nd.array(y)\n",
    "            super().__init__(x, lens, y)\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    class ImdbLstm(gl.Block):\n",
    "\n",
    "        def __init__(self, vocabulary_size, embedding_dim, hidden_size):\n",
    "            super().__init__()\n",
    "            with self.name_scope():\n",
    "                self.embed = gl.nn.Embedding(input_dim=vocabulary_size, output_dim=embedding_dim,\n",
    "                                             weight_initializer=init.Uniform(1.0))\n",
    "                self.lstm = gl.rnn.LSTMCell(input_size=embedding_dim, hidden_size=hidden_size,\n",
    "                                            i2h_weight_initializer=init.Xavier(),\n",
    "                                            h2h_weight_initializer=init.Orthogonal())\n",
    "                self.fc = gl.nn.Dense(in_units=hidden_size, units=1,\n",
    "                                      weight_initializer=init.Xavier())\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x, lens = inputs\n",
    "            x = self.embed(x)\n",
    "            o, (h, c) = self.lstm.unroll(x.shape[1], x, valid_length=lens)\n",
    "            f = self.fc(h)\n",
    "            return f.reshape(-1)\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    def imdb_train(model, data_loader, criterion, trainer, epoch, print_every=100):\n",
    "        losses = []\n",
    "        for i, (seqs, lens, labels) in enumerate(data_loader):\n",
    "            with mx.autograd.record():\n",
    "                outputs = model((seqs, lens))\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "            trainer.step(batch_size=labels.shape[0])\n",
    "\n",
    "            losses.append(loss.mean().asscalar())\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('[%d, %5d] train loss: %.3f' % (epoch, i + 1, np.mean(losses)))\n",
    "                losses = []\n",
    "\n",
    "\n",
    "    def imdb_test(model, data_loader, criterion, epoch):\n",
    "        accuracy = mx.metric.Accuracy()\n",
    "        losses = []\n",
    "        for seqs, lens, labels in data_loader:\n",
    "            outputs = model((seqs, lens))\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            losses.append(mx.nd.mean(loss).asscalar())\n",
    "            preds = (outputs >= 0.0)\n",
    "            accuracy.update(labels, preds)\n",
    "\n",
    "        print('[%d] test loss: %.3f' % (epoch, np.mean(losses)))\n",
    "        print('[%d] accuracy: %.3f' % (epoch, accuracy.get()[1] * 100))\n",
    "\n",
    "\n",
    "    def imdb_run(n_epochs,vocabulary_size, seq_len, batch_size, embedding_size, hidden_size):\n",
    "        mx.random.seed(1)\n",
    "        np.random.seed(1)\n",
    "        random.seed(1)\n",
    "\n",
    "\n",
    "        train_dataset = ImdbDataset(train=True, vocabulary_size=vocabulary_size, seq_len=seq_len)\n",
    "        test_dataset = ImdbDataset(train=False, vocabulary_size=vocabulary_size, seq_len=seq_len)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        model = ImdbLstm(vocabulary_size, embedding_size, hidden_size)\n",
    "        model.initialize()\n",
    "        criterion = gl.loss.SigmoidBCELoss()\n",
    "        trainer = gl.Trainer(model.collect_params(), mx.optimizer.Adam())\n",
    "\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"MXNet_CPU\",\"IMDB\")\n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "        epoch=1\n",
    "\n",
    "        while (time_consumed <= 86400 and epoch <= n_epochs):\n",
    "            imdb_train(model, train_loader, criterion, trainer, epoch)\n",
    "            imdb_test(model, test_loader, criterion, epoch)\n",
    "            epoch += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Time since beginning \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "            sys.stdout.flush()\n",
    "            if epoch % 10== 0:\n",
    "                model.save_parameters('MXNet_CPU_IMDB_LSTM_model')\n",
    "\n",
    "        end = time.time()\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"\\n\\nTotal Time Consumed \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    imdb_run(n_epochs,vocabulary_size = 5000, seq_len = 500, batch_size = 64, embedding_size = 32, hidden_size = 100)\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_manythings(n_epochs):\n",
    "    \n",
    "    # Converts the unicode file to ascii\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "    def preprocess_sentence(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.rstrip().strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "\n",
    "    # 1. Remove the accents\n",
    "    # 2. Clean the sentences\n",
    "    # 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "    def create_dataset(path):\n",
    "        lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "        word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:100000]]\n",
    "\n",
    "        return word_pairs\n",
    "\n",
    "        # This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "    # (e.g., 5 -> \"dad\") for each language,\n",
    "    class LanguageIndex():\n",
    "        def __init__(self, lang):\n",
    "            self.lang = lang\n",
    "            self.word2idx = {}\n",
    "            self.idx2word = {}\n",
    "            self.vocab = set()\n",
    "\n",
    "            self.create_index()\n",
    "\n",
    "        def create_index(self):\n",
    "            for phrase in self.lang:\n",
    "                self.vocab.update(phrase.split(' '))\n",
    "\n",
    "            self.vocab = sorted(self.vocab)\n",
    "\n",
    "            self.word2idx['<pad>'] = 0\n",
    "            for index, word in enumerate(self.vocab):\n",
    "                self.word2idx[word] = index + 1\n",
    "\n",
    "            for word, index in self.word2idx.items():\n",
    "                self.idx2word[index] = word\n",
    "    def max_length(tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "    def load_dataset(path):\n",
    "        # creating cleaned input, output pairs\n",
    "        pairs = create_dataset(path)\n",
    "\n",
    "        # index language using the class defined above    \n",
    "        inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "        targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "\n",
    "        # Vectorize the input and target languages\n",
    "\n",
    "        # Spanish sentences\n",
    "        input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # English sentences\n",
    "        target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # Calculate max_length of input and output tensor\n",
    "        # Here, we'll set those to the longest sentence in the dataset\n",
    "        max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "        # Padding the input and output tensor to the maximum length\n",
    "        input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                     maxlen=max_length_inp,\n",
    "                                                                     padding='post')\n",
    "\n",
    "        target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                      maxlen=max_length_tar, \n",
    "                                                                      padding='post')\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    def create_db(path_to_file):\n",
    "        input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file)\n",
    "        # Creating training and validation sets using an 80-20 split\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2,random_state=42)\n",
    "        vocab_inp_size = len(inp_lang.word2idx)\n",
    "        vocab_tar_size = len(targ_lang.word2idx)\n",
    "        return input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    class Encoder(gl.Block):\n",
    "        def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "            super().__init__()\n",
    "            self.batch_sz = batch_sz\n",
    "            self.enc_units = enc_units\n",
    "            self.embedding = gl.nn.Embedding(input_dim=vocab_size, output_dim=embedding_dim, weight_initializer=init.Uniform(1.0))\n",
    "            self.LSTM = gl.rnn.LSTM(hidden_size=enc_units,layout ='NTC',input_size = embedding_dim, i2h_weight_initializer=init.Xavier(),\n",
    "                                            h2h_weight_initializer=init.Orthogonal())\n",
    "\n",
    "        def forward(self, x, hidden):\n",
    "            x = self.embedding(x)\n",
    "            output, state = self.LSTM(x,hidden)\n",
    "            return output, state\n",
    "        def init_hidden(self):\n",
    "            return mx.nd.zeros((1,self.batch_sz,self.enc_units))\n",
    "\n",
    "\n",
    "    class Decoder(gl.Block):\n",
    "        def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "            super().__init__()\n",
    "            self.batch_sz = batch_sz\n",
    "            self.dec_units = dec_units\n",
    "            self.embedding = gl.nn.Embedding(input_dim=vocab_size, output_dim=embedding_dim, weight_initializer=init.Uniform(1.0))\n",
    "            self.LSTM = gl.rnn.LSTM(hidden_size=dec_units,layout='NTC',input_size = embedding_dim,i2h_weight_initializer=init.Xavier(),\n",
    "                                            h2h_weight_initializer=init.Orthogonal())\n",
    "            self.fc = gl.nn.Dense(units=vocab_size,flatten =False,weight_initializer=init.Xavier())\n",
    "\n",
    "\n",
    "        def forward(self, x, hidden, enc_output):\n",
    "            x = self.embedding(x)\n",
    "            output, state = self.LSTM(x,hidden)\n",
    "            x = self.fc(output)\n",
    "            return x, state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    class Encap(gl.Block):\n",
    "        def __init__(self, encoder,decoder):\n",
    "            super().__init__()\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "\n",
    "        def forward(self, inp,targ, hidden, BATCH_SIZE,vocab):\n",
    "            loss = 0\n",
    "            enc_output, enc_hidden = self.encoder(inp, [hidden,hidden])\n",
    "\n",
    "            dec_hidden = enc_hidden\n",
    "\n",
    "            dec_input = targ[:,:-1]\n",
    "\n",
    "            predictions, dec_hidden = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss = loss_function(targ[:,1:], predictions,vocab)\n",
    "            accuracy=acc_function(targ[:,1:], predictions,vocab)\n",
    "\n",
    "            return [loss,accuracy]\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    def loss_function(real, pred,vocab):\n",
    "        loss_c = gl.loss.SoftmaxCrossEntropyLoss(axis=-1, sparse_label=True, from_logits=False, weight=None)\n",
    "        real = real.reshape(-1)\n",
    "        pred = pred.reshape(-1,vocab)\n",
    "        return (loss_c(pred,real)).mean()\n",
    "\n",
    "\n",
    "    # In[7]:\n",
    "\n",
    "\n",
    "    def acc_function(real, pred,vocab):\n",
    "        pred = pred.reshape(-1,vocab)\n",
    "        real = real.reshape(-1)\n",
    "        pred = np.argmax(pred,1)\n",
    "        acc = mx.metric.Accuracy()\n",
    "        acc.update(pred, real)\n",
    "        return ((acc.get())[1])\n",
    "\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "\n",
    "    def train(data_iter,model,hidden,batch_size,vocab_tar_size,trainer):\n",
    "        for batch, dat in enumerate(data_iter):\n",
    "            inp = dat.data[0]\n",
    "            targ = dat.label[0]\n",
    "            #batch_size = batch_size.as_in_context(ctx)\n",
    "            #vocab_tar_size = vocab_tar_size.as_in_context(ctx)\n",
    "            with mx.autograd.record():\n",
    "                result = model(inp,targ,hidden,batch_size,vocab_tar_size)\n",
    "            loss=result[0]\n",
    "            acc=result[1]\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size = batch_size)\n",
    "\n",
    "            if batch % 600 == 0:\n",
    "                print('Batch {} Loss {}'.format(batch,loss.asnumpy()[0]))\n",
    "                print('Batch {} Accuracy {}'.format(batch,acc))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        data_iter.reset()\n",
    "\n",
    "\n",
    "    # In[9]:\n",
    "\n",
    "\n",
    "    def test_old(val_iter,model,hidden,batch_size,vocab_tar_size):\n",
    "        val_loss = 0\n",
    "        for batch, dat in enumerate(val_iter):\n",
    "            inp = dat.data[0]\n",
    "            targ = dat.label[0]\n",
    "            loss = model(inp,targ,hidden,batch_size,vocab_tar_size)\n",
    "            val_loss += loss.asnumpy()[0]\n",
    "        print(\"Validation Perplexity :\",np.power(2,val_loss/batch))\n",
    "        val_iter.reset()\n",
    "\n",
    "    def test(val_iter,model,hidden,batch_size,vocab_tar_size):\n",
    "        val_loss = 0\n",
    "        t_acc = 0\n",
    "        for batch, dat in enumerate(val_iter):\n",
    "            inp = dat.data[0]\n",
    "            targ = dat.label[0]\n",
    "            result = model(inp,targ,hidden,batch_size,vocab_tar_size)\n",
    "            val_loss += (result[0]).asnumpy()[0]\n",
    "            t_acc += result[1]\n",
    "        print(\"Validation Loss :\",(val_loss/batch))  \n",
    "        print(\"Validation Perplexity :\",np.power(2,val_loss/batch))\n",
    "        print(\"Validation Acc :\",t_acc/batch)\n",
    "        sys.stdout.flush()\n",
    "        val_iter.reset()\n",
    "\n",
    "\n",
    "    # In[10]:\n",
    "\n",
    "\n",
    "    def run_tr(n_epochs, BATCH_SIZE, embedding_dim, units):\n",
    "        path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
    "        path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "        mx.random.seed(1)\n",
    "        np.random.seed(1)\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ = create_db(path_to_file)\n",
    "        BUFFER_SIZE = len(input_tensor_train)\n",
    "        BATCH_SIZE = 128\n",
    "        data_iter = mx.io.NDArrayIter(mx.nd.array(input_tensor_train),mx.nd.array(target_tensor_train),BATCH_SIZE,True)\n",
    "        val_iter = mx.io.NDArrayIter(mx.nd.array(input_tensor_val),mx.nd.array(target_tensor_val),BATCH_SIZE,True)\n",
    "        N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "        embedding_dim = 256\n",
    "        units = 256\n",
    "        train_samples = len(input_tensor_train)\n",
    "        val_samples = len(input_tensor_val)\n",
    "        encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "        hidden = encoder.init_hidden()\n",
    "        decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "        model = Encap(encoder,decoder)\n",
    "        model.initialize()\n",
    "        trainer = gl.Trainer(model.collect_params(), mx.optimizer.Adam(learning_rate=0.0001))\n",
    "        start = time.time()\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"MXNet_CPU\",\"Manythings\")\n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "        epoch=1\n",
    "        while (time_consumed <= 86400 and epoch <= n_epochs):\n",
    "            print(\"\\n\\nEpoch \",epoch)\n",
    "            print(\"Time since beginning \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "            train(data_iter,model,hidden,BATCH_SIZE,vocab_tar_size,trainer)\n",
    "            test(val_iter,model,hidden,BATCH_SIZE,vocab_tar_size)\n",
    "            epoch += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            sys.stdout.flush()\n",
    "            if epoch % 5 == 0:\n",
    "                model.save_parameters('MXNet_CPU_Translation_LSTM_model')\n",
    "        end = time.time()\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"\\n\\nTotal Time Consumed \", str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "\n",
    "\n",
    "    # In[11]:\n",
    "\n",
    "\n",
    "    run_tr(n_epochs, BATCH_SIZE = 128, embedding_dim = 256, units = 256)\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ptb(n_epochs):\n",
    "\n",
    "    class PtbIterator():\n",
    "\n",
    "        def __init__(self, train, batch_size, seq_len, skip_step=5):\n",
    "            self.data = load_ptb(train)\n",
    "            self.batch_size = batch_size\n",
    "            self.seq_len = seq_len\n",
    "            self.skip_step = skip_step\n",
    "            self.reset()\n",
    "\n",
    "        def __iter__(self):\n",
    "            self.reset()\n",
    "            return self\n",
    "\n",
    "        def __next__(self):\n",
    "            x = mx.nd.empty((self.batch_size, self.seq_len), dtype=np.int32)\n",
    "            y = mx.nd.empty((self.batch_size, self.seq_len), dtype=np.int32)\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                if self.cur_idx + self.seq_len >= len(self.data):\n",
    "                    raise StopIteration\n",
    "                x[i, :] = self.data[self.cur_idx:self.cur_idx+self.seq_len]\n",
    "                y[i, :] = self.data[self.cur_idx+1:self.cur_idx+self.seq_len+1]\n",
    "                self.cur_idx += self.skip_step\n",
    "            return x, y\n",
    "\n",
    "        def reset(self):\n",
    "            self.cur_idx = 0\n",
    "\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "\n",
    "    class PtbLstm(gl.Block):\n",
    "\n",
    "        def __init__(self, vocabulary_size, hidden_size, num_layers, dropout):\n",
    "            super().__init__()\n",
    "            with self.name_scope():\n",
    "                self.embed = gl.nn.Embedding(input_dim=vocabulary_size, output_dim=hidden_size,\n",
    "                                             weight_initializer=init.Uniform(1.0))\n",
    "                self.lstm = gl.rnn.LSTM(input_size=hidden_size, hidden_size=hidden_size, \n",
    "                                        num_layers=num_layers, layout='TNC',\n",
    "                                        i2h_weight_initializer=init.Xavier(),\n",
    "                                        h2h_weight_initializer=init.Orthogonal())\n",
    "                self.dropout = gl.nn.Dropout(rate=dropout)\n",
    "                self.fc = gl.nn.Dense(in_units=hidden_size, units=vocabulary_size, flatten=False,\n",
    "                                      weight_initializer=init.Xavier())\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.embed(x)\n",
    "            o = self.lstm(x)\n",
    "            o = self.dropout(o)\n",
    "            f = self.fc(o)\n",
    "            return f\n",
    "\n",
    "\n",
    "    # In[9]:\n",
    "\n",
    "\n",
    "    def ptb_train(model, data_iter, criterion, trainer, epoch, print_every=1000):\n",
    "        losses = []\n",
    "        t_acc = 0\n",
    "        total = 0\n",
    "        for i, (inputs, labels) in enumerate(data_iter):\n",
    "            with mx.autograd.record():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                outputs=np.argmax(outputs,-1)\n",
    "                acc = mx.metric.Accuracy()\n",
    "                acc.update(outputs, labels)\n",
    "                t_acc += (acc.get())[1]\n",
    "                total += 1\n",
    "            losses.append(loss.mean().asscalar())\n",
    "            trainer.step(batch_size=labels.shape[0])\n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('[%d, %5d] train loss: %.3f' % (epoch, i + 1, np.mean(losses)))\n",
    "                print('[%d, %5d] train acc: %.3f' % (epoch, i + 1, t_acc/total))\n",
    "                losses = []\n",
    "                t_acc = 0\n",
    "                total = 0\n",
    "                sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def ptb_test(model, data_iter, criterion, epoch):\n",
    "        losses = []\n",
    "        t_acc = 0\n",
    "        total = 0\n",
    "        for inputs, labels in data_iter:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            losses.append(mx.nd.mean(loss).asscalar())\n",
    "\n",
    "            outputs=np.argmax(outputs,-1)\n",
    "            acc = mx.metric.Accuracy()\n",
    "            acc.update(outputs, labels)\n",
    "            t_acc += (acc.get())[1]\n",
    "            total += 1\n",
    "\n",
    "\n",
    "        loss = np.mean(losses)\n",
    "        perplexity = np.exp(loss)\n",
    "        print('[%d] test loss: %.3f perplexity: %.3f' % (epoch, loss, perplexity))\n",
    "        print('[%d] test acc: %.3f ' % (epoch, t_acc/total))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def ptb_run(n_epochs, hidden_size, batch_size, seq_len, dropout, num_layers):\n",
    "        mx.random.seed(1)\n",
    "        np.random.seed(1)\n",
    "        random.seed(1)\n",
    "\n",
    "        ptb_vocab = load_ptb_vocab()\n",
    "        vocabulary_size = len(ptb_vocab)\n",
    "\n",
    "        train_iter = PtbIterator(train=True, batch_size=batch_size, seq_len=seq_len)\n",
    "        test_iter = PtbIterator(train=False, batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "        model = PtbLstm(vocabulary_size, hidden_size, num_layers, dropout)\n",
    "        model.initialize()\n",
    "        criterion = gl.loss.SoftmaxCrossEntropyLoss()\n",
    "        trainer = gl.Trainer(model.collect_params(), mx.optimizer.AdaDelta(rho=0.95, epsilon=1e-06))\n",
    "\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"MXNet_CPU\", \"PTB\")\n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "        epoch=1\n",
    "\n",
    "        while (time_consumed <= 86400 and epoch <= n_epochs):\n",
    "            ptb_train(model, train_iter, criterion, trainer, epoch)\n",
    "            ptb_test(model, test_iter, criterion, epoch)\n",
    "            epoch += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Time since beginning: \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "            sys.stdout.flush()\n",
    "            if epoch % 10 == 0:\n",
    "                model.save_parameters('MXNet_CPU_PTB_LSTM_model')\n",
    "\n",
    "        end = time.time()\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"\\n\\nTotal Time Consumed \", str(datetime.timedelta(seconds=end-start)))\n",
    "        model.save_parameters('MXNet_CPU_PTB_LSTM_model')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ptb_run(n_epochs, hidden_size = 200, batch_size = 20, seq_len = 30, dropout = 0.5, num_layers = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_imdb(n_epochs=50)\n",
    "#run_manythings(n_epochs=100)\n",
    "run_ptb(n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
