{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import chainer as ch\n",
    "from chainer import functions as F\n",
    "from chainer import links as L \n",
    "from chainer import initializers as init\n",
    "from chainer.datasets import TupleDataset\n",
    "from chainer import serializers\n",
	
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import LoggerYN as YN\n",
    "from data import load_imdb \n",
    "import chainer\n",
    "import chainer as ch\n",
    "from chainer.dataset import Iterator\n",
    "from data import load_ptb, load_ptb_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_imdb(n_epochs):\n",
    "    \n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "    class ImdbDataset(TupleDataset):\n",
    "\n",
    "        def __init__(self, train, vocabulary_size, seq_len):\n",
    "            x, y = load_imdb(train, vocabulary_size, seq_len)\n",
    "\n",
    "            lens = [len(xi) for xi in x]\n",
    "            x = F.pad_sequence([np.array(xi) for xi in x], length=seq_len)\n",
    "            super().__init__(x, lens, y)\n",
    "\n",
    "    def collate_sequences(batch):\n",
    "        sorted_batch = sorted(batch, key=lambda elem: elem[1], reverse=True)\n",
    "        sequences, lengths, labels = zip(*sorted_batch)\n",
    "        sequences = F.stack(sequences)\n",
    "        labels = np.array(labels)\n",
    "        return (sequences, lengths), labels\n",
    "\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "\n",
    "    class ImdbLstm(ch.Chain):\n",
    "\n",
    "        def __init__(self, vocabulary_size, embedding_dim, hidden_size):\n",
    "            super().__init__()\n",
    "            with self.init_scope():\n",
    "                self.embed = L.EmbedID(in_size=vocabulary_size, out_size=embedding_dim,\n",
    "                                       initialW=init.Uniform(1.0))\n",
    "                self.lstm = L.LSTM(in_size=embedding_dim, out_size=hidden_size,\n",
    "                                   upward_init=init.GlorotUniform(),\n",
    "                                   lateral_init=init.Orthogonal())\n",
    "                self.fc = L.Linear(in_size=hidden_size, out_size=1,\n",
    "                                   initialW=init.GlorotUniform())\n",
    "\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x, lens = inputs\n",
    "            x = self.embed(x)\n",
    "            x = [xi[:l] for xi, l in zip(x, lens)]\n",
    "            x = F.transpose_sequence(x)\n",
    "            self.lstm.reset_state()\n",
    "            h = []\n",
    "            for xt in x:\n",
    "                ht = self.lstm(xt)\n",
    "                h.append(ht)\n",
    "            h = F.transpose_sequence(h)\n",
    "            h = F.stack([hi[-1] for hi in h])\n",
    "            f = self.fc(h)\n",
    "            return F.flatten(f)\n",
    "\n",
    "\n",
    "    # In[9]:\n",
    "\n",
    "\n",
    "    def imdb_train(model, data_iter, criterion, optimizer, epoch, print_every=150):\n",
    "        i = 0\n",
    "        losses = []\n",
    "        correct=0\n",
    "        total=0\n",
    "        data_iter.reset()\n",
    "        while not data_iter.is_new_epoch:\n",
    "            inputs, labels = collate_sequences(data_iter.next())\n",
    "            model.cleargrads()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "\n",
    "            losses.append(loss.array)\n",
    "\n",
    "\n",
    "            preds = (outputs.array >= 0.0) == labels\n",
    "            correct += preds.sum()\n",
    "            total += preds.shape[0] \n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('[%d, %5d] train loss: %.3f' % (epoch, i + 1, np.mean(losses)))\n",
    "                print('[%d, %5d] train  acc: %.3f' % (epoch, i + 1, correct/total))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                losses = []\n",
    "                count=0\n",
    "                total=0\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    def imdb_test(model, data_iter, criterion, epoch):   \n",
    "        losses = []\n",
    "        correct, total = 0, 0\n",
    "        with ch.no_backprop_mode():\n",
    "            data_iter.reset()\n",
    "            while not data_iter.is_new_epoch:\n",
    "                inputs, labels = collate_sequences(data_iter.next())\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                losses.append(loss.array)\n",
    "                preds = (outputs.array >= 0.0) == labels\n",
    "                correct += preds.sum()\n",
    "                total += preds.shape[0] \n",
    "\n",
    "        print(\"\\n\")\n",
    "        print('[%d] test loss: %.3f' % (epoch, np.mean(losses)))\n",
    "        print('[%d] accuracy: %.3f' % (epoch, correct / total * 100))\n",
    "        print(\"\\n\\n\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def imdb_run(n_epochs, vocabulary_size, seq_len, batch_size, embedding_size, hidden_size):\n",
    "        random.seed(1)\n",
    "        np.random.seed(1)\n",
    "\n",
    "\n",
    "        train_dataset = ImdbDataset(train=True, vocabulary_size=vocabulary_size, seq_len=seq_len)\n",
    "        test_dataset = ImdbDataset(train=False, vocabulary_size=vocabulary_size, seq_len=seq_len)\n",
    "        train_iter = ch.iterators.SerialIterator(train_dataset, batch_size=batch_size)\n",
    "        test_iter = ch.iterators.SerialIterator(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = ImdbLstm(vocabulary_size, embedding_size, hidden_size)\n",
    "        criterion = F.sigmoid_cross_entropy\n",
    "        optimizer = ch.optimizers.Adam().setup(model)\n",
    "\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"Chainer_CPU\",\"IMDB\")\n",
    "\n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "        epoch=1\n",
    "\n",
    "        while (time_consumed <= 86400 and epoch <= n_epochs):\n",
    "\n",
    "            imdb_train(model, train_iter, criterion, optimizer, epoch)\n",
    "            imdb_test(model, test_iter, criterion, epoch)\n",
    "            epoch += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Time since beginning \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "            sys.stdout.flush()\n",
    "            if epoch % 10 == 0:\n",
    "                serializers.save_npz('Chainer_CPU_IMDB_LSTM_model', model)\n",
    "\n",
    "        end = time.time()\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        serializers.save_npz('Chainer_CPU_IMDB_LSTM_model', model)\n",
    "        print(\"\\n\\nTotal Time Consumed \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    imdb_run(n_epochs, vocabulary_size = 5000, seq_len = 500, batch_size = 64, embedding_size = 32, hidden_size = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_manythings(n_epochs):\n",
    "    \n",
    "\n",
    "    # Converts the unicode file to ascii\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "    def preprocess_sentence(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.rstrip().strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "\n",
    "    # 1. Remove the accents\n",
    "    # 2. Clean the sentences\n",
    "    # 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "    def create_dataset(path):\n",
    "        lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "        word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:100000]]\n",
    "\n",
    "        return word_pairs\n",
    "\n",
    "        # This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "    # (e.g., 5 -> \"dad\") for each language,\n",
    "    class LanguageIndex():\n",
    "        def __init__(self, lang):\n",
    "            self.lang = lang\n",
    "            self.word2idx = {}\n",
    "            self.idx2word = {}\n",
    "            self.vocab = set()\n",
    "\n",
    "            self.create_index()\n",
    "\n",
    "        def create_index(self):\n",
    "            for phrase in self.lang:\n",
    "                self.vocab.update(phrase.split(' '))\n",
    "\n",
    "            self.vocab = sorted(self.vocab)\n",
    "\n",
    "            self.word2idx['<pad>'] = 0\n",
    "            for index, word in enumerate(self.vocab):\n",
    "                self.word2idx[word] = index + 1\n",
    "\n",
    "            for word, index in self.word2idx.items():\n",
    "                self.idx2word[index] = word\n",
    "\n",
    "    def max_length(tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "    def load_dataset(path):\n",
    "        # creating cleaned input, output pairs\n",
    "        pairs = create_dataset(path)\n",
    "\n",
    "        # index language using the class defined above    \n",
    "        inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "        targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "\n",
    "        # Vectorize the input and target languages\n",
    "\n",
    "        # Spanish sentences\n",
    "        input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # English sentences\n",
    "        target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # Calculate max_length of input and output tensor\n",
    "        # Here, we'll set those to the longest sentence in the dataset\n",
    "        max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "        # Padding the input and output tensor to the maximum length\n",
    "        input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                     maxlen=max_length_inp,\n",
    "                                                                     padding='post')\n",
    "\n",
    "        target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                      maxlen=max_length_tar, \n",
    "                                                                      padding='post')\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    def create_db(path_to_file):\n",
    "        input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file)\n",
    "        # Creating training and validation sets using an 80-20 split\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2,random_state=42)\n",
    "        vocab_inp_size = len(inp_lang.word2idx)\n",
    "        vocab_tar_size = len(targ_lang.word2idx)\n",
    "        return input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    def loss_function(real, pred):\n",
    "        correct=0\n",
    "        real = F.flatten(real)\n",
    "        y_pred=chainer.functions.argmax(pred, axis=1)\n",
    "        correct = (y_pred.array == real.array).sum()\n",
    "        return  [F.softmax_cross_entropy(pred, real),correct,real.size]\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    class Model(chainer.Chain):\n",
    "        def __init__(self,batch_sz,units,embedding_dim,vocab_enc,vocab_dec):\n",
    "            super(Model, self).__init__()\n",
    "            with self.init_scope():\n",
    "                self.batch_sz = batch_sz\n",
    "                self.units = units\n",
    "                self.embedding_enc = L.EmbedID(in_size=vocab_enc, out_size=embedding_dim,initialW=init.Uniform(1.0))\n",
    "                self.LSTM_enc = L.NStepLSTM(in_size=embedding_dim, out_size=units, n_layers=1, dropout=0.0)\n",
    "                self.embedding_dec = L.EmbedID(in_size=vocab_dec, out_size=embedding_dim,initialW=init.Uniform(1.0))\n",
    "                self.LSTM_dec = L.NStepLSTM(in_size=embedding_dim, out_size=units, n_layers=1, dropout=0.0)\n",
    "                self.fc = L.Linear(units,vocab_dec,initialW=init.GlorotUniform())\n",
    "\n",
    "        def forward(self, inp,targ):\n",
    "            x = self.embedding_enc(inp)\n",
    "            x= list(x)\n",
    "            hy,cy,_  = self.LSTM_enc(None,None,x)\n",
    "            x = self.embedding_dec(targ[:,:])\n",
    "            x=list(x)\n",
    "            _,_, output = self.LSTM_dec(hy,cy,x)\n",
    "            output= F.concat(output, axis=0)\n",
    "            predictions = self.fc(output)\n",
    "            return predictions\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    def train(model,train_iterator,optimizer):\n",
    "        t_correct=0\n",
    "        total=0\n",
    "        for (batch, data) in enumerate(train_iterator):\n",
    "            model.cleargrads()\n",
    "            inp =np.array([x for x, _ in data],dtype ='int')\n",
    "            targ =np.array([x for _, x in data],dtype ='int')\n",
    "            #inp= (cuda.to_gpu(inp))\n",
    "            predictions = model(inp,targ)\n",
    "            result = loss_function(targ[:,:], predictions)\n",
    "            loss=result[0]\n",
    "            t_correct+=result[1]\n",
    "            total+=result[2]\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "            if(batch % 300 ==0):\n",
    "                print('\\nBatch: {} loss: {}'.format(batch,loss))\n",
    "                print('Batch: {} acc: {}'.format(batch,t_correct/total))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        train_iterator.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # In[7]:\n",
    "\n",
    "\n",
    "    def test(model,test_iterator):\n",
    "        total_loss = 0\n",
    "        t_correct=0\n",
    "        total=0\n",
    "        for (batch, data) in enumerate(test_iterator):\n",
    "            model.cleargrads()\n",
    "            inp =np.array([x for x, _ in data],dtype ='int')\n",
    "            targ =np.array([x for _, x in data],dtype ='int')\n",
    "            predictions = model(inp,targ)\n",
    "            result = loss_function(targ[:,:], predictions)\n",
    "            loss=result[0]\n",
    "            t_correct+=result[1]\n",
    "            total+=result[2]\n",
    "            total_loss += loss.data\n",
    "        print('\\nPerplexity :',np.power(2,total_loss/batch))\n",
    "        print('Test Loss :',total_loss)\n",
    "        print('Test Acc :',t_correct/total)\n",
    "        sys.stdout.flush()\n",
    "        test_iterator.reset()\n",
    "\n",
    "\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "\n",
    "    def run_tr(n_epochs, BATCH_SIZE, embedding_dim, units):\n",
    "        path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
    "        path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ = create_db(path_to_file)\n",
    "        BUFFER_SIZE = len(input_tensor_train)\n",
    "        N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "        train_samples = len(input_tensor_train)\n",
    "        val_samples = len(input_tensor_val)\n",
    "        # create dataset\n",
    "        dataset_train = chainer.datasets.TupleDataset(input_tensor_train, target_tensor_train)\n",
    "        train_iterator = chainer.iterators.SerialIterator(dataset_train, BATCH_SIZE, repeat=False, shuffle=False, order_sampler=None)\n",
    "        dataset_test = chainer.datasets.TupleDataset(input_tensor_train, target_tensor_train)\n",
    "        test_iterator = chainer.iterators.SerialIterator(dataset_test, BATCH_SIZE, repeat=False, shuffle=False, order_sampler=None)\n",
    "        model = Model(BATCH_SIZE,units,embedding_dim,vocab_inp_size,vocab_tar_size)\n",
    "        optimizer = chainer.optimizers.Adam(0.00001).setup(model)\n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"Chainer_CPU\",\"Manythings\")\n",
    "        epoch=1\n",
    "        while(epoch <= n_epochs and time_consumed <= 86400 ):\n",
    "            print(\"Epoch\", epoch )\n",
    "            train(model,train_iterator,optimizer)\n",
    "            test(model,test_iterator)\n",
    "            epoch += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Time since beginning \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "            sys.stdout.flush()\n",
    "            if epoch % 1 == 10:\n",
    "                serializers.save_npz('Chainer_CPU_ManyThings_LSTM_model', model)\n",
    "\n",
    "        end = time.time()\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"\\n\\nTotal Time Consumed \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "\n",
    "\n",
    "    run_tr(n_epochs, BATCH_SIZE = 128, embedding_dim = 256, units = 256 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ptb(n_epochs):\n",
    "    \n",
    "    class PtbIterator(Iterator):\n",
    "\n",
    "        def __init__(self, train, batch_size, seq_len, skip_step=5):\n",
    "            self.data = load_ptb(train)\n",
    "            self.batch_size = batch_size\n",
    "            self.seq_len = seq_len\n",
    "            self.skip_step = skip_step\n",
    "            self.reset()\n",
    "\n",
    "        def __next__(self):\n",
    "            x = np.zeros((self.batch_size, self.seq_len), dtype=np.int32)\n",
    "            y = np.zeros((self.batch_size, self.seq_len), dtype=np.int32)\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                if self.cur_idx + self.seq_len + 1 >= len(self.data):\n",
    "                    self.epoch += 1\n",
    "                    self.is_new_epoch = True\n",
    "                    self.cur_idx = 0\n",
    "                x[i, :] = self.data[self.cur_idx:self.cur_idx+self.seq_len]\n",
    "                y[i, :] = self.data[self.cur_idx+1:self.cur_idx+self.seq_len+1]\n",
    "                self.cur_idx += self.skip_step\n",
    "            return x, F.flatten(y.T)\n",
    "\n",
    "        def reset(self):\n",
    "            self.epoch = 0\n",
    "            self.is_new_epoch = False\n",
    "            self.cur_idx = 0\n",
    "\n",
    "\n",
    "    # In[43]:\n",
    "\n",
    "\n",
    "    class PtbLstm(ch.Chain):\n",
    "\n",
    "        def __init__(self, vocabulary_size, hidden_size, num_layers, dropout):\n",
    "            super().__init__()\n",
    "            self.dropout = dropout\n",
    "            with self.init_scope():\n",
    "                self.embed = L.EmbedID(in_size=vocabulary_size, out_size=hidden_size,\n",
    "                                       initialW=init.Uniform(1.0))\n",
    "                self.lstms = []\n",
    "                for i in range(1, num_layers + 1):\n",
    "                    lstm = L.LSTM(in_size=hidden_size, out_size=hidden_size,\n",
    "                                  upward_init=init.GlorotUniform(),\n",
    "                                  lateral_init=init.Orthogonal())\n",
    "                    setattr(self, 'lstm_' + str(i), lstm)\n",
    "                    self.lstms.append(lstm)   \n",
    "                self.fc = L.Linear(in_size=hidden_size, out_size=vocabulary_size,\n",
    "                                   initialW=init.GlorotUniform())\n",
    "\n",
    "        def reset_state(self):\n",
    "            for lstm in self.lstms:\n",
    "                lstm.reset_state()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.embed(x.T)\n",
    "            f = []\n",
    "            self.reset_state()\n",
    "            for xt in x:\n",
    "                ht = xt\n",
    "                for lstm in self.lstms:\n",
    "                    ht = lstm(ht)\n",
    "                ht = F.dropout(ht, self.dropout)\n",
    "                ft = self.fc(ht)\n",
    "                f.append(ft)\n",
    "            f = F.concat(f, axis=0)\n",
    "            return f\n",
    "\n",
    "\n",
    "    # In[44]:\n",
    "\n",
    "\n",
    "    def ptb_train(model, data_iter, criterion, optimizer, epoch, print_every=5000):\n",
    "        i = 0\n",
    "        losses = []\n",
    "        data_iter.reset()\n",
    "        t_correct=0\n",
    "        total=0\n",
    "        while not data_iter.is_new_epoch:\n",
    "            inputs, targets = data_iter.next()\n",
    "            model.cleargrads()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.update()\n",
    "            y_pred = chainer.functions.argmax(outputs, axis=1)\n",
    "            t_correct += (y_pred.array == targets.array).sum()\n",
    "            total += targets.size\n",
    "            losses.append(loss.array)\n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('[%d, %5d] train loss: %.3f' % (epoch, i + 1, np.mean(losses)))\n",
    "                print('[%d, %5d] train accu: %.3f' % (epoch,i + 1, t_correct/total))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                losses = []\n",
    "                t_correct=0\n",
    "                total=0\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    def ptb_test(model, data_iter, criterion, epoch):   \n",
    "        losses = []\n",
    "        t_correct=0\n",
    "        total=0\n",
    "        with ch.no_backprop_mode():\n",
    "            data_iter.reset()\n",
    "            while not data_iter.is_new_epoch:\n",
    "                inputs, labels = data_iter.next()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                losses.append(loss.array)\n",
    "\n",
    "                y_pred = chainer.functions.argmax(outputs, axis=1)\n",
    "                t_correct += (y_pred.array == labels.array).sum()\n",
    "                total += labels.size\n",
    "\n",
    "        loss = np.mean(losses)\n",
    "        perplexity = np.exp(loss)\n",
    "        print('[%d] test loss: %.3f perplexity: %.3f' % (epoch, loss, perplexity))\n",
    "        print('[%d] test accu: %.3f' % (epoch, t_correct/total))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def ptb_run(n_epochs, hidden_size, batch_size, seq_len, dropout, num_layers):\n",
    "        random.seed(1)\n",
    "        np.random.seed(1)\n",
    "\n",
    "\n",
    "        ptb_vocab = load_ptb_vocab()\n",
    "        vocabulary_size = len(ptb_vocab)\n",
    "\n",
    "        train_iter = PtbIterator(train=True, batch_size=batch_size, seq_len=seq_len)\n",
    "        test_iter = PtbIterator(train=False, batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "        model = PtbLstm(vocabulary_size, hidden_size, num_layers, dropout)\n",
    "        criterion = F.softmax_cross_entropy\n",
    "        optimizer = ch.optimizers.AdaDelta().setup(model)\n",
    "\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"Chainer_CPU\", \"PTB\")\n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "        epoch=1\n",
    "\n",
    "        while(epoch <= n_epochs and time_consumed <= 86400 ):\n",
    "            ptb_train(model, train_iter, criterion, optimizer, epoch)\n",
    "            ptb_test(model, test_iter, criterion, epoch)\n",
    "            epoch += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Time since beginning \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "            sys.stdout.flush()\n",
    "            if epoch % 1 == 0:\n",
    "                serializers.save_npz('Chainer_CPU_PTB_model', model)    \n",
    "\n",
    "        end = time.time()        \n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"\\n\\nTotal Time Consumed \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "\n",
    "\n",
    "    # In[45]:\n",
    "\n",
    "\n",
    "    ptb_run(n_epochs, hidden_size = 200, batch_size = 20, seq_len = 30, dropout = 0.5, num_layers = 2)\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_imdb(n_epochs=50)\n",
    "#run_manythings(n_epochs=100)\n",
    "#run_ptb(n_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
