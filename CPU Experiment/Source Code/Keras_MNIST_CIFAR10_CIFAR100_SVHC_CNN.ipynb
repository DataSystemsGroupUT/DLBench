{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10,mnist,cifar100\n",
    "from keras import Sequential,optimizers\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import LoggerYN as YN\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import utilsYN as uYN\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    global Dataset    \n",
    "    global pbatchSize\n",
    "    global pnumClasses\n",
    "    global pEpochs\n",
    "    global pLearningRate\n",
    "    global pMomentum\n",
    "    global pWeightDecay\n",
    "    Dataset = dataset\n",
    "    pbatchSize = batchSize\n",
    "    pnumClasses = numClasses\n",
    "    pEpochs = epochs\n",
    "    pLearningRate = learningRate\n",
    "    pMomentum = momentum\n",
    "    pWeightDecay = weightDecay\n",
    "    \n",
    "def NormalizeData(x_train,x_test):\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "        return x_train, x_test\n",
    "\n",
    "def CategorizeData(y_train,y_test,pnumClasses):\n",
    "    y_train = keras.utils.to_categorical(y_train, pnumClasses)\n",
    "    y_test = keras.utils.to_categorical(y_test, pnumClasses)\n",
    "    \n",
    "    \n",
    "    return y_train, y_test\n",
    "    \n",
    "def loadData():\n",
    "    fineFlag= False\n",
    "    global Dataset\n",
    "    if Dataset == \"mnist\":\n",
    "        Dataset = mnist\n",
    "    elif Dataset == \"cifar100\":\n",
    "        Dataset = cifar100\n",
    "    else:\n",
    "        Dataset = cifar10\n",
    "    (x_train, y_train), (x_test, y_test) = Dataset.load_data(label_mode='fine') if fineFlag else Dataset.load_data()\n",
    "    \n",
    "    global imgRows\n",
    "    global imgCols\n",
    "    global inputShape\n",
    "    \n",
    "    imgRows = x_train.shape[1]\n",
    "    imgCols = x_train.shape[2]\n",
    "\n",
    "    try:\n",
    "        imgRGB_Dimensions = x_train.shape[3]\n",
    "    except Exception:\n",
    "        imgRGB_Dimensions = 1 #For Gray Scale Images\n",
    "\n",
    "    print(x_train.shape)\n",
    "    x_train = x_train.reshape(x_train.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_test = x_test.reshape(x_test.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train, x_test = NormalizeData(x_train, x_test)\n",
    "    y_train, y_test = CategorizeData(y_train,y_test,pnumClasses)\n",
    "    inputShape = (imgRows, imgCols, imgRGB_Dimensions)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def loadDataSVHN(fname = './%s_32x32.mat',extra=False):\n",
    "    \"\"\"Load the SVHN dataset (optionally with extra images)\n",
    "    Args:\n",
    "        extra (bool, optional): load extra training data\n",
    "    Returns:\n",
    "        Dataset: SVHN data\n",
    "    \"\"\"\n",
    "    def load_mat(fname):\n",
    "        data = sio.loadmat(fname)\n",
    "        X = data['X'].transpose(3, 0, 1, 2)\n",
    "        y = data['y'] % 10  # map label \"10\" --> \"0\"\n",
    "        return X, y\n",
    "\n",
    "    data = uYN.Dataset()\n",
    "    data.classes = np.arange(10)\n",
    "\n",
    "    X, y = load_mat(fname % 'train')\n",
    "    data.train_images = X\n",
    "    data.train_labels = y.reshape(-1)\n",
    "\n",
    "    X, y = load_mat(fname % 'test')\n",
    "    data.test_images = X\n",
    "    data.test_labels = y.reshape(-1)\n",
    "\n",
    "    new_x = data.train_images\n",
    "    new_y = data.train_labels\n",
    "    \n",
    "    if extra:\n",
    "        X, y = load_mat(fname % 'extra')\n",
    "        data.extra_images = X\n",
    "        data.extra_labels = y.reshape(-1)\n",
    "    \n",
    "        # Use extra dataset\n",
    "        new_x = data.extra_images#np.concatenate((data.extra_images, data.train_images), axis=0)\n",
    "        new_y = data.extra_labels#np.concatenate((data.extra_labels, data.train_labels), axis=0)\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test)  = (new_x,new_y),(data.test_images,data.test_labels)\n",
    "    \n",
    "    global imgRows\n",
    "    global imgCols\n",
    "    global inputShape\n",
    "    \n",
    "    imgRows = x_train.shape[1]\n",
    "    imgCols = x_train.shape[2]\n",
    "\n",
    "    try:\n",
    "        imgRGB_Dimensions = x_train.shape[3]\n",
    "    except Exception:\n",
    "        imgRGB_Dimensions = 1 #For Gray Scale Images\n",
    "\n",
    "    print(x_train.shape)\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_test = x_test.reshape(x_test.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train, x_test = NormalizeData(x_train, x_test)\n",
    "    y_train, y_test = CategorizeData(y_train,y_test,10)\n",
    "    inputShape = (imgRows, imgCols, imgRGB_Dimensions)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def model_MNIST():\n",
    "    MNIST_model = Sequential()\n",
    "    MNIST_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=inputShape))\n",
    "    MNIST_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    MNIST_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    MNIST_model.add(Dropout(0.25))\n",
    "    MNIST_model.add(Flatten())\n",
    "    MNIST_model.add(Dense(128, activation='relu'))\n",
    "    MNIST_model.add(Dropout(0.5))\n",
    "    MNIST_model.add(Dense(pnumClasses, activation='softmax'))     \n",
    "    return MNIST_model\n",
    "\n",
    "def model_CIFAR10():\n",
    "    CIFAR_model = Sequential()\n",
    "    CIFAR_model.add(Conv2D(32, (3, 3), padding='same',input_shape=inputShape))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(Conv2D(32, (3, 3)))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    CIFAR_model.add(Dropout(0.25))\n",
    "    CIFAR_model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(Conv2D(64, (3, 3)))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    CIFAR_model.add(Dropout(0.25))\n",
    "    CIFAR_model.add(Flatten())\n",
    "    CIFAR_model.add(Dense(512))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(Dropout(0.5))\n",
    "    CIFAR_model.add(Dense(pnumClasses))\n",
    "    CIFAR_model.add(Activation('softmax'))    \n",
    "    return CIFAR_model\n",
    "\n",
    "def model_CIFAR100():\n",
    "    CIFAR_model = Sequential()\n",
    "    CIFAR_model.add(Conv2D(128, (3, 3), padding='same',strides=1,input_shape=inputShape))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(Conv2D(128, (3, 3)))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(MaxPooling2D(pool_size=(2, 2),strides=2,padding='valid'))\n",
    "    CIFAR_model.add(Dropout(0.1))\n",
    "    \n",
    "    CIFAR_model.add(Conv2D(256, (3, 3), padding='same',strides=1))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(Conv2D(256, (3, 3)))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(MaxPooling2D(pool_size=(2, 2),strides=2,padding='valid'))\n",
    "    CIFAR_model.add(Dropout(0.25))\n",
    "    \n",
    "    CIFAR_model.add(Conv2D(512, (3, 3), padding='same',strides=1))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(Conv2D(512, (3, 3)))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(MaxPooling2D(pool_size=(2, 2),strides=2,padding='valid'))\n",
    "    CIFAR_model.add(Dropout(0.5))\n",
    "    \n",
    "    \n",
    "    CIFAR_model.add(Flatten())\n",
    "    CIFAR_model.add(Dense(1024))\n",
    "    CIFAR_model.add(Activation('relu'))\n",
    "    CIFAR_model.add(Dropout(0.5))\n",
    "    CIFAR_model.add(Dense(pnumClasses))\n",
    "    CIFAR_model.add(Activation('softmax'))    \n",
    "    return CIFAR_model\n",
    "\n",
    "def model_SVHC():\n",
    "    model_SVHC = Sequential()\n",
    "    model_SVHC.add(Conv2D(48, (5, 5), padding='same',input_shape=inputShape))\n",
    "    model_SVHC.add(Activation('relu'))\n",
    "    model_SVHC.add(MaxPooling2D(pool_size=(2, 2),padding='same',strides=2))\n",
    "    model_SVHC.add(Dropout(0.2))\n",
    "    \n",
    "    model_SVHC.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    model_SVHC.add(keras.layers.normalization.BatchNormalization())\n",
    "    model_SVHC.add(Activation('relu'))\n",
    "    model_SVHC.add(MaxPooling2D(pool_size=(2, 2),padding='same',strides=1))\n",
    "    model_SVHC.add(Dropout(0.2))\n",
    "    \n",
    "    model_SVHC.add(Conv2D(128, (5, 5), padding='same'))\n",
    "    model_SVHC.add(keras.layers.normalization.BatchNormalization())\n",
    "    model_SVHC.add(Activation('relu'))\n",
    "    model_SVHC.add(MaxPooling2D(pool_size=(2, 2),padding='same',strides=2))\n",
    "    model_SVHC.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    model_SVHC.add(Conv2D(160, (5, 5), padding='same'))\n",
    "    model_SVHC.add(keras.layers.normalization.BatchNormalization())\n",
    "    model_SVHC.add(Activation('relu'))\n",
    "    model_SVHC.add(MaxPooling2D(pool_size=(2, 2),padding='same',strides=1))\n",
    "    model_SVHC.add(Dropout(0.2))\n",
    "    \n",
    "    model_SVHC.add(Conv2D(192, (5, 5), padding='same'))\n",
    "    model_SVHC.add(keras.layers.normalization.BatchNormalization())\n",
    "    model_SVHC.add(Activation('relu'))\n",
    "    model_SVHC.add(MaxPooling2D(pool_size=(2, 2),padding='same',strides=2))\n",
    "    model_SVHC.add(Dropout(0.2))\n",
    "    \n",
    "    model_SVHC.add(Conv2D(192, (5, 5), padding='same'))\n",
    "    model_SVHC.add(keras.layers.normalization.BatchNormalization())\n",
    "    model_SVHC.add(Activation('relu'))\n",
    "    model_SVHC.add(MaxPooling2D(pool_size=(2, 2),padding='same',strides=1))\n",
    "    model_SVHC.add(Dropout(0.2))\n",
    "    \n",
    "    model_SVHC.add(Conv2D(192, (5, 5), padding='same'))\n",
    "    model_SVHC.add(keras.layers.normalization.BatchNormalization())\n",
    "    model_SVHC.add(Activation('relu'))\n",
    "    model_SVHC.add(MaxPooling2D(pool_size=(2, 2),padding='same',strides=2))\n",
    "    model_SVHC.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    model_SVHC.add(Conv2D(192, (5, 5), padding='same'))\n",
    "    model_SVHC.add(keras.layers.normalization.BatchNormalization())\n",
    "    model_SVHC.add(Activation('relu'))\n",
    "    model_SVHC.add(MaxPooling2D(pool_size=(2, 2),padding='same',strides=1))\n",
    "    model_SVHC.add(Dropout(0.2))\n",
    "    \n",
    "    model_SVHC.add(Flatten())\n",
    "    \n",
    "    model_SVHC.add(Dense(3072))\n",
    "    model_SVHC.add(Activation('relu'))\n",
    "    \n",
    "    \n",
    "    model_SVHC.add(Dropout(0.5))\n",
    "    model_SVHC.add(Dense(pnumClasses))\n",
    "    model_SVHC.add(Activation('softmax'))    \n",
    "     \n",
    "    return model_SVHC        \n",
    "\n",
    "def evaluateModel(model,x_test,y_test,verbose):\n",
    "    pLoss, pAcc = model.evaluate(x_test, y_test, verbose)\n",
    "    print(\"Test Loss\", pLoss)\n",
    "    print(\"Test Accuracy\", pAcc)\n",
    "     \n",
    "def RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    x_train, y_train, x_test, y_test = loadData()\n",
    "    MNIST_model = model_MNIST()\n",
    "    MNIST_sgd = optimizers.SGD(lr=learningRate, decay=weightDecay, momentum=momentum, nesterov=False)\n",
    "    MNIST_model.compile(loss='categorical_crossentropy',optimizer=MNIST_sgd, metrics=['accuracy'])\n",
    "    #print(MNIST_model.summary())\n",
    "    #Training the model\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Keras\",\"MNIST\")\n",
    "    start = time.time()\n",
    "    MNIST_model.fit(x_train, y_train,batch_size=batchSize,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
    "    end = time.time()\n",
    "    evaluateModel(MNIST_model,x_test, y_test, verbose=1)\n",
    "    #Model performance evaluation\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "def RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    x_train, y_train, x_test, y_test = loadData()\n",
    "    CIFAR_model = model_CIFAR10()\n",
    "    CIFAR_sgd = optimizers.SGD(lr=learningRate, decay=weightDecay, momentum=momentum, nesterov=False)\n",
    "    CIFAR_model.compile(loss='categorical_crossentropy',optimizer=CIFAR_sgd, metrics=['accuracy'])\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Keras\",\"CIFAR10\")\n",
    "    start = time.time()\n",
    "    CIFAR_model.fit(x_train, y_train,batch_size=batchSize,epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "    end = time.time()\n",
    "    evaluateModel(CIFAR_model,x_test, y_test, verbose=1)\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "def RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    x_train, y_train, x_test, y_test = loadData()\n",
    "    CIFAR_model = model_CIFAR100()\n",
    "    CIFAR_sgd = optimizers.SGD(lr=learningRate, decay=weightDecay, momentum=momentum, nesterov=False)\n",
    "    CIFAR_model.compile(loss='categorical_crossentropy',optimizer=CIFAR_sgd, metrics=['accuracy'])\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Keras\",\"CIFAR100\")\n",
    "    start = time.time()\n",
    "    CIFAR_model.fit(x_train, y_train,batch_size=batchSize,epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "    end = time.time()\n",
    "    evaluateModel(CIFAR_model,x_test, y_test, verbose=1)\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "def RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay,fname):\n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    x_train, y_train, x_test, y_test = loadDataSVHN(fname,True)\n",
    "    SVHC_model = model_SVHC()\n",
    "    SVHC_sgd = optimizers.SGD(lr=learningRate, decay=weightDecay, momentum=momentum, nesterov=False)\n",
    "    SVHC_model.compile(loss='categorical_crossentropy',optimizer=SVHC_sgd, metrics=['accuracy'])\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Keras\",\"SVHN\")\n",
    "    start = time.time()\n",
    "    SVHC_model.fit(x_train, y_train,batch_size=batchSize,epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "    end = time.time()\n",
    "    evaluateModel(SVHC_model,x_test, y_test, verbose=1)\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "        \n",
    "def runModel(dataset,batchSize=128,numClasses=10,epochs=12,learningRate=0.01,momentum=0.5,weightDecay=1e-6):\n",
    "    \n",
    "    if dataset is \"mnist\":\n",
    "        RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar10\":\n",
    "        RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar100\":\n",
    "        RunCIFAR100(dataset,batchSize,numClasses=100,epochs=epochs,learningRate=learningRate,momentum=momentum,weightDecay=weightDecay)\n",
    "    elif dataset is \"SVHN\":\n",
    "        fname = \"./%s_32x32.mat\"\n",
    "        RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay,fname)\n",
    "    else:\n",
    "        print(\"Choose cifar10 or mnist or svhc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#    runModel(\"mnist\",epochs=15)\n",
    "#     runModel(\"cifar10\",epochs=100)\n",
    "     runModel(\"cifar100\",epochs=200)\n",
    "#     runModel(\"SVHN\",epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
