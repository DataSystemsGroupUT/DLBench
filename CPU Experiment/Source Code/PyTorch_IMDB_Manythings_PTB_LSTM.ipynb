{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as utils\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
	
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import LoggerYN as YN\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "from data import load_imdb \n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "from data import load_ptb, load_ptb_vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_imdb(n_epochs):\n",
    "\n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    class ImdbDataset(Dataset):\n",
    "\n",
    "        def __init__(self, train, vocabulary_size, seq_len):\n",
    "            x, y = load_imdb(train, vocabulary_size, seq_len)\n",
    "\n",
    "            self.lens = torch.LongTensor([len(xi) for xi in x])\n",
    "            self.x = pad_sequence([torch.LongTensor(xi) for xi in x], batch_first=True)\n",
    "            self.y = torch.FloatTensor(y)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.x)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.x[idx], self.lens[idx], self.y[idx]\n",
    "\n",
    "    def collate_sequences(batch):\n",
    "        sorted_batch = sorted(batch, key=lambda elem: elem[1], reverse=True)\n",
    "        sequences, lengths, labels = zip(*sorted_batch)\n",
    "        sequences = torch.stack(sequences)\n",
    "        lengths = torch.LongTensor(lengths)\n",
    "        labels = torch.FloatTensor(labels)\n",
    "        return (sequences, lengths), labels\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    class ImdbLstm(nn.Module):\n",
    "\n",
    "        def __init__(self, vocabulary_size, embedding_size, hidden_size):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=embedding_size)\n",
    "            nn.init.uniform_(self.embed.weight, -1.0, 1.0)\n",
    "\n",
    "            self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, batch_first=True)\n",
    "            nn.init.xavier_uniform_(self.lstm.weight_ih_l0)\n",
    "            nn.init.orthogonal_(self.lstm.weight_hh_l0)\n",
    "            nn.init.constant_(self.lstm.bias_ih_l0, 0.0)\n",
    "            nn.init.constant_(self.lstm.bias_ih_l0[hidden_size:2*hidden_size], 1.0)\n",
    "            nn.init.constant_(self.lstm.bias_hh_l0, 0.0)\n",
    "\n",
    "            self.fc = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "            nn.init.xavier_uniform_(self.fc.weight)\n",
    "            nn.init.constant_(self.fc.bias, 0.0)\n",
    "\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x, lens = inputs\n",
    "            x = self.embed(x)\n",
    "            x = pack_padded_sequence(x, lens, batch_first=True)\n",
    "            o, (h, c) = self.lstm(x)\n",
    "            f = self.fc(h[-1]) \n",
    "            return torch.sigmoid(f).flatten()\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    def imdb_train(model, data_loader, criterion, optimizer, epoch, print_every=50):\n",
    "        model.train()\n",
    "\n",
    "        losses = []\n",
    "        for i, (inputs, labels) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('[%d, %5d] train loss: %.3f' % (epoch, i + 1, np.mean(losses)))\n",
    "                losses = []\n",
    "                sys.stdout.flush()\n",
    "\n",
    "\n",
    "    def imdb_test(model, data_loader, criterion, epoch):\n",
    "        model.eval()\n",
    "\n",
    "        losses = []\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                preds = (outputs >= 0.5).float() == labels\n",
    "                correct += preds.sum().item()\n",
    "                total += preds.size(0)\n",
    "\n",
    "        print('[%d] test loss: %.3f accuracy: %.3f' % (epoch, np.mean(losses), correct / total * 100))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    def imdb_run(n_epochs, vocabulary_size, seq_len, batch_size, embedding_size, hidden_size):\n",
    "        torch.manual_seed(1)\n",
    "\n",
    "\n",
    "        train_dataset = ImdbDataset(train=True, vocabulary_size=vocabulary_size, seq_len=seq_len)\n",
    "        test_dataset = ImdbDataset(train=False, vocabulary_size=vocabulary_size, seq_len=seq_len)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_sequences)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_sequences)\n",
    "\n",
    "        model = ImdbLstm(vocabulary_size, embedding_size, hidden_size)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"PyTorch_CPU\",\"IMDB\")\n",
    "\n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "        epoch=0\n",
    "\n",
    "        while (time_consumed <= 86400 and epoch <= n_epochs):\n",
    "            print(\"\\n\\nEpoch \",epoch)\n",
    "            imdb_train(model, train_loader, criterion, optimizer, epoch)\n",
    "            imdb_test(model, test_loader, criterion, epoch)\n",
    "            epoch += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Time since beginning \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "            sys.stdout.flush()\n",
    "            if epoch % 20 == 0:\n",
    "                torch.save(model.state_dict(), 'Pytorch_GPU_IMDB_LSTM_model')\n",
    "\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"\\n\\nTotal Time Consumed \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    imdb_run(n_epochs, vocabulary_size = 5000, seq_len = 500, batch_size = 64, embedding_size = 32, hidden_size = 100)\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_manythings(n_epochs):\n",
    "    \n",
    "\n",
    "    # Converts the unicode file to ascii\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "    def preprocess_sentence(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.rstrip().strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "\n",
    "    # 1. Remove the accents\n",
    "    # 2. Clean the sentences\n",
    "    # 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "    def create_dataset(path):\n",
    "        lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "        word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines]\n",
    "\n",
    "        return word_pairs\n",
    "\n",
    "        # This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "    # (e.g., 5 -> \"dad\") for each language,\n",
    "    class LanguageIndex():\n",
    "        def __init__(self, lang):\n",
    "            self.lang = lang\n",
    "            self.word2idx = {}\n",
    "            self.idx2word = {}\n",
    "            self.vocab = set()\n",
    "\n",
    "            self.create_index()\n",
    "\n",
    "        def create_index(self):\n",
    "            for phrase in self.lang:\n",
    "                self.vocab.update(phrase.split(' '))\n",
    "\n",
    "            self.vocab = sorted(self.vocab)\n",
    "\n",
    "            self.word2idx['<pad>'] = 0\n",
    "            for index, word in enumerate(self.vocab):\n",
    "                self.word2idx[word] = index + 1\n",
    "\n",
    "            for word, index in self.word2idx.items():\n",
    "                self.idx2word[index] = word\n",
    "\n",
    "    def max_length(tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "    def load_dataset(path):\n",
    "        # creating cleaned input, output pairs\n",
    "        pairs = create_dataset(path)\n",
    "\n",
    "        # index language using the class defined above    \n",
    "        inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "        targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "\n",
    "        # Vectorize the input and target languages\n",
    "\n",
    "        # Spanish sentences\n",
    "        input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # English sentences\n",
    "        target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # Calculate max_length of input and output tensor\n",
    "        # Here, we'll set those to the longest sentence in the dataset\n",
    "        max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "        # Padding the input and output tensor to the maximum length\n",
    "        input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                     maxlen=max_length_inp,\n",
    "                                                                     padding='post')\n",
    "\n",
    "        target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                      maxlen=max_length_tar, \n",
    "                                                                      padding='post')\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n",
    "\n",
    "\n",
    "    # In[14]:\n",
    "\n",
    "\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.batch_sz = batch_sz\n",
    "            self.enc_units = enc_units\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "            self.LSTM = nn.LSTM(input_size =embedding_dim, hidden_size = self.enc_units, batch_first=True)\n",
    "            nn.init.xavier_uniform_(self.LSTM.weight_ih_l0)\n",
    "            nn.init.orthogonal_(self.LSTM.weight_hh_l0)\n",
    "            nn.init.constant_(self.LSTM.bias_ih_l0, 0.0)\n",
    "            nn.init.constant_(self.LSTM.bias_ih_l0[self.enc_units:2*self.enc_units], 1.0)\n",
    "            nn.init.constant_(self.LSTM.bias_hh_l0, 0.0)\n",
    "\n",
    "        def forward(self, x, hidden):\n",
    "            x = self.embedding(x)\n",
    "            output, state = self.LSTM(x) \n",
    "            return output, state\n",
    "\n",
    "        def initialize_hidden_state(self):\n",
    "            return torch.zeros((self.batch_sz,self.enc_units))\n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "            super(Decoder, self).__init__()\n",
    "            self.batch_sz = batch_sz\n",
    "            self.dec_units = dec_units\n",
    "            self.embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_dim)\n",
    "            self.LSTM = nn.LSTM(input_size = embedding_dim, hidden_size = self.dec_units, batch_first=True)\n",
    "            self.fc = nn.Linear(self.dec_units,vocab_size)\n",
    "            nn.init.xavier_uniform_(self.LSTM.weight_ih_l0)\n",
    "            nn.init.orthogonal_(self.LSTM.weight_hh_l0)\n",
    "            nn.init.constant_(self.LSTM.bias_ih_l0, 0.0)\n",
    "            nn.init.constant_(self.LSTM.bias_ih_l0[self.dec_units:2*self.dec_units], 1.0)\n",
    "            nn.init.constant_(self.LSTM.bias_hh_l0, 0.0)\n",
    "\n",
    "        def forward(self, x, hidden, enc_output):\n",
    "            x = self.embedding(x)\n",
    "            output, state = self.LSTM(x,hidden)\n",
    "            x = self.fc(output)\n",
    "\n",
    "            return x, state\n",
    "\n",
    "        def initialize_hidden_state(self):\n",
    "            return torch.zeros((self.batch_sz,self.dec_units))\n",
    "\n",
    "\n",
    "\n",
    "    # In[15]:\n",
    "\n",
    "\n",
    "    class Encap(nn.Module):\n",
    "        def __init__(self, encoder,decoder):\n",
    "            super(Encap, self).__init__()\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "\n",
    "        def forward(self, inp,targ, hidden, BATCH_SIZE,vocab_tar_size):\n",
    "            loss = 0\n",
    "            enc_output, enc_hidden = self.encoder(inp, [hidden,hidden])   \n",
    "            dec_hidden = enc_hidden\n",
    "            dec_input = targ[:,:-1]\n",
    "\n",
    "            predictions, dec_hidden = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss = loss_function(targ[:,1:], predictions,vocab_tar_size)\n",
    "            accuracy=acc_function(targ[:,1:], predictions,vocab_tar_size)\n",
    "            #\n",
    "            print(\"Accuracy\", accuracy)\n",
    "\n",
    "            return [loss,accuracy] \n",
    "\n",
    "\n",
    "    def create_db(path_to_file):\n",
    "        input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file)\n",
    "        # Creating training and validation sets using an 80-20 split\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2,random_state=42)\n",
    "        vocab_inp_size = len(inp_lang.word2idx)\n",
    "        vocab_tar_size = len(targ_lang.word2idx)\n",
    "        return input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ\n",
    "\n",
    "    def loss_function(real, pred,vocab):\n",
    "        sfmax = nn.LogSoftmax(dim=1)\n",
    "        cross_ent=F.nll_loss\n",
    "        pred = pred.view(-1,vocab)\n",
    "        pred = sfmax(pred)\n",
    "        real = real.reshape(-1)\n",
    "        loss = cross_ent(pred, real)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def acc_function(real, pred,vocab):\n",
    "        total=0\n",
    "        correct=0\n",
    "        sfmax = nn.LogSoftmax(dim =1)\n",
    "        cross_ent=F.nll_loss\n",
    "        pred = pred.view(-1,vocab)\n",
    "        pred = sfmax(pred)\n",
    "        real = real.reshape(-1)\n",
    "        total += real.size(0)\n",
    "        values, indices = torch.max(pred, 1)\n",
    "        correct += (indices == real).sum().item()\n",
    "        return (correct / total)\n",
    "\n",
    "\n",
    "\n",
    "    def train(model,epoch,my_dataloader,hidden,BATCH_SIZE,vocab_tar_size,optimizer):\n",
    "        for (batch, (inp, targ)) in enumerate(my_dataloader):\n",
    "            model.zero_grad()\n",
    "            result = model(inp,targ,hidden,BATCH_SIZE,vocab_tar_size)\n",
    "            loss=result[0]\n",
    "            acc=result[1]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch % 300 == 0:\n",
    "                print('Batch {} Loss {}'.format(batch,loss))\n",
    "                print('Batch {} Acc {}'.format(batch,acc))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "\n",
    "    def test(model,val_dataloader,hidden,BATCH_SIZE,vocab_tar_size):\n",
    "        t_loss = 0\n",
    "        t_acc = 0\n",
    "        for (batch, (inp, targ)) in enumerate(val_dataloader):\n",
    "\n",
    "            result = model(inp,targ,hidden,BATCH_SIZE,vocab_tar_size)\n",
    "            loss=result[0]\n",
    "            acc=result[1]\n",
    "            t_loss +=loss\n",
    "            t_acc +=acc\n",
    "        print('\\n\\nValidation Acc :{}'.format(t_acc/batch))\n",
    "        print('Validation Loss {}'.format(t_loss/batch))\n",
    "        print('Validation Perplexity {}'.format(math.pow(2,(t_loss/batch))))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    # In[21]:\n",
    "\n",
    "\n",
    "    def run_tr(epochs,BATCH_SIZE,  embedding_dim, units):\n",
    "        path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
    "        path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "        torch.manual_seed(1)\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ = create_db(path_to_file)\n",
    "        # Get parameters\n",
    "        BUFFER_SIZE = len(input_tensor_train)\n",
    "        N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "\n",
    "        train_samples = len(input_tensor_train)\n",
    "        val_samples = len(input_tensor_val)\n",
    "        #data pre-pre processing\n",
    "        input_tensor_train = np.array(input_tensor_train,dtype = 'int')\n",
    "        target_tensor_train = np.array(target_tensor_train,dtype = 'int')\n",
    "        input_tensor_val = np.array(input_tensor_val,dtype = 'int')\n",
    "        target_tensor_val = np.array(target_tensor_val,dtype = 'int') \n",
    "        tensor_x = torch.stack([torch.from_numpy(i) for i in input_tensor_train]) \n",
    "        tensor_y = torch.stack([torch.from_numpy(i) for i in target_tensor_train])\n",
    "        val_x = torch.stack([torch.from_numpy(i) for i in input_tensor_val])\n",
    "        val_y = torch.stack([torch.from_numpy(i) for i in target_tensor_val])\n",
    "        my_dataset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "        my_dataloader = utils.DataLoader(my_dataset,batch_size=BATCH_SIZE)\n",
    "        val_dataset = utils.TensorDataset(val_x,val_y) # create your datset\n",
    "        val_dataloader = utils.DataLoader(val_dataset,batch_size=BATCH_SIZE)\n",
    "        #create model\n",
    "        encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "        decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "        model = Encap(encoder,decoder)\n",
    "\n",
    "        optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr=0.0001)  \n",
    "\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"PyTorch_CPU\",\"Manythings\")\n",
    "\n",
    "        hidden = encoder.initialize_hidden_state()\n",
    "        epoch=1\n",
    "\n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "\n",
    "        while (time_consumed <= 86400 and epoch <= epochs):\n",
    "            print(\"Epoch \",epoch)\n",
    "            train(model,epoch,my_dataloader,hidden,BATCH_SIZE,vocab_tar_size,optimizer)\n",
    "            test(model,val_dataloader,hidden,BATCH_SIZE,vocab_tar_size)\n",
    "            epoch += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Time since beginning: \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                torch.save(model.state_dict(), 'Pytorch_CPU_Manythings_LSTM_model')\n",
    "\n",
    "\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"\\n\\nTotal Time Consumed: \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    # In[22]:\n",
    "\n",
    "\n",
    "    run_tr(n_epochs, BATCH_SIZE = 128,  embedding_dim = 256, units = 256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ptb(n_epochs):\n",
    "    \n",
    "    class PtbIterator:\n",
    "\n",
    "        def __init__(self, train, batch_size, seq_len, skip_step=5):\n",
    "            self.data = load_ptb(train)\n",
    "            self.batch_size = batch_size\n",
    "            self.seq_len = seq_len\n",
    "            self.skip_step = skip_step\n",
    "            self.reset()\n",
    "\n",
    "        def __iter__(self):\n",
    "            self.reset()\n",
    "            return self\n",
    "\n",
    "        def __next__(self):\n",
    "            x = np.zeros((self.batch_size, self.seq_len))\n",
    "            y = np.zeros((self.batch_size, self.seq_len))\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                if self.cur_idx + self.seq_len >= len(self.data):\n",
    "                    raise StopIteration\n",
    "                x[i, :] = self.data[self.cur_idx:self.cur_idx+self.seq_len]\n",
    "                y[i, :] = self.data[self.cur_idx+1:self.cur_idx+self.seq_len+1]\n",
    "                self.cur_idx += self.skip_step\n",
    "            return torch.LongTensor(x), torch.LongTensor(y.ravel())\n",
    "\n",
    "        def reset(self):\n",
    "            self.cur_idx = 0\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    class PtbLstm(nn.Module):\n",
    "        def __init__(self, vocabulary_size, hidden_size, num_layers, dropout):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Embedding(vocabulary_size, hidden_size)\n",
    "            nn.init.uniform_(self.embed.weight, -1.0, 1.0)\n",
    "\n",
    "            self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "            for i in range(num_layers):\n",
    "                li = 'l' + str(i)\n",
    "                nn.init.xavier_uniform_(getattr(self.lstm, 'weight_ih_' + li))\n",
    "                nn.init.orthogonal_(getattr(self.lstm, 'weight_hh_' + li))\n",
    "                nn.init.constant_(getattr(self.lstm, 'bias_ih_' + li), 0.0)\n",
    "                nn.init.constant_(getattr(self.lstm, 'bias_ih_' + li)[hidden_size:2*hidden_size], 1.0)\n",
    "                nn.init.constant_(getattr(self.lstm, 'bias_hh_' + li), 0.0)\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout) \n",
    "            self.linear = nn.Linear(hidden_size, vocabulary_size)\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "            nn.init.constant_(self.linear.bias, 0.0)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.embed(x)\n",
    "            o, (h, c) = self.lstm(x)\n",
    "            o = self.dropout(o)\n",
    "            o = o.reshape(o.size(0)*o.size(1), o.size(2))\n",
    "            f = self.linear(o)\n",
    "            return f\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    def ptb_train(model, data_iter, criterion, optimizer, epoch, print_every=500):\n",
    "        model.train()\n",
    "\n",
    "        losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (inputs, labels) in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "\n",
    "                print('[%d, %5d] train loss: %.3f' % (epoch, i + 1, np.mean(losses)))\n",
    "                print('[%d, %5d] train acc: %.3f' % (epoch, i + 1, (correct / total)))\n",
    "                sys.stdout.flush()  \n",
    "                losses = []\n",
    "                correct = 0\n",
    "                total = 0\n",
    "\n",
    "    def ptb_test(model, data_iter, criterion, epoch):\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_iter:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                losses.append(loss.item())\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        loss = np.mean(losses)\n",
    "        perplexity = np.exp(loss)\n",
    "        print('[%d] test loss: %.3f perplexity: %.3f' % (epoch, loss, perplexity))\n",
    "        #print('Accuracy of the network on the 10000 test images: %d %%' % (correct / total))\n",
    "        print('[%d] test acc: %.3f ' % (epoch,(correct/total)))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def ptb_run(n_epochs, hidden_size, batch_size, seq_len, dropout, num_layers):\n",
    "        torch.manual_seed(1)\n",
    "        ptb_vocab = load_ptb_vocab()\n",
    "        vocabulary_size = len(ptb_vocab)\n",
    "\n",
    "        train_iter = PtbIterator(train=True, batch_size=batch_size, seq_len=seq_len)\n",
    "        test_iter = PtbIterator(train=False, batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "        model = PtbLstm(vocabulary_size, hidden_size, num_layers, dropout)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adadelta(model.parameters())\n",
    "\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"PyTorch_CPU\", \"PTB\")\n",
    "        start = time.time()\n",
    "        epoch=1\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "\n",
    "        while (time_consumed <= 86400 and epoch <= n_epochs):\n",
    "\n",
    "            ptb_train(model, train_iter, criterion, optimizer, epoch)\n",
    "            ptb_test(model, test_iter, criterion, epoch)\n",
    "            epoch += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Time since beginning: \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "            sys.stdout.flush()\n",
    "            if epoch % 1 == 0:\n",
    "                torch.save(model.state_dict(), 'Pytorch_CPU_PTB_LSTM_model')\n",
    "\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        end = time.time()\n",
    "        print(\"\\n\\nTotal Time Consumed: \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "        sys.stdout.flush()\n",
    "        torch.save(model.state_dict(), 'Pytorch_CPU_PTB_LSTM_model')\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    ptb_run(n_epochs, hidden_size = 200, batch_size = 20, seq_len = 30, dropout = 0.5, num_layers = 2)\n",
    "\n",
    "\n",
    "    # In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] train loss: 6.861\n",
      "[1,   500] train acc: 0.061\n"
     ]
    }
   ],
   "source": [
    "#run_imdb(n_epochs = 50)\n",
    "#run_manythings(n_epochs = 100)\n",
    "run_ptb(n_epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
