{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "import os\n",
    "import platform\n",
    "import gzip\n",
    "import LoggerYN as YN\n",
    "import scipy.io as sio\n",
    "import utilsYN as uYN\n",
    "import warnings\n",
    "import time\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\", message=\"Reloaded modules: <module_name>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    global Dataset    \n",
    "    global pbatchSize\n",
    "    global pnumClasses\n",
    "    global pEpochs\n",
    "    global pLearningRate\n",
    "    global pMomentum\n",
    "    global pWeightDecay\n",
    "    Dataset = dataset\n",
    "    pbatchSize = batchSize\n",
    "    pnumClasses = numClasses\n",
    "    pEpochs = epochs\n",
    "    pLearningRate = learningRate\n",
    "    pMomentum = momentum\n",
    "    pWeightDecay = weightDecay\n",
    "    \n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def NormalizeData(x_train,x_test):\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "        return x_train, x_test\n",
    "    \n",
    "\n",
    "\n",
    "# CIFAR 10 ###############################################################\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_CIFAR10_data(num_training=50000, num_validation=1000, num_test=10000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = loadDataCIFAR10_temp(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "\n",
    "def get_CIFAR100_data():\n",
    "        # Check if cifar data exists\n",
    "    if not os.path.exists(\"./cifar-100-batches-py\"):\n",
    "        print(\"CIFAR-10 dataset can not be found. Please download the dataset from 'https://www.cs.toronto.edu/~kriz/cifar.html'.\")\n",
    "        return\n",
    "\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    data = loadDataCIFAR100_temp()\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    y_train = data['Y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['Y_test']\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    num_training = 50000\n",
    "    num_test=10000\n",
    "    \n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def loadDataCIFAR10_temp(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)/np.float32(255)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    Xte = Xte / np.float32(255)\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "\n",
    "def loadDataCIFAR100_temp():\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for j in range(1):\n",
    "      d = unpickle('cifar-100-batches-py/train')\n",
    "      x = d['data']\n",
    "      y = d['fine_labels']\n",
    "      xs.append(x)\n",
    "      ys.append(y)\n",
    "    d = unpickle('cifar-100-batches-py/test')\n",
    "    xs.append(d['data'])\n",
    "    ys.append(d['fine_labels'])\n",
    "    x = np.concatenate(xs)/np.float32(255)\n",
    "    y = np.concatenate(ys)\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3))\n",
    "\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "\n",
    "    X_test = x[50000:,:,:,:]\n",
    "    Y_test = y[50000:]\n",
    "\n",
    "    return dict(X_train=X_train,Y_train=Y_train.astype('int32'),X_test = X_test,Y_test = Y_test.astype('int32'),)   \n",
    "\n",
    "\n",
    "\n",
    "def loadDataCIFAR10():\n",
    "    X_train, y_train, X_test, y_test = get_CIFAR10_data()\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def loadDataCIFAR100():\n",
    "    X_train, y_train, X_test, y_test = get_CIFAR100_data()\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def loadDataMNIST():\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        return data / np.float32(255)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "#    print(X_train.shape)\n",
    "    global imgRows\n",
    "    global imgCols\n",
    "    global imgRGB_Dimensions\n",
    "    global inputShape\n",
    "    \n",
    "    imgRGB_Dimensions = X_train.shape[1]\n",
    "    imgRows = X_train.shape[2]\n",
    "    imgCols = X_train.shape[3]\n",
    "    \n",
    "    \n",
    "    inputShape = (imgRows, imgCols, imgRGB_Dimensions)\n",
    "    \n",
    "    X_train = X_train.transpose(0,2,3,1).astype(\"float\")\n",
    "    X_test = X_test.transpose(0,2,3,1).astype(\"float\")\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def loadDataSVHN(fname,extra=False):\n",
    "\n",
    "    def load_mat(fname):\n",
    "        data = sio.loadmat(fname)\n",
    "        X = data['X'].transpose(3, 0, 1, 2)\n",
    "        y = data['y'] % 10  # map label \"10\" --> \"0\"\n",
    "        return X, y\n",
    "\n",
    "    data = uYN.Dataset()\n",
    "    data.classes = np.arange(10)\n",
    "\n",
    "\n",
    "    X, y = load_mat(fname % 'train')\n",
    "    data.train_images = X\n",
    "    data.train_labels = y.reshape(-1)\n",
    "\n",
    "    X, y = load_mat(fname % 'test')\n",
    "    data.test_images = X\n",
    "    data.test_labels = y.reshape(-1)\n",
    "\n",
    "    new_x = data.train_images\n",
    "    new_y = data.train_labels\n",
    "    \n",
    "    if extra:\n",
    "        X, y = load_mat(fname % 'extra')\n",
    "        data.extra_images = X\n",
    "        data.extra_labels = y.reshape(-1)\n",
    "    \n",
    "        # Use extra dataset\n",
    "        new_x = data.extra_images#np.concatenate((data.extra_images, data.train_images), axis=0)\n",
    "        new_y = data.extra_labels#np.concatenate((data.extra_labels, data.train_labels), axis=0)\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test)  = (new_x,new_y),(data.test_images,data.test_labels)\n",
    "    \n",
    "    global imgRows\n",
    "    global imgCols\n",
    "    global imgRGB_Dimensions\n",
    "    global inputShape\n",
    "    \n",
    "    imgRows = x_train.shape[1]\n",
    "    imgCols = x_train.shape[2]\n",
    "\n",
    "    try:\n",
    "        imgRGB_Dimensions = x_train.shape[3]\n",
    "    except Exception:\n",
    "        imgRGB_Dimensions = 1 #For Gray Scale Images\n",
    "\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_test = x_test.reshape(x_test.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train, x_test = NormalizeData(x_train, x_test)\n",
    "    inputShape = (imgRows, imgCols, imgRGB_Dimensions)\n",
    "    \n",
    "    \n",
    "    x_train = x_train.reshape(len(y_train), imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_test = x_test.reshape(len(y_test), imgRows, imgCols, imgRGB_Dimensions)\n",
    "    \n",
    "    num_training= x_train.shape[0]\n",
    "    mask = range(num_training)\n",
    "    x_train = x_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    \n",
    "    \n",
    "    return x_train,y_train,x_test,y_test    \n",
    "    \n",
    "\n",
    "\n",
    "### Models\n",
    "    \n",
    "class ModelMNIST():\n",
    "    def __init__(self):\n",
    "       \n",
    "        \n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "        self.Wconv1 = tf.get_variable(\"Wconv1\", shape=[3, 3, 1, 32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x64 = 50176\n",
    "        self.Wconv2 = tf.get_variable(\"Wconv2\", shape=[3, 3, 32, 64],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv2 = tf.get_variable(\"bconv2\", shape=[64])\n",
    "   \n",
    " \n",
    "        # affine layer with 1024\n",
    "        self.W1 = tf.get_variable(\"W1\", shape=[9216, 128],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b1 = tf.get_variable(\"b1\", shape=[128])\n",
    "        # affine layer with 10\n",
    "        self.W2 = tf.get_variable(\"W2\", shape=[128, 10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b2 = tf.get_variable(\"b2\", shape=[10])        \n",
    "        \n",
    "    def forward(self, X, y, is_training):\n",
    "        conv1 = tf.nn.conv2d(X, self.Wconv1, strides=[1, 1, 1, 1], padding='VALID') + self.bconv1\n",
    "        relu1 = tf.nn.relu(conv1)\n",
    "        \n",
    "        # Conv\n",
    "        conv2 = tf.nn.conv2d(relu1, self.Wconv2, strides=[1, 1, 1, 1], padding='VALID') + self.bconv2\n",
    "        relu2 = tf.nn.relu(conv2)\n",
    "        \n",
    "        maxpool = tf.layers.max_pooling2d(relu2, pool_size=(2,2),strides=2)\n",
    "        drop1 = tf.layers.dropout(inputs=maxpool, training=is_training,rate=0.25)\n",
    "        \n",
    "        maxpool_flat = tf.reshape(drop1,[-1,9216])\n",
    "\n",
    "        \n",
    "        affine1 = tf.matmul(maxpool_flat, self.W1) + self.b1\n",
    "        \n",
    "     \n",
    "        # ReLU Activation Layer\n",
    "        relu2 = tf.nn.relu(affine1)\n",
    "        \n",
    "        # dropout\n",
    "        drop2 = tf.layers.dropout(inputs=relu2, training=is_training, rate = 0.5)\n",
    "        \n",
    "        # Affine layer from 1024 input units to 10 outputs\n",
    "        affine2 = tf.matmul(drop2, self.W2) + self.b2\n",
    "        \n",
    "        self.predict = affine2\n",
    "    \n",
    "        return self.predict\n",
    "    \n",
    "    def run(self, session, loss_val, Xd, yd,Xt,yt,epochs=1, batch_size=64, print_every=100,training=None, plot_losses=False, isSoftMax=False):\n",
    "        # have tensorflow compute accuracy\n",
    "            \n",
    "        correct_prediction = tf.equal(tf.argmax(self.predict,1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # shuffle indicies\n",
    "        train_indicies = np.arange(Xd.shape[0])\n",
    "        np.random.shuffle(train_indicies)\n",
    "\n",
    "        # setting up variables we want to compute (and optimizing)\n",
    "        # if we have a training function, add that to things we compute\n",
    "        variables = [mean_loss, correct_prediction, accuracy]\n",
    "        variables[-1] = training\n",
    "\n",
    "        # counter \n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            # keep track of losses and accuracy\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            # make sure we iterate over the dataset once\n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xd[idx,:],\n",
    "                             y: yd[idx],\n",
    "                             is_training: True }\n",
    "                # get batch size\n",
    "                actual_batch_size = yd[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "\n",
    "                # aggregate performance stats\n",
    "                losses.append(loss*actual_batch_size)\n",
    "                correct += np.sum(corr)\n",
    "\n",
    "                # print every now and then\n",
    "                if  (iter_cnt % print_every) == 0:\n",
    "                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                iter_cnt += 1\n",
    "\n",
    "            test_indicies = np.arange(Xt.shape[0])\n",
    "            np.random.shuffle(test_indicies)    \n",
    "            correctTest = 0\n",
    "            lossesTest = []\n",
    "            correct_prediction_test = tf.equal(tf.argmax(self.predict,1), y)#tf.nn.softmax(self.predict)\n",
    "            accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32))\n",
    "            variables_test = [mean_loss, correct_prediction_test, accuracy_test]\n",
    "            for i in range(int(math.ceil(Xt.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xt.shape[0]\n",
    "                idx = test_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xt[idx,:],\n",
    "                             y: yt[idx],\n",
    "                             is_training: False }\n",
    "                # get batch size\n",
    "                actual_batch_size = yt[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables_test,feed_dict=feed_dict)\n",
    "            \n",
    "\n",
    "                # aggregate performance stats\n",
    "                lossesTest.append(loss*actual_batch_size)\n",
    "                correctTest += np.sum(corr)\n",
    "\n",
    "                \n",
    "                \n",
    "            total_correct = correct/Xd.shape[0]\n",
    "            total_loss = np.sum(losses)/Xd.shape[0]\n",
    "            total_correct_test = correctTest/Xt.shape[0]\n",
    "            total_loss_test = np.sum(lossesTest)/Xt.shape[0]\n",
    "            print((\"Epoch {2}, Train loss: {0:.3g} and Train accuracy of {1:.3g}\"\n",
    "                  + \", Test loss: {3:.3g} and Test accuracy of {4:.3g}\").format(total_loss,total_correct,e+1,total_loss_test,total_correct_test))                \n",
    "                \n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch loss')\n",
    "                plt.show()\n",
    "        return total_loss, total_correct\n",
    "\n",
    "    \n",
    "    \n",
    "class ModelSVHN():\n",
    "    def __init__(self):\n",
    "       \n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "        self.Wconv1 = tf.get_variable(\"Wconv1\", shape=[5, 5, 3, 48],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv1 = tf.get_variable(\"bconv1\", shape=[48])\n",
    "    \n",
    "    \n",
    "        self.Wconv2 = tf.get_variable(\"Wconv2\", shape=[5, 5, 48, 64],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv2 = tf.get_variable(\"bconv2\", shape=[64])\n",
    "        \n",
    "        self.Wconv3 = tf.get_variable(\"Wconv3\", shape=[5, 5, 64, 128],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv3 = tf.get_variable(\"bconv3\", shape=[128])\n",
    "        \n",
    "        self.Wconv4 = tf.get_variable(\"Wconv4\", shape=[5, 5, 128, 160],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv4 = tf.get_variable(\"bconv4\", shape=[160])\n",
    "     \n",
    "        self.Wconv5 = tf.get_variable(\"Wconv5\", shape=[5, 5, 160, 192],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv5 = tf.get_variable(\"bconv5\", shape=[192])\n",
    "    \n",
    "        self.Wconv6 = tf.get_variable(\"Wconv6\", shape=[5, 5, 192, 192],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv6 = tf.get_variable(\"bconv6\", shape=[192])\n",
    "\n",
    "        self.Wconv7 = tf.get_variable(\"Wconv7\", shape=[5, 5, 192, 192],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv7 = tf.get_variable(\"bconv7\", shape=[192])\n",
    "\n",
    "        self.Wconv8 = tf.get_variable(\"Wconv8\", shape=[5, 5, 192, 192],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv8 = tf.get_variable(\"bconv8\", shape=[192])        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # affine layer with 1024\n",
    "        self.W1 = tf.get_variable(\"W1\", shape=[192, 3072],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b1 = tf.get_variable(\"b1\", shape=[3072])\n",
    "        # affine layer with 10\n",
    "        self.W2 = tf.get_variable(\"W2\", shape=[3072, 10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b2 = tf.get_variable(\"b2\", shape=[10])        \n",
    "        \n",
    "    def forward(self, X, y, is_training):\n",
    "\n",
    "        conv1 = tf.nn.conv2d(X, self.Wconv1, strides=[1, 1, 1, 1], padding='SAME') + self.bconv1\n",
    "        relu1 = tf.nn.relu(conv1)\n",
    "        bN1 = tf.layers.batch_normalization(relu1, training=is_training)\n",
    "        maxpool = tf.layers.max_pooling2d(bN1, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop1 = tf.layers.dropout(inputs=maxpool,rate=0.2, training=is_training)\n",
    "\n",
    "        conv2 = tf.nn.conv2d(drop1, self.Wconv2, strides=[1, 1, 1, 1], padding='SAME') + self.bconv2\n",
    "        relu2 = tf.nn.relu(conv2)\n",
    "        bN2 = tf.layers.batch_normalization(relu2, training=is_training)\n",
    "        maxpool2 = tf.layers.max_pooling2d(bN2, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop2 = tf.layers.dropout(inputs=maxpool2,rate=0.2, training=is_training)\n",
    "\n",
    "        conv3 = tf.nn.conv2d(drop2, self.Wconv3, strides=[1, 1, 1, 1], padding='SAME') + self.bconv3\n",
    "        relu3 = tf.nn.relu(conv3)\n",
    "        bN3 = tf.layers.batch_normalization(relu3, training=is_training)\n",
    "        maxpool3 = tf.layers.max_pooling2d(bN3, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop3 = tf.layers.dropout(inputs=maxpool3,rate=0.2, training=is_training)\n",
    "\n",
    "        conv4 = tf.nn.conv2d(drop3, self.Wconv4, strides=[1, 1, 1, 1], padding='SAME') + self.bconv4\n",
    "        relu4 = tf.nn.relu(conv4)\n",
    "        bN4 = tf.layers.batch_normalization(relu4, training=is_training)\n",
    "        maxpool4 = tf.layers.max_pooling2d(bN4, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop4 = tf.layers.dropout(inputs=maxpool4,rate=0.2, training=is_training)\n",
    "\n",
    "        conv5 = tf.nn.conv2d(drop4, self.Wconv5, strides=[1, 1, 1, 1], padding='SAME') + self.bconv5\n",
    "        relu5 = tf.nn.relu(conv5)\n",
    "        bN5 = tf.layers.batch_normalization(relu5, training=is_training)\n",
    "        maxpool5 = tf.layers.max_pooling2d(bN5, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop5 = tf.layers.dropout(inputs=maxpool5,rate=0.2, training=is_training)\n",
    "\n",
    "        conv6 = tf.nn.conv2d(drop5, self.Wconv6, strides=[1, 1, 1, 1], padding='SAME') + self.bconv6\n",
    "        relu6 = tf.nn.relu(conv6)\n",
    "        bN6 = tf.layers.batch_normalization(relu6, training=is_training)\n",
    "        maxpool6 = tf.layers.max_pooling2d(bN6, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop6 = tf.layers.dropout(inputs=maxpool6,rate=0.2, training=is_training)\n",
    "\n",
    "        conv7 = tf.nn.conv2d(drop6, self.Wconv7, strides=[1, 1, 1, 1], padding='SAME') + self.bconv7\n",
    "        relu7 = tf.nn.relu(conv7)\n",
    "        bN7 = tf.layers.batch_normalization(relu7, training=is_training)\n",
    "        maxpool7 = tf.layers.max_pooling2d(bN7, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop7 = tf.layers.dropout(inputs=maxpool7,rate=0.2, training=is_training)\n",
    "\n",
    "        conv8 = tf.nn.conv2d(drop7, self.Wconv8, strides=[1, 1, 1, 1], padding='SAME') + self.bconv8\n",
    "        relu8 = tf.nn.relu(conv8)\n",
    "        bN8 = tf.layers.batch_normalization(relu8, training=is_training)\n",
    "        maxpool8 = tf.layers.max_pooling2d(bN8, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop8 = tf.layers.dropout(inputs=maxpool8,rate=0.2, training=is_training)\n",
    "\n",
    "        maxpool_flat = tf.reshape(drop8,[-1,192])\n",
    "\n",
    "        affine1 = tf.matmul(maxpool_flat, self.W1) + self.b1\n",
    "\n",
    "\n",
    "        # ReLU Activation Layer\n",
    "        relu9= tf.nn.relu(affine1)\n",
    "\n",
    "        # dropout\n",
    "        drop9 = tf.layers.dropout(inputs=relu9, training=is_training)\n",
    "\n",
    "        # Affine layer from 3072 input units to 10 outputs\n",
    "        affine2 = tf.matmul(drop9, self.W2) + self.b2\n",
    "\n",
    "        self.predict = affine2\n",
    "\n",
    "        return self.predict\n",
    "\n",
    "    def run(self, session, loss_val, Xd, yd,Xt,yt,epochs=1, batch_size=64, print_every=100,training=None, plot_losses=False, isSoftMax=False):\n",
    "        # have tensorflow compute accuracy\n",
    "            \n",
    "        correct_prediction = tf.equal(tf.argmax(self.predict,1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # shuffle indicies\n",
    "        train_indicies = np.arange(Xd.shape[0])\n",
    "        np.random.shuffle(train_indicies)\n",
    "\n",
    "        # setting up variables we want to compute (and optimizing)\n",
    "        # if we have a training function, add that to things we compute\n",
    "        variables = [mean_loss, correct_prediction, accuracy]\n",
    "        variables[-1] = training\n",
    "\n",
    "        # counter \n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            # keep track of losses and accuracy\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            # make sure we iterate over the dataset once\n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xd[idx,:],\n",
    "                             y: yd[idx],\n",
    "                             is_training: True }\n",
    "                # get batch size\n",
    "                actual_batch_size = yd[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "\n",
    "                # aggregate performance stats\n",
    "                losses.append(loss*actual_batch_size)\n",
    "                correct += np.sum(corr)\n",
    "\n",
    "                # print every now and then\n",
    "                if  (iter_cnt % print_every) == 0:\n",
    "                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                iter_cnt += 1\n",
    "\n",
    "            test_indicies = np.arange(Xt.shape[0])\n",
    "            np.random.shuffle(test_indicies)    \n",
    "            correctTest = 0\n",
    "            lossesTest = []\n",
    "            correct_prediction_test = tf.equal(tf.argmax(self.predict,1), y)#tf.nn.softmax(self.predict)\n",
    "            accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32))\n",
    "            variables_test = [mean_loss, correct_prediction_test, accuracy_test]\n",
    "            for i in range(int(math.ceil(Xt.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xt.shape[0]\n",
    "                idx = test_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xt[idx,:],\n",
    "                             y: yt[idx],\n",
    "                             is_training: False }\n",
    "                # get batch size\n",
    "                actual_batch_size = yt[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables_test,feed_dict=feed_dict)\n",
    "            \n",
    "\n",
    "                # aggregate performance stats\n",
    "                lossesTest.append(loss*actual_batch_size)\n",
    "                correctTest += np.sum(corr)\n",
    "\n",
    "                \n",
    "                \n",
    "            total_correct = correct/Xd.shape[0]\n",
    "            total_loss = np.sum(losses)/Xd.shape[0]\n",
    "            total_correct_test = correctTest/Xt.shape[0]\n",
    "            total_loss_test = np.sum(lossesTest)/Xt.shape[0]\n",
    "            print((\"Epoch {2}, Train loss: {0:.3g} and Train accuracy of {1:.3g}\"\n",
    "                  + \", Test loss: {3:.3g} and Test accuracy of {4:.3g}\").format(total_loss,total_correct,e+1,total_loss_test,total_correct_test))                \n",
    "                \n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch loss')\n",
    "                plt.show()\n",
    "        return total_loss, total_correct\n",
    "\n",
    "\n",
    "\n",
    "class ModelCIFAR10():\n",
    "    def __init__(self):\n",
    "       \n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "        self.Wconv1 = tf.get_variable(\"Wconv1\", shape=[3, 3, 3, 32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x64 = 50176\n",
    "        self.Wconv2 = tf.get_variable(\"Wconv2\", shape=[3, 3, 32, 32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv2 = tf.get_variable(\"bconv2\", shape=[32])\n",
    "        \n",
    "        self.Wconv3 = tf.get_variable(\"Wconv3\", shape=[3, 3, 32, 64],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv3 = tf.get_variable(\"bconv3\", shape=[64])\n",
    "        \n",
    "        self.Wconv4 = tf.get_variable(\"Wconv4\", shape=[3, 3, 64, 64],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv4 = tf.get_variable(\"bconv4\", shape=[64])\n",
    "        \n",
    "        # affine layer with 1024\n",
    "        self.W1 = tf.get_variable(\"W1\", shape=[2304, 512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b1 = tf.get_variable(\"b1\", shape=[512])\n",
    "        # affine layer with 10\n",
    "        self.W2 = tf.get_variable(\"W2\", shape=[512, 10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b2 = tf.get_variable(\"b2\", shape=[10])        \n",
    "        \n",
    "    def forward(self, X, y, is_training):\n",
    "        \n",
    "        conv1 = tf.nn.conv2d(X, self.Wconv1, strides=[1, 1, 1, 1], padding='SAME') + self.bconv1\n",
    "        relu1 = tf.nn.relu(conv1)\n",
    "        # Conv\n",
    "        conv2 = tf.nn.conv2d(relu1, self.Wconv2, strides=[1, 1, 1, 1], padding='VALID') + self.bconv2\n",
    "        relu2 = tf.nn.relu(conv2)\n",
    "        \n",
    "        maxpool = tf.layers.max_pooling2d(relu2, pool_size=(2,2),strides=2)\n",
    "        \n",
    "        \n",
    "        drop1 = tf.layers.dropout(inputs=maxpool,rate=0.25, training=is_training)\n",
    "        \n",
    "        conv3 = tf.nn.conv2d(drop1, self.Wconv3, strides=[1, 1, 1, 1], padding='SAME') + self.bconv3\n",
    "        relu3 = tf.nn.relu(conv3)\n",
    "        \n",
    "        conv4 = tf.nn.conv2d(relu3, self.Wconv4, strides=[1, 1, 1, 1], padding='VALID') + self.bconv4\n",
    "        relu4 = tf.nn.relu(conv4)\n",
    "        \n",
    "        maxpool2 = tf.layers.max_pooling2d(relu4, pool_size=(2,2),strides=2,padding=\"VALID\")\n",
    "        \n",
    "        \n",
    "        drop2 = tf.layers.dropout(inputs=maxpool2,rate=0.25, training=is_training)\n",
    "        \n",
    "        maxpool_flat = tf.reshape(drop2,[-1,2304])\n",
    "        affine1 = tf.matmul(maxpool_flat, self.W1) + self.b1\n",
    "        \n",
    "     \n",
    "        # ReLU Activation Layer\n",
    "        relu2 = tf.nn.relu(affine1)\n",
    "        \n",
    "        # dropout\n",
    "        drop1 = tf.layers.dropout(inputs=relu2,rate=0.5, training=is_training)\n",
    "        \n",
    "        # Affine layer from 1024 input units to 10 outputs\n",
    "        affine2 = tf.matmul(drop1, self.W2) + self.b2\n",
    "           \n",
    "        self.predict = affine2\n",
    "        return self.predict\n",
    "    \n",
    "    def run(self, session, loss_val, Xd, yd,Xt,yt,epochs=1, batch_size=64, print_every=100,training=None, plot_losses=False, isSoftMax=False):\n",
    "        # have tensorflow compute accuracy\n",
    "            \n",
    "        correct_prediction = tf.equal(tf.argmax(self.predict,1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # shuffle indicies\n",
    "        train_indicies = np.arange(Xd.shape[0])\n",
    "        np.random.shuffle(train_indicies)\n",
    "\n",
    "        # setting up variables we want to compute (and optimizing)\n",
    "        # if we have a training function, add that to things we compute\n",
    "        variables = [mean_loss, correct_prediction, accuracy]\n",
    "        variables[-1] = training\n",
    "\n",
    "        # counter \n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            # keep track of losses and accuracy\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            # make sure we iterate over the dataset once\n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xd[idx,:],\n",
    "                             y: yd[idx],\n",
    "                             is_training: True }\n",
    "                # get batch size\n",
    "                actual_batch_size = yd[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "\n",
    "                # aggregate performance stats\n",
    "                losses.append(loss*actual_batch_size)\n",
    "                correct += np.sum(corr)\n",
    "\n",
    "                # print every now and then\n",
    "                if  (iter_cnt % print_every) == 0:\n",
    "                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                iter_cnt += 1\n",
    "\n",
    "            test_indicies = np.arange(Xt.shape[0])\n",
    "            np.random.shuffle(test_indicies)    \n",
    "            correctTest = 0\n",
    "            lossesTest = []\n",
    "            correct_prediction_test = tf.equal(tf.argmax(self.predict,1), y)#tf.nn.softmax(self.predict)\n",
    "            accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32))\n",
    "            variables_test = [mean_loss, correct_prediction_test, accuracy_test]\n",
    "            for i in range(int(math.ceil(Xt.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xt.shape[0]\n",
    "                idx = test_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xt[idx,:],\n",
    "                             y: yt[idx],\n",
    "                             is_training: False }\n",
    "                # get batch size\n",
    "                actual_batch_size = yt[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables_test,feed_dict=feed_dict)\n",
    "            \n",
    "\n",
    "                # aggregate performance stats\n",
    "                lossesTest.append(loss*actual_batch_size)\n",
    "                correctTest += np.sum(corr)\n",
    "\n",
    "                \n",
    "                \n",
    "            total_correct = correct/Xd.shape[0]\n",
    "            total_loss = np.sum(losses)/Xd.shape[0]\n",
    "            total_correct_test = correctTest/Xt.shape[0]\n",
    "            total_loss_test = np.sum(lossesTest)/Xt.shape[0]\n",
    "            print((\"Epoch {2}, Train loss: {0:.3g} and Train accuracy of {1:.3g}\"\n",
    "                  + \", Test loss: {3:.3g} and Test accuracy of {4:.3g}\").format(total_loss,total_correct,e+1,total_loss_test,total_correct_test))                \n",
    "                \n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch loss')\n",
    "                plt.show()\n",
    "        return total_loss, total_correct\n",
    "\n",
    "class ModelCIFAR100():\n",
    "    def __init__(self):\n",
    "       \n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "        self.Wconv1 = tf.get_variable(\"Wconv1\", shape=[3, 3, 3, 128],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv1 = tf.get_variable(\"bconv1\", shape=[128])\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x64 = 50176\n",
    "        self.Wconv2 = tf.get_variable(\"Wconv2\", shape=[3, 3, 128, 128],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv2 = tf.get_variable(\"bconv2\", shape=[128])\n",
    "        \n",
    "        self.Wconv3 = tf.get_variable(\"Wconv3\", shape=[3, 3, 128, 256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv3 = tf.get_variable(\"bconv3\", shape=[256])\n",
    "        \n",
    "        self.Wconv4 = tf.get_variable(\"Wconv4\", shape=[3, 3, 256, 256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv4 = tf.get_variable(\"bconv4\", shape=[256])\n",
    "        \n",
    "        \n",
    "        self.Wconv5 = tf.get_variable(\"Wconv5\", shape=[3, 3, 256, 512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv5 = tf.get_variable(\"bconv5\", shape=[512])\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.Wconv6 = tf.get_variable(\"Wconv6\", shape=[3, 3, 512, 512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv6 = tf.get_variable(\"bconv6\", shape=[512])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # affine layer with 1024\n",
    "        self.W1 = tf.get_variable(\"W1\", shape=[2048, 1024],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b1 = tf.get_variable(\"b1\", shape=[1024])\n",
    "        # affine layer with 10\n",
    "        self.W2 = tf.get_variable(\"W2\", shape=[1024, 100],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b2 = tf.get_variable(\"b2\", shape=[100])        \n",
    "        \n",
    "    def forward(self, X, y, is_training):\n",
    "\n",
    "        conv1 = tf.nn.conv2d(X, self.Wconv1, strides=[1, 1, 1, 1], padding='SAME') + self.bconv1\n",
    "        relu1 = tf.nn.relu(conv1)\n",
    "        print(relu1.shape)\n",
    "        # Conv\n",
    "        conv2 = tf.nn.conv2d(relu1, self.Wconv2, strides=[1, 1, 1, 1], padding='VALID') + self.bconv2\n",
    "        relu2 = tf.nn.relu(conv2)\n",
    "        print(relu2.shape)\n",
    "        \n",
    "        maxpool = tf.layers.max_pooling2d(relu2, pool_size=(2,2),strides=2)\n",
    "        drop1 = tf.layers.dropout(inputs=maxpool, training=is_training,rate=0.1)\n",
    "        print(maxpool.shape)\n",
    "        \n",
    "        conv3 = tf.nn.conv2d(drop1, self.Wconv3, strides=[1, 1, 1, 1], padding='SAME') + self.bconv3\n",
    "        relu3 = tf.nn.relu(conv3)\n",
    "        print(relu3.shape)\n",
    "        \n",
    "        conv4 = tf.nn.conv2d(relu3, self.Wconv4, strides=[1, 1, 1, 1], padding='VALID') + self.bconv4\n",
    "        relu4 = tf.nn.relu(conv4)\n",
    "        print(relu4.shape)\n",
    "        \n",
    "        maxpool2 = tf.layers.max_pooling2d(relu4, pool_size=(2,2),strides=2)\n",
    "        drop2 = tf.layers.dropout(inputs=maxpool2, training=is_training,rate=0.25)\n",
    "        print(maxpool2.shape)\n",
    "        \n",
    "        \n",
    "        conv5 = tf.nn.conv2d(drop2, self.Wconv5, strides=[1, 1, 1, 1], padding='SAME') + self.bconv5\n",
    "        relu5 = tf.nn.relu(conv5)\n",
    "        print(relu5.shape)\n",
    "        \n",
    "        conv6 = tf.nn.conv2d(relu5, self.Wconv6, strides=[1, 1, 1, 1], padding='VALID') + self.bconv6\n",
    "        relu6 = tf.nn.relu(conv6)\n",
    "        print(relu6.shape)\n",
    "        \n",
    "        \n",
    "        maxpool3 = tf.layers.max_pooling2d(relu6, pool_size=(2,2),strides=2)\n",
    "        drop3 = tf.layers.dropout(inputs=maxpool3, training=is_training,rate=0.5)\n",
    "        print(maxpool3.shape)\n",
    "        \n",
    "        \n",
    "        maxpool_flat = tf.reshape(drop3,[-1,2048])\n",
    "\n",
    "        affine1 = tf.matmul(maxpool_flat, self.W1) + self.b1\n",
    "        \n",
    "     \n",
    "        # ReLU Activation Layer\n",
    "        relu7 = tf.nn.relu(affine1)\n",
    "        \n",
    "        # dropout\n",
    "        drop4 = tf.layers.dropout(inputs=relu7, training=is_training,rate=0.5)\n",
    "        \n",
    "        # Affine layer from 1024 input units to 10 outputs\n",
    "        affine2 = tf.matmul(drop4, self.W2) + self.b2\n",
    "        \n",
    "   \n",
    "        \n",
    "        self.predict = affine2#tf.layers.batch_normalization(inputs=affine2, center=True, scale=True, training=is_training)\n",
    "        \n",
    "        return self.predict\n",
    "    \n",
    "    def run(self, session, loss_val, Xd, yd,Xt,yt,epochs=1, batch_size=64, print_every=100,training=None, plot_losses=False, isSoftMax=False):\n",
    "        # have tensorflow compute accuracy\n",
    "            \n",
    "        correct_prediction = tf.equal(tf.argmax(self.predict,1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # shuffle indicies\n",
    "        train_indicies = np.arange(Xd.shape[0])\n",
    "        np.random.shuffle(train_indicies)\n",
    "\n",
    "        # setting up variables we want to compute (and optimizing)\n",
    "        # if we have a training function, add that to things we compute\n",
    "        variables = [mean_loss, correct_prediction, accuracy]\n",
    "        variables[-1] = training\n",
    "\n",
    "        # counter \n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            # keep track of losses and accuracy\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            # make sure we iterate over the dataset once\n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xd[idx,:],\n",
    "                             y: yd[idx],\n",
    "                             is_training: True }\n",
    "                # get batch size\n",
    "                actual_batch_size = yd[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "\n",
    "                # aggregate performance stats\n",
    "                losses.append(loss*actual_batch_size)\n",
    "                correct += np.sum(corr)\n",
    "\n",
    "                # print every now and then\n",
    "                if  (iter_cnt % print_every) == 0:\n",
    "                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                iter_cnt += 1\n",
    "\n",
    "            test_indicies = np.arange(Xt.shape[0])\n",
    "            np.random.shuffle(test_indicies)    \n",
    "            correctTest = 0\n",
    "            lossesTest = []\n",
    "            correct_prediction_test = tf.equal(tf.argmax(self.predict,1), y)#tf.nn.softmax(self.predict)\n",
    "            accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32))\n",
    "            variables_test = [mean_loss, correct_prediction_test, accuracy_test]\n",
    "            for i in range(int(math.ceil(Xt.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xt.shape[0]\n",
    "                idx = test_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xt[idx,:],\n",
    "                             y: yt[idx],\n",
    "                             is_training: False }\n",
    "                # get batch size\n",
    "                actual_batch_size = yt[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables_test,feed_dict=feed_dict)\n",
    "            \n",
    "\n",
    "                # aggregate performance stats\n",
    "                lossesTest.append(loss*actual_batch_size)\n",
    "                correctTest += np.sum(corr)\n",
    "\n",
    "                \n",
    "                \n",
    "            total_correct = correct/Xd.shape[0]\n",
    "            total_loss = np.sum(losses)/Xd.shape[0]\n",
    "            total_correct_test = correctTest/Xt.shape[0]\n",
    "            total_loss_test = np.sum(lossesTest)/Xt.shape[0]\n",
    "            print((\"Epoch {2}, Train loss: {0:.3g} and Train accuracy of {1:.3g}\"\n",
    "                  + \", Test loss: {3:.3g} and Test accuracy of {4:.3g}\").format(total_loss,total_correct,e+1,total_loss_test,total_correct_test))                \n",
    "                \n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch loss')\n",
    "                plt.show()\n",
    "        return total_loss, total_correct\n",
    "    \n",
    "\n",
    "def RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = loadDataMNIST()\n",
    "    tf.reset_default_graph()\n",
    "    global X\n",
    "    global y\n",
    "    global mean_loss\n",
    "    global is_training\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    net = ModelMNIST()\n",
    "    net.forward(X,y,is_training)\n",
    "    \n",
    "    \n",
    "    # Annealing the learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Feel free to play with this cell\n",
    "    mean_loss = None\n",
    "    optimizer = None\n",
    "    \n",
    "    # define our loss\n",
    "    cross_entr_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=net.predict)\n",
    "    l2_loss = weightDecay * tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()])\n",
    "    mean_loss = tf.reduce_mean(cross_entr_loss + l2_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # define our optimizer\n",
    "    optimizer = tf.train.MomentumOptimizer(learningRate,momentum=momentum)\n",
    "    \n",
    "    \n",
    "    # batch normalization in tensorflow requires this extra dependency\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_step = optimizer.minimize(mean_loss, global_step=global_step)\n",
    "        \n",
    "        \n",
    "    # train with 10 epochs\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Tensorflow\",\"MNIST\")    \n",
    "    start = time.time()\n",
    "    try:\n",
    "        with tf.device(\"/cpu:0\") as dev:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Training')\n",
    "            net.run(sess, mean_loss, X_train, y_train,X_test, y_test, epochs, batchSize, batchSize, train_step, False)\n",
    "            \n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    YN.EndLogger(memT,cpuT,gpuT)   \n",
    "\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    X_train,y_train ,X_test,y_test = loadDataCIFAR10()\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    global X\n",
    "    global y\n",
    "    global mean_loss\n",
    "    global is_training\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    net = ModelCIFAR10()\n",
    "    net.forward(X,y,is_training)\n",
    "    \n",
    "    \n",
    "    # Annealing the learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Feel free to play with this cell\n",
    "    mean_loss = None\n",
    "    optimizer = None\n",
    "    \n",
    "    # define our loss\n",
    "    cross_entr_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=net.predict)\n",
    "    l2_loss = weightDecay * tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()])\n",
    "    mean_loss = tf.reduce_mean(cross_entr_loss + l2_loss)\n",
    "    \n",
    "    # define our optimizer\n",
    "    optimizer = tf.train.MomentumOptimizer(learningRate,momentum=momentum)\n",
    "    \n",
    "    \n",
    "    # batch normalization in tensorflow requires this extra dependency\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_step = optimizer.minimize(mean_loss, global_step=global_step)\n",
    "        \n",
    "        \n",
    "    # train with 10 epochs\n",
    "    sess = tf.Session()\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Tensorflow\",\"CIFAR10\")\n",
    "    start = time.time()    \n",
    "    try:\n",
    "        with tf.device(\"/cpu:0\") as dev:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Training')\n",
    "            net.run(sess, mean_loss, X_train, y_train,X_test, y_test, epochs, batchSize, batchSize, train_step, False)\n",
    "            \n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")\n",
    "    end = time.time()\n",
    "    \n",
    "    YN.EndLogger(memT,cpuT,gpuT)   \n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "def RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    X_train,y_train ,X_test,y_test =  loadDataCIFAR100()\n",
    "    print(\"####################\")\n",
    "    tf.reset_default_graph()\n",
    "    global X\n",
    "    global y\n",
    "    global mean_loss\n",
    "    global is_training\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    net = ModelCIFAR100()\n",
    "    net.forward(X,y,is_training)\n",
    "    \n",
    "    \n",
    "    # Annealing the learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Feel free to play with this cell\n",
    "    mean_loss = None\n",
    "    optimizer = None\n",
    "    \n",
    "    # define our loss\n",
    "    cross_entr_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=net.predict)\n",
    "    l2_loss = weightDecay * tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()])\n",
    "    mean_loss = tf.reduce_mean(cross_entr_loss + l2_loss)\n",
    "    \n",
    "    # define our optimizer\n",
    "    optimizer = tf.train.MomentumOptimizer(learningRate,momentum=momentum)\n",
    "    \n",
    "    \n",
    "    # batch normalization in tensorflow requires this extra dependency\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_step = optimizer.minimize(mean_loss, global_step=global_step)\n",
    "        \n",
    "        \n",
    "    # train with 100 epochs\n",
    "    sess = tf.Session()\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Tensorflow\",\"CIFAR100\")\n",
    "    start = time.time()    \n",
    "    try:\n",
    "        with tf.device(\"/cpu:0\") as dev:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Training')\n",
    "            net.run(sess, mean_loss, X_train, y_train,X_test, y_test, epochs, batchSize, batchSize, train_step, False)\n",
    "            \n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")\n",
    "    end = time.time()\n",
    "    YN.EndLogger(memT,cpuT,gpuT)   \n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "\n",
    "def RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay,fname):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    X_train,y_train,X_test,y_test = loadDataSVHN(fname,True)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    global X\n",
    "    global y\n",
    "    global mean_loss\n",
    "    global is_training\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    net = ModelSVHN()\n",
    "    net.forward(X,y,is_training)\n",
    "    \n",
    "    \n",
    "    # Annealing the learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Feel free to play with this cell\n",
    "    mean_loss = None\n",
    "    optimizer = None\n",
    "    \n",
    "    # define our loss\n",
    "    cross_entr_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=net.predict)\n",
    "    l2_loss = weightDecay * tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()])\n",
    "    mean_loss = tf.reduce_mean(cross_entr_loss + l2_loss)\n",
    "    \n",
    "    # define our optimizer\n",
    "    optimizer = tf.train.MomentumOptimizer(learningRate,momentum=momentum)\n",
    "    \n",
    "    \n",
    "    # batch normalization in tensorflow requires this extra dependency\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_step = optimizer.minimize(mean_loss, global_step=global_step)\n",
    "        \n",
    "        \n",
    "    # train with 10 epochs\n",
    "    sess = tf.Session()\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Tensorflow\",\"SVHN\")\n",
    "    start = time.time()    \n",
    "    try:\n",
    "        with tf.device(\"/cpu:0\") as dev:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Training')\n",
    "            net.run(sess, mean_loss, X_train, y_train,X_test, y_test, epochs, batchSize, batchSize, train_step, False)\n",
    "            \n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")\n",
    "    end = time.time()\n",
    "    \n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "def runModel(dataset,batchSize=128,numClasses=10,epochs=12,learningRate=0.01,momentum=0.5,weightDecay=1e-6):\n",
    "    if dataset is \"mnist\":\n",
    "        RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar10\":\n",
    "        RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar100\":\n",
    "        RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"SVHN\":\n",
    "        fname = './%s_32x32.mat'\n",
    "        RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay,fname) \n",
    "    else:\n",
    "        print(\"Choose cifar10 or mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#    runModel(\"mnist\",epochs=15)\n",
    "#    runModel(\"cifar10\",epochs=100)\n",
    "#    runModel(\"SVHN\",epochs=100)\n",
    "    runModel(\"cifar100\",epochs=200)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(50000, 32, 32, 3)\n",
      "(50000,)\n",
      "(10000, 32, 32, 3)\n",
      "(10000,)\n",
      "####################\n",
      "(?, 32, 32, 128)\n",
      "(?, 30, 30, 128)\n",
      "(?, 15, 15, 128)\n",
      "(?, 15, 15, 256)\n",
      "(?, 13, 13, 256)\n",
      "(?, 6, 6, 256)\n",
      "(?, 6, 6, 512)\n",
      "(?, 4, 4, 512)\n",
      "(?, 2, 2, 512)\n",
      "Training\n",
      "Iteration 0: with minibatch training loss = 4.63 and accuracy of 0.0078\n",
      "Iteration 128: with minibatch training loss = 4.63 and accuracy of 0.0078\n",
      "Iteration 256: with minibatch training loss = 4.6 and accuracy of 0.023\n",
      "Iteration 384: with minibatch training loss = 4.61 and accuracy of 0\n",
      "Epoch 1, Train loss: 4.61 and Train accuracy of 0.00984, Test loss: 4.61 and Test accuracy of 0.01\n",
      "Iteration 512: with minibatch training loss = 4.62 and accuracy of 0.0078\n",
      "Iteration 640: with minibatch training loss = 4.6 and accuracy of 0\n",
      "Iteration 768: with minibatch training loss = 4.61 and accuracy of 0.016\n",
      "Epoch 2, Train loss: 4.61 and Train accuracy of 0.0103, Test loss: 4.6 and Test accuracy of 0.0165\n",
      "Iteration 896: with minibatch training loss = 4.6 and accuracy of 0\n",
      "Iteration 1024: with minibatch training loss = 4.59 and accuracy of 0.016\n",
      "Iteration 1152: with minibatch training loss = 4.59 and accuracy of 0.0078\n",
      "Epoch 3, Train loss: 4.6 and Train accuracy of 0.0125, Test loss: 4.58 and Test accuracy of 0.0187\n",
      "Iteration 1280: with minibatch training loss = 4.59 and accuracy of 0.0078\n",
      "Iteration 1408: with minibatch training loss = 4.56 and accuracy of 0.0078\n",
      "Iteration 1536: with minibatch training loss = 4.53 and accuracy of 0\n",
      "Epoch 4, Train loss: 4.56 and Train accuracy of 0.0206, Test loss: 4.51 and Test accuracy of 0.0317\n",
      "Iteration 1664: with minibatch training loss = 4.52 and accuracy of 0.039\n",
      "Iteration 1792: with minibatch training loss = 4.48 and accuracy of 0.023\n",
      "Iteration 1920: with minibatch training loss = 4.49 and accuracy of 0.0078\n",
      "Epoch 5, Train loss: 4.51 and Train accuracy of 0.0263, Test loss: 4.41 and Test accuracy of 0.0383\n",
      "Iteration 2048: with minibatch training loss = 4.42 and accuracy of 0.031\n",
      "Iteration 2176: with minibatch training loss = 4.36 and accuracy of 0.016\n",
      "Iteration 2304: with minibatch training loss = 4.38 and accuracy of 0.016\n",
      "Epoch 6, Train loss: 4.39 and Train accuracy of 0.0353, Test loss: 4.25 and Test accuracy of 0.0546\n",
      "Iteration 2432: with minibatch training loss = 4.22 and accuracy of 0.031\n",
      "Iteration 2560: with minibatch training loss = 4.3 and accuracy of 0.047\n",
      "Iteration 2688: with minibatch training loss = 4.34 and accuracy of 0.031\n",
      "Epoch 7, Train loss: 4.28 and Train accuracy of 0.0452, Test loss: 4.15 and Test accuracy of 0.073\n",
      "Iteration 2816: with minibatch training loss = 4.28 and accuracy of 0.047\n",
      "Iteration 2944: with minibatch training loss = 4.28 and accuracy of 0.047\n",
      "Iteration 3072: with minibatch training loss = 4.19 and accuracy of 0.055\n",
      "Epoch 8, Train loss: 4.2 and Train accuracy of 0.0588, Test loss: 4.06 and Test accuracy of 0.0866\n",
      "Iteration 3200: with minibatch training loss = 4.22 and accuracy of 0.055\n",
      "Iteration 3328: with minibatch training loss = 3.93 and accuracy of 0.094\n",
      "Iteration 3456: with minibatch training loss = 4.06 and accuracy of 0.062\n",
      "Epoch 9, Train loss: 4.13 and Train accuracy of 0.0669, Test loss: 3.99 and Test accuracy of 0.0952\n",
      "Iteration 3584: with minibatch training loss = 3.94 and accuracy of 0.094\n",
      "Iteration 3712: with minibatch training loss = 3.99 and accuracy of 0.12\n",
      "Iteration 3840: with minibatch training loss = 4.11 and accuracy of 0.078\n",
      "Epoch 10, Train loss: 4.07 and Train accuracy of 0.0755, Test loss: 3.97 and Test accuracy of 0.0984\n",
      "Iteration 3968: with minibatch training loss = 4.01 and accuracy of 0.086\n",
      "Iteration 4096: with minibatch training loss = 4.06 and accuracy of 0.039\n",
      "Iteration 4224: with minibatch training loss = 4.13 and accuracy of 0.078\n",
      "Epoch 11, Train loss: 4.01 and Train accuracy of 0.0824, Test loss: 3.88 and Test accuracy of 0.115\n",
      "Iteration 4352: with minibatch training loss = 3.98 and accuracy of 0.094\n",
      "Iteration 4480: with minibatch training loss = 3.86 and accuracy of 0.086\n",
      "Iteration 4608: with minibatch training loss = 4.01 and accuracy of 0.086\n",
      "Epoch 12, Train loss: 3.95 and Train accuracy of 0.093, Test loss: 3.82 and Test accuracy of 0.121\n",
      "Iteration 4736: with minibatch training loss = 4.11 and accuracy of 0.094\n",
      "Iteration 4864: with minibatch training loss = 3.71 and accuracy of 0.094\n",
      "Iteration 4992: with minibatch training loss = 3.75 and accuracy of 0.12\n",
      "Epoch 13, Train loss: 3.9 and Train accuracy of 0.1, Test loss: 3.76 and Test accuracy of 0.134\n",
      "Iteration 5120: with minibatch training loss = 3.89 and accuracy of 0.094\n",
      "Iteration 5248: with minibatch training loss = 3.96 and accuracy of 0.13\n",
      "Iteration 5376: with minibatch training loss = 3.93 and accuracy of 0.11\n",
      "Epoch 14, Train loss: 3.84 and Train accuracy of 0.108, Test loss: 3.7 and Test accuracy of 0.142\n",
      "Iteration 5504: with minibatch training loss = 3.82 and accuracy of 0.094\n",
      "Iteration 5632: with minibatch training loss = 3.74 and accuracy of 0.1\n",
      "Iteration 5760: with minibatch training loss = 3.85 and accuracy of 0.07\n",
      "Epoch 15, Train loss: 3.79 and Train accuracy of 0.118, Test loss: 3.64 and Test accuracy of 0.151\n",
      "Iteration 5888: with minibatch training loss = 3.8 and accuracy of 0.078\n",
      "Iteration 6016: with minibatch training loss = 3.64 and accuracy of 0.19\n",
      "Iteration 6144: with minibatch training loss = 3.53 and accuracy of 0.16\n",
      "Epoch 16, Train loss: 3.75 and Train accuracy of 0.122, Test loss: 3.62 and Test accuracy of 0.157\n",
      "Iteration 6272: with minibatch training loss = 3.69 and accuracy of 0.086\n",
      "Iteration 6400: with minibatch training loss = 3.72 and accuracy of 0.1\n",
      "Iteration 6528: with minibatch training loss = 3.69 and accuracy of 0.16\n",
      "Epoch 17, Train loss: 3.71 and Train accuracy of 0.128, Test loss: 3.56 and Test accuracy of 0.168\n",
      "Iteration 6656: with minibatch training loss = 3.65 and accuracy of 0.1\n",
      "Iteration 6784: with minibatch training loss = 3.72 and accuracy of 0.12\n",
      "Iteration 6912: with minibatch training loss = 3.69 and accuracy of 0.12\n",
      "Epoch 18, Train loss: 3.67 and Train accuracy of 0.136, Test loss: 3.52 and Test accuracy of 0.172\n",
      "Iteration 7040: with minibatch training loss = 3.53 and accuracy of 0.15\n",
      "Iteration 7168: with minibatch training loss = 3.57 and accuracy of 0.15\n",
      "Iteration 7296: with minibatch training loss = 3.61 and accuracy of 0.16\n",
      "Iteration 7424: with minibatch training loss = 3.74 and accuracy of 0.13\n",
      "Epoch 19, Train loss: 3.63 and Train accuracy of 0.142, Test loss: 3.48 and Test accuracy of 0.178\n",
      "Iteration 7552: with minibatch training loss = 3.58 and accuracy of 0.11\n",
      "Iteration 7680: with minibatch training loss = 3.5 and accuracy of 0.2\n",
      "Iteration 7808: with minibatch training loss = 3.64 and accuracy of 0.16\n",
      "Epoch 20, Train loss: 3.59 and Train accuracy of 0.149, Test loss: 3.43 and Test accuracy of 0.191\n",
      "Iteration 7936: with minibatch training loss = 3.56 and accuracy of 0.14\n",
      "Iteration 8064: with minibatch training loss = 3.43 and accuracy of 0.17\n",
      "Iteration 8192: with minibatch training loss = 3.51 and accuracy of 0.16\n",
      "Epoch 21, Train loss: 3.55 and Train accuracy of 0.158, Test loss: 3.4 and Test accuracy of 0.196\n",
      "Iteration 8320: with minibatch training loss = 3.73 and accuracy of 0.14\n",
      "Iteration 8448: with minibatch training loss = 3.59 and accuracy of 0.1\n",
      "Iteration 8576: with minibatch training loss = 3.64 and accuracy of 0.15\n",
      "Epoch 22, Train loss: 3.51 and Train accuracy of 0.163, Test loss: 3.37 and Test accuracy of 0.202\n",
      "Iteration 8704: with minibatch training loss = 3.46 and accuracy of 0.17\n",
      "Iteration 8832: with minibatch training loss = 3.41 and accuracy of 0.21\n",
      "Iteration 8960: with minibatch training loss = 3.64 and accuracy of 0.13\n",
      "Epoch 23, Train loss: 3.48 and Train accuracy of 0.169, Test loss: 3.34 and Test accuracy of 0.206\n",
      "Iteration 9088: with minibatch training loss = 3.32 and accuracy of 0.17\n",
      "Iteration 9216: with minibatch training loss = 3.4 and accuracy of 0.21\n",
      "Iteration 9344: with minibatch training loss = 3.13 and accuracy of 0.22\n",
      "Epoch 24, Train loss: 3.44 and Train accuracy of 0.179, Test loss: 3.3 and Test accuracy of 0.212\n",
      "Iteration 9472: with minibatch training loss = 3.37 and accuracy of 0.17\n",
      "Iteration 9600: with minibatch training loss = 3.57 and accuracy of 0.15\n",
      "Iteration 9728: with minibatch training loss = 3.32 and accuracy of 0.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Train loss: 3.39 and Train accuracy of 0.186, Test loss: 3.27 and Test accuracy of 0.221\n",
      "Iteration 9856: with minibatch training loss = 3.33 and accuracy of 0.19\n",
      "Iteration 9984: with minibatch training loss = 3.23 and accuracy of 0.23\n",
      "Iteration 10112: with minibatch training loss = 3.33 and accuracy of 0.2\n",
      "Epoch 26, Train loss: 3.37 and Train accuracy of 0.192, Test loss: 3.23 and Test accuracy of 0.232\n",
      "Iteration 10240: with minibatch training loss = 3.18 and accuracy of 0.23\n",
      "Iteration 10368: with minibatch training loss = 3.48 and accuracy of 0.16\n",
      "Iteration 10496: with minibatch training loss = 3.26 and accuracy of 0.16\n",
      "Epoch 27, Train loss: 3.33 and Train accuracy of 0.198, Test loss: 3.19 and Test accuracy of 0.238\n",
      "Iteration 10624: with minibatch training loss = 3.57 and accuracy of 0.14\n",
      "Iteration 10752: with minibatch training loss = 3.17 and accuracy of 0.2\n",
      "Iteration 10880: with minibatch training loss = 3.1 and accuracy of 0.27\n",
      "Epoch 28, Train loss: 3.29 and Train accuracy of 0.205, Test loss: 3.17 and Test accuracy of 0.239\n",
      "Iteration 11008: with minibatch training loss = 3.09 and accuracy of 0.2\n",
      "Iteration 11136: with minibatch training loss = 3.14 and accuracy of 0.18\n",
      "Iteration 11264: with minibatch training loss = 3.15 and accuracy of 0.24\n",
      "Epoch 29, Train loss: 3.26 and Train accuracy of 0.21, Test loss: 3.12 and Test accuracy of 0.249\n",
      "Iteration 11392: with minibatch training loss = 3.38 and accuracy of 0.21\n",
      "Iteration 11520: with minibatch training loss = 3.15 and accuracy of 0.23\n",
      "Iteration 11648: with minibatch training loss = 3.14 and accuracy of 0.22\n",
      "Epoch 30, Train loss: 3.22 and Train accuracy of 0.217, Test loss: 3.08 and Test accuracy of 0.257\n",
      "Iteration 11776: with minibatch training loss = 2.84 and accuracy of 0.3\n",
      "Iteration 11904: with minibatch training loss = 3.24 and accuracy of 0.2\n",
      "Iteration 12032: with minibatch training loss = 3.24 and accuracy of 0.23\n",
      "Epoch 31, Train loss: 3.18 and Train accuracy of 0.223, Test loss: 3.05 and Test accuracy of 0.26\n",
      "Iteration 12160: with minibatch training loss = 3.32 and accuracy of 0.23\n",
      "Iteration 12288: with minibatch training loss = 3.34 and accuracy of 0.16\n",
      "Iteration 12416: with minibatch training loss = 3.16 and accuracy of 0.22\n",
      "Epoch 32, Train loss: 3.15 and Train accuracy of 0.232, Test loss: 3.03 and Test accuracy of 0.262\n",
      "Iteration 12544: with minibatch training loss = 3.26 and accuracy of 0.16\n",
      "Iteration 12672: with minibatch training loss = 3.07 and accuracy of 0.23\n",
      "Iteration 12800: with minibatch training loss = 2.96 and accuracy of 0.29\n",
      "Epoch 33, Train loss: 3.12 and Train accuracy of 0.237, Test loss: 3.01 and Test accuracy of 0.269\n",
      "Iteration 12928: with minibatch training loss = 3.25 and accuracy of 0.23\n",
      "Iteration 13056: with minibatch training loss = 3.16 and accuracy of 0.22\n",
      "Iteration 13184: with minibatch training loss = 2.97 and accuracy of 0.24\n",
      "Epoch 34, Train loss: 3.08 and Train accuracy of 0.245, Test loss: 2.98 and Test accuracy of 0.273\n",
      "Iteration 13312: with minibatch training loss = 3.11 and accuracy of 0.3\n",
      "Iteration 13440: with minibatch training loss = 3.22 and accuracy of 0.24\n",
      "Iteration 13568: with minibatch training loss = 3.03 and accuracy of 0.26\n",
      "Epoch 35, Train loss: 3.05 and Train accuracy of 0.251, Test loss: 2.97 and Test accuracy of 0.275\n",
      "Iteration 13696: with minibatch training loss = 3.02 and accuracy of 0.27\n",
      "Iteration 13824: with minibatch training loss = 3.01 and accuracy of 0.23\n",
      "Iteration 13952: with minibatch training loss = 2.96 and accuracy of 0.23\n",
      "Epoch 36, Train loss: 3.02 and Train accuracy of 0.256, Test loss: 2.91 and Test accuracy of 0.286\n",
      "Iteration 14080: with minibatch training loss = 3.11 and accuracy of 0.24\n",
      "Iteration 14208: with minibatch training loss = 3.02 and accuracy of 0.25\n",
      "Iteration 14336: with minibatch training loss = 2.99 and accuracy of 0.3\n",
      "Iteration 14464: with minibatch training loss = 2.88 and accuracy of 0.34\n",
      "Epoch 37, Train loss: 2.99 and Train accuracy of 0.261, Test loss: 2.89 and Test accuracy of 0.295\n",
      "Iteration 14592: with minibatch training loss = 2.86 and accuracy of 0.26\n",
      "Iteration 14720: with minibatch training loss = 2.96 and accuracy of 0.25\n",
      "Iteration 14848: with minibatch training loss = 3.15 and accuracy of 0.27\n",
      "Epoch 38, Train loss: 2.96 and Train accuracy of 0.266, Test loss: 2.86 and Test accuracy of 0.302\n",
      "Iteration 14976: with minibatch training loss = 3.08 and accuracy of 0.27\n",
      "Iteration 15104: with minibatch training loss = 2.73 and accuracy of 0.33\n",
      "Iteration 15232: with minibatch training loss = 3.02 and accuracy of 0.31\n",
      "Epoch 39, Train loss: 2.93 and Train accuracy of 0.272, Test loss: 2.87 and Test accuracy of 0.299\n",
      "Iteration 15360: with minibatch training loss = 2.96 and accuracy of 0.21\n",
      "Iteration 15488: with minibatch training loss = 3.09 and accuracy of 0.21\n",
      "Iteration 15616: with minibatch training loss = 3.06 and accuracy of 0.23\n",
      "Epoch 40, Train loss: 2.9 and Train accuracy of 0.279, Test loss: 2.81 and Test accuracy of 0.312\n",
      "Iteration 15744: with minibatch training loss = 3.01 and accuracy of 0.27\n",
      "Iteration 15872: with minibatch training loss = 2.87 and accuracy of 0.3\n",
      "Iteration 16000: with minibatch training loss = 2.94 and accuracy of 0.25\n",
      "Epoch 41, Train loss: 2.87 and Train accuracy of 0.286, Test loss: 2.81 and Test accuracy of 0.309\n",
      "Iteration 16128: with minibatch training loss = 2.7 and accuracy of 0.3\n",
      "Iteration 16256: with minibatch training loss = 2.94 and accuracy of 0.27\n",
      "Iteration 16384: with minibatch training loss = 2.76 and accuracy of 0.27\n",
      "Epoch 42, Train loss: 2.84 and Train accuracy of 0.289, Test loss: 2.79 and Test accuracy of 0.314\n",
      "Iteration 16512: with minibatch training loss = 2.84 and accuracy of 0.32\n",
      "Iteration 16640: with minibatch training loss = 2.5 and accuracy of 0.31\n",
      "Iteration 16768: with minibatch training loss = 2.75 and accuracy of 0.26\n",
      "Epoch 43, Train loss: 2.81 and Train accuracy of 0.293, Test loss: 2.76 and Test accuracy of 0.322\n",
      "Iteration 16896: with minibatch training loss = 2.73 and accuracy of 0.29\n",
      "Iteration 17024: with minibatch training loss = 2.76 and accuracy of 0.32\n",
      "Iteration 17152: with minibatch training loss = 2.56 and accuracy of 0.31\n",
      "Epoch 44, Train loss: 2.79 and Train accuracy of 0.301, Test loss: 2.75 and Test accuracy of 0.323\n",
      "Iteration 17280: with minibatch training loss = 2.94 and accuracy of 0.3\n",
      "Iteration 17408: with minibatch training loss = 2.47 and accuracy of 0.34\n",
      "Iteration 17536: with minibatch training loss = 2.81 and accuracy of 0.28\n",
      "Epoch 45, Train loss: 2.75 and Train accuracy of 0.305, Test loss: 2.71 and Test accuracy of 0.332\n",
      "Iteration 17664: with minibatch training loss = 2.77 and accuracy of 0.3\n",
      "Iteration 17792: with minibatch training loss = 2.68 and accuracy of 0.37\n",
      "Iteration 17920: with minibatch training loss = 2.63 and accuracy of 0.35\n",
      "Epoch 46, Train loss: 2.73 and Train accuracy of 0.313, Test loss: 2.71 and Test accuracy of 0.327\n",
      "Iteration 18048: with minibatch training loss = 2.74 and accuracy of 0.29\n",
      "Iteration 18176: with minibatch training loss = 2.86 and accuracy of 0.27\n",
      "Iteration 18304: with minibatch training loss = 2.63 and accuracy of 0.35\n",
      "Epoch 47, Train loss: 2.7 and Train accuracy of 0.317, Test loss: 2.7 and Test accuracy of 0.333\n",
      "Iteration 18432: with minibatch training loss = 2.54 and accuracy of 0.34\n",
      "Iteration 18560: with minibatch training loss = 2.52 and accuracy of 0.31\n",
      "Iteration 18688: with minibatch training loss = 2.66 and accuracy of 0.31\n",
      "Epoch 48, Train loss: 2.67 and Train accuracy of 0.322, Test loss: 2.66 and Test accuracy of 0.339\n",
      "Iteration 18816: with minibatch training loss = 2.54 and accuracy of 0.38\n",
      "Iteration 18944: with minibatch training loss = 2.56 and accuracy of 0.27\n",
      "Iteration 19072: with minibatch training loss = 2.85 and accuracy of 0.28\n",
      "Epoch 49, Train loss: 2.65 and Train accuracy of 0.324, Test loss: 2.66 and Test accuracy of 0.343\n",
      "Iteration 19200: with minibatch training loss = 2.75 and accuracy of 0.3\n",
      "Iteration 19328: with minibatch training loss = 2.89 and accuracy of 0.3\n",
      "Iteration 19456: with minibatch training loss = 2.72 and accuracy of 0.3\n",
      "Epoch 50, Train loss: 2.63 and Train accuracy of 0.331, Test loss: 2.66 and Test accuracy of 0.339\n",
      "Iteration 19584: with minibatch training loss = 2.73 and accuracy of 0.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19712: with minibatch training loss = 2.58 and accuracy of 0.34\n",
      "Iteration 19840: with minibatch training loss = 2.39 and accuracy of 0.4\n",
      "Epoch 51, Train loss: 2.59 and Train accuracy of 0.337, Test loss: 2.63 and Test accuracy of 0.345\n",
      "Iteration 19968: with minibatch training loss = 2.78 and accuracy of 0.3\n",
      "Iteration 20096: with minibatch training loss = 2.65 and accuracy of 0.29\n",
      "Iteration 20224: with minibatch training loss = 2.65 and accuracy of 0.37\n",
      "Epoch 52, Train loss: 2.57 and Train accuracy of 0.341, Test loss: 2.62 and Test accuracy of 0.347\n",
      "Iteration 20352: with minibatch training loss = 2.5 and accuracy of 0.37\n",
      "Iteration 20480: with minibatch training loss = 2.56 and accuracy of 0.38\n",
      "Iteration 20608: with minibatch training loss = 2.46 and accuracy of 0.39\n",
      "Epoch 53, Train loss: 2.54 and Train accuracy of 0.351, Test loss: 2.61 and Test accuracy of 0.349\n",
      "Iteration 20736: with minibatch training loss = 2.46 and accuracy of 0.37\n",
      "Iteration 20864: with minibatch training loss = 2.53 and accuracy of 0.33\n",
      "Iteration 20992: with minibatch training loss = 2.59 and accuracy of 0.34\n",
      "Epoch 54, Train loss: 2.52 and Train accuracy of 0.357, Test loss: 2.6 and Test accuracy of 0.353\n",
      "Iteration 21120: with minibatch training loss = 2.7 and accuracy of 0.34\n",
      "Iteration 21248: with minibatch training loss = 2.1 and accuracy of 0.45\n",
      "Iteration 21376: with minibatch training loss = 2.4 and accuracy of 0.36\n",
      "Iteration 21504: with minibatch training loss = 2.52 and accuracy of 0.34\n",
      "Epoch 55, Train loss: 2.49 and Train accuracy of 0.357, Test loss: 2.58 and Test accuracy of 0.358\n",
      "Iteration 21632: with minibatch training loss = 2.27 and accuracy of 0.45\n",
      "Iteration 21760: with minibatch training loss = 2.34 and accuracy of 0.39\n",
      "Iteration 21888: with minibatch training loss = 2.48 and accuracy of 0.4\n",
      "Epoch 56, Train loss: 2.46 and Train accuracy of 0.363, Test loss: 2.57 and Test accuracy of 0.357\n",
      "Iteration 22016: with minibatch training loss = 2.73 and accuracy of 0.34\n",
      "Iteration 22144: with minibatch training loss = 2.52 and accuracy of 0.38\n",
      "Iteration 22272: with minibatch training loss = 2.3 and accuracy of 0.4\n",
      "Epoch 57, Train loss: 2.44 and Train accuracy of 0.369, Test loss: 2.55 and Test accuracy of 0.361\n",
      "Iteration 22400: with minibatch training loss = 2.32 and accuracy of 0.41\n",
      "Iteration 22528: with minibatch training loss = 2.67 and accuracy of 0.33\n",
      "Iteration 22656: with minibatch training loss = 2.37 and accuracy of 0.34\n",
      "Epoch 58, Train loss: 2.41 and Train accuracy of 0.373, Test loss: 2.54 and Test accuracy of 0.362\n",
      "Iteration 22784: with minibatch training loss = 2.01 and accuracy of 0.52\n",
      "Iteration 22912: with minibatch training loss = 2.52 and accuracy of 0.38\n",
      "Iteration 23040: with minibatch training loss = 2.45 and accuracy of 0.38\n",
      "Epoch 59, Train loss: 2.38 and Train accuracy of 0.382, Test loss: 2.56 and Test accuracy of 0.361\n",
      "Iteration 23168: with minibatch training loss = 2.22 and accuracy of 0.45\n",
      "Iteration 23296: with minibatch training loss = 2.24 and accuracy of 0.42\n",
      "Iteration 23424: with minibatch training loss = 2.26 and accuracy of 0.38\n",
      "Epoch 60, Train loss: 2.35 and Train accuracy of 0.386, Test loss: 2.55 and Test accuracy of 0.362\n",
      "Iteration 23552: with minibatch training loss = 2.17 and accuracy of 0.39\n",
      "Iteration 23680: with minibatch training loss = 2.54 and accuracy of 0.34\n",
      "Iteration 23808: with minibatch training loss = 2.28 and accuracy of 0.43\n",
      "Epoch 61, Train loss: 2.32 and Train accuracy of 0.39, Test loss: 2.52 and Test accuracy of 0.367\n",
      "Iteration 23936: with minibatch training loss = 2.21 and accuracy of 0.44\n",
      "Iteration 24064: with minibatch training loss = 2.47 and accuracy of 0.38\n",
      "Iteration 24192: with minibatch training loss = 2.12 and accuracy of 0.45\n",
      "Epoch 62, Train loss: 2.3 and Train accuracy of 0.397, Test loss: 2.52 and Test accuracy of 0.37\n",
      "Iteration 24320: with minibatch training loss = 2.24 and accuracy of 0.4\n",
      "Iteration 24448: with minibatch training loss = 2.26 and accuracy of 0.41\n",
      "Iteration 24576: with minibatch training loss = 2.4 and accuracy of 0.34\n",
      "Epoch 63, Train loss: 2.27 and Train accuracy of 0.401, Test loss: 2.51 and Test accuracy of 0.372\n",
      "Iteration 24704: with minibatch training loss = 2.18 and accuracy of 0.43\n",
      "Iteration 24832: with minibatch training loss = 2.24 and accuracy of 0.39\n",
      "Iteration 24960: with minibatch training loss = 2.44 and accuracy of 0.37\n",
      "Epoch 64, Train loss: 2.24 and Train accuracy of 0.408, Test loss: 2.49 and Test accuracy of 0.378\n",
      "Iteration 25088: with minibatch training loss = 2.01 and accuracy of 0.5\n",
      "Iteration 25216: with minibatch training loss = 2.35 and accuracy of 0.41\n",
      "Iteration 25344: with minibatch training loss = 2.44 and accuracy of 0.38\n",
      "Epoch 65, Train loss: 2.22 and Train accuracy of 0.411, Test loss: 2.49 and Test accuracy of 0.376\n",
      "Iteration 25472: with minibatch training loss = 2.04 and accuracy of 0.43\n",
      "Iteration 25600: with minibatch training loss = 2.1 and accuracy of 0.48\n",
      "Iteration 25728: with minibatch training loss = 2.13 and accuracy of 0.45\n",
      "Epoch 66, Train loss: 2.19 and Train accuracy of 0.421, Test loss: 2.49 and Test accuracy of 0.377\n",
      "Iteration 25856: with minibatch training loss = 2.21 and accuracy of 0.43\n",
      "Iteration 25984: with minibatch training loss = 2.01 and accuracy of 0.45\n",
      "Iteration 26112: with minibatch training loss = 2.66 and accuracy of 0.32\n",
      "Epoch 67, Train loss: 2.16 and Train accuracy of 0.424, Test loss: 2.51 and Test accuracy of 0.371\n",
      "Iteration 26240: with minibatch training loss = 2.05 and accuracy of 0.43\n",
      "Iteration 26368: with minibatch training loss = 1.9 and accuracy of 0.55\n",
      "Iteration 26496: with minibatch training loss = 2.25 and accuracy of 0.41\n",
      "Epoch 68, Train loss: 2.14 and Train accuracy of 0.431, Test loss: 2.48 and Test accuracy of 0.378\n",
      "Iteration 26624: with minibatch training loss = 1.99 and accuracy of 0.44\n",
      "Iteration 26752: with minibatch training loss = 2.15 and accuracy of 0.44\n",
      "Iteration 26880: with minibatch training loss = 2.39 and accuracy of 0.39\n",
      "Epoch 69, Train loss: 2.1 and Train accuracy of 0.438, Test loss: 2.5 and Test accuracy of 0.378\n",
      "Iteration 27008: with minibatch training loss = 1.98 and accuracy of 0.46\n",
      "Iteration 27136: with minibatch training loss = 1.98 and accuracy of 0.45\n",
      "Iteration 27264: with minibatch training loss = 2.09 and accuracy of 0.49\n",
      "Epoch 70, Train loss: 2.08 and Train accuracy of 0.442, Test loss: 2.49 and Test accuracy of 0.376\n",
      "Iteration 27392: with minibatch training loss = 2.15 and accuracy of 0.42\n",
      "Iteration 27520: with minibatch training loss = 1.63 and accuracy of 0.52\n",
      "Iteration 27648: with minibatch training loss = 1.96 and accuracy of 0.46\n",
      "Epoch 71, Train loss: 2.05 and Train accuracy of 0.45, Test loss: 2.49 and Test accuracy of 0.385\n",
      "Iteration 27776: with minibatch training loss = 1.91 and accuracy of 0.45\n",
      "Iteration 27904: with minibatch training loss = 2.23 and accuracy of 0.41\n",
      "Iteration 28032: with minibatch training loss = 2.06 and accuracy of 0.47\n",
      "Epoch 72, Train loss: 2.02 and Train accuracy of 0.455, Test loss: 2.48 and Test accuracy of 0.383\n",
      "Iteration 28160: with minibatch training loss = 1.98 and accuracy of 0.48\n",
      "Iteration 28288: with minibatch training loss = 2.1 and accuracy of 0.44\n",
      "Iteration 28416: with minibatch training loss = 1.59 and accuracy of 0.54\n",
      "Epoch 73, Train loss: 2 and Train accuracy of 0.463, Test loss: 2.46 and Test accuracy of 0.386\n",
      "Iteration 28544: with minibatch training loss = 2.01 and accuracy of 0.48\n",
      "Iteration 28672: with minibatch training loss = 2.03 and accuracy of 0.43\n",
      "Iteration 28800: with minibatch training loss = 2.02 and accuracy of 0.45\n",
      "Iteration 28928: with minibatch training loss = 2.02 and accuracy of 0.48\n",
      "Epoch 74, Train loss: 1.96 and Train accuracy of 0.468, Test loss: 2.48 and Test accuracy of 0.395\n",
      "Iteration 29056: with minibatch training loss = 1.72 and accuracy of 0.49\n",
      "Iteration 29184: with minibatch training loss = 1.63 and accuracy of 0.52\n",
      "Iteration 29312: with minibatch training loss = 2.17 and accuracy of 0.45\n",
      "Epoch 75, Train loss: 1.94 and Train accuracy of 0.472, Test loss: 2.47 and Test accuracy of 0.39\n",
      "Iteration 29440: with minibatch training loss = 1.71 and accuracy of 0.53\n",
      "Iteration 29568: with minibatch training loss = 1.96 and accuracy of 0.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29696: with minibatch training loss = 1.69 and accuracy of 0.48\n",
      "Epoch 76, Train loss: 1.9 and Train accuracy of 0.48, Test loss: 2.49 and Test accuracy of 0.388\n",
      "Iteration 29824: with minibatch training loss = 1.81 and accuracy of 0.54\n",
      "Iteration 29952: with minibatch training loss = 1.97 and accuracy of 0.41\n",
      "Iteration 30080: with minibatch training loss = 2.1 and accuracy of 0.43\n",
      "Epoch 77, Train loss: 1.88 and Train accuracy of 0.485, Test loss: 2.48 and Test accuracy of 0.388\n",
      "Iteration 30208: with minibatch training loss = 1.78 and accuracy of 0.53\n",
      "Iteration 30336: with minibatch training loss = 2.22 and accuracy of 0.4\n",
      "Iteration 30464: with minibatch training loss = 1.6 and accuracy of 0.52\n",
      "Epoch 78, Train loss: 1.85 and Train accuracy of 0.492, Test loss: 2.5 and Test accuracy of 0.393\n",
      "Iteration 30592: with minibatch training loss = 1.67 and accuracy of 0.55\n",
      "Iteration 30720: with minibatch training loss = 1.9 and accuracy of 0.48\n",
      "Iteration 30848: with minibatch training loss = 1.88 and accuracy of 0.44\n",
      "Epoch 79, Train loss: 1.82 and Train accuracy of 0.502, Test loss: 2.51 and Test accuracy of 0.389\n",
      "Iteration 30976: with minibatch training loss = 1.76 and accuracy of 0.51\n",
      "Iteration 31104: with minibatch training loss = 1.56 and accuracy of 0.57\n",
      "Iteration 31232: with minibatch training loss = 1.51 and accuracy of 0.55\n",
      "Epoch 80, Train loss: 1.79 and Train accuracy of 0.505, Test loss: 2.5 and Test accuracy of 0.397\n",
      "Iteration 31360: with minibatch training loss = 1.76 and accuracy of 0.52\n",
      "Iteration 31488: with minibatch training loss = 1.88 and accuracy of 0.43\n",
      "Iteration 31616: with minibatch training loss = 1.47 and accuracy of 0.6\n",
      "Epoch 81, Train loss: 1.76 and Train accuracy of 0.512, Test loss: 2.51 and Test accuracy of 0.393\n",
      "Iteration 31744: with minibatch training loss = 1.75 and accuracy of 0.52\n",
      "Iteration 31872: with minibatch training loss = 1.66 and accuracy of 0.52\n",
      "Iteration 32000: with minibatch training loss = 1.42 and accuracy of 0.58\n",
      "Epoch 82, Train loss: 1.73 and Train accuracy of 0.52, Test loss: 2.49 and Test accuracy of 0.4\n",
      "Iteration 32128: with minibatch training loss = 1.66 and accuracy of 0.52\n",
      "Iteration 32256: with minibatch training loss = 1.67 and accuracy of 0.55\n",
      "Iteration 32384: with minibatch training loss = 1.7 and accuracy of 0.5\n",
      "Epoch 83, Train loss: 1.71 and Train accuracy of 0.523, Test loss: 2.51 and Test accuracy of 0.395\n",
      "Iteration 32512: with minibatch training loss = 1.66 and accuracy of 0.54\n",
      "Iteration 32640: with minibatch training loss = 2.02 and accuracy of 0.41\n",
      "Iteration 32768: with minibatch training loss = 1.84 and accuracy of 0.53\n",
      "Epoch 84, Train loss: 1.68 and Train accuracy of 0.53, Test loss: 2.52 and Test accuracy of 0.401\n",
      "Iteration 32896: with minibatch training loss = 1.8 and accuracy of 0.52\n",
      "Iteration 33024: with minibatch training loss = 1.71 and accuracy of 0.52\n",
      "Iteration 33152: with minibatch training loss = 1.47 and accuracy of 0.59\n",
      "Epoch 85, Train loss: 1.65 and Train accuracy of 0.537, Test loss: 2.52 and Test accuracy of 0.394\n",
      "Iteration 33280: with minibatch training loss = 1.92 and accuracy of 0.49\n",
      "Iteration 33408: with minibatch training loss = 1.46 and accuracy of 0.55\n",
      "Iteration 33536: with minibatch training loss = 1.42 and accuracy of 0.56\n",
      "Epoch 86, Train loss: 1.62 and Train accuracy of 0.542, Test loss: 2.53 and Test accuracy of 0.399\n",
      "Iteration 33664: with minibatch training loss = 1.62 and accuracy of 0.54\n",
      "Iteration 33792: with minibatch training loss = 1.56 and accuracy of 0.57\n",
      "Iteration 33920: with minibatch training loss = 1.58 and accuracy of 0.55\n",
      "Epoch 87, Train loss: 1.6 and Train accuracy of 0.547, Test loss: 2.51 and Test accuracy of 0.403\n",
      "Iteration 34048: with minibatch training loss = 1.62 and accuracy of 0.48\n",
      "Iteration 34176: with minibatch training loss = 1.67 and accuracy of 0.54\n",
      "Iteration 34304: with minibatch training loss = 1.44 and accuracy of 0.55\n",
      "Epoch 88, Train loss: 1.57 and Train accuracy of 0.552, Test loss: 2.53 and Test accuracy of 0.403\n",
      "Iteration 34432: with minibatch training loss = 1.69 and accuracy of 0.52\n",
      "Iteration 34560: with minibatch training loss = 1.48 and accuracy of 0.58\n",
      "Iteration 34688: with minibatch training loss = 1.43 and accuracy of 0.57\n",
      "Epoch 89, Train loss: 1.54 and Train accuracy of 0.56, Test loss: 2.52 and Test accuracy of 0.397\n",
      "Iteration 34816: with minibatch training loss = 1.37 and accuracy of 0.61\n",
      "Iteration 34944: with minibatch training loss = 1.75 and accuracy of 0.53\n",
      "Iteration 35072: with minibatch training loss = 1.3 and accuracy of 0.62\n",
      "Epoch 90, Train loss: 1.51 and Train accuracy of 0.569, Test loss: 2.54 and Test accuracy of 0.4\n",
      "Iteration 35200: with minibatch training loss = 1.2 and accuracy of 0.62\n",
      "Iteration 35328: with minibatch training loss = 1.36 and accuracy of 0.64\n",
      "Iteration 35456: with minibatch training loss = 1.61 and accuracy of 0.56\n",
      "Epoch 91, Train loss: 1.48 and Train accuracy of 0.574, Test loss: 2.56 and Test accuracy of 0.4\n",
      "Iteration 35584: with minibatch training loss = 1.45 and accuracy of 0.57\n",
      "Iteration 35712: with minibatch training loss = 1.36 and accuracy of 0.62\n",
      "Iteration 35840: with minibatch training loss = 1.32 and accuracy of 0.62\n",
      "Iteration 35968: with minibatch training loss = 1.43 and accuracy of 0.55\n",
      "Epoch 92, Train loss: 1.45 and Train accuracy of 0.583, Test loss: 2.59 and Test accuracy of 0.398\n",
      "Iteration 36096: with minibatch training loss = 1.33 and accuracy of 0.61\n",
      "Iteration 36224: with minibatch training loss = 1.35 and accuracy of 0.6\n",
      "Iteration 36352: with minibatch training loss = 1.35 and accuracy of 0.59\n",
      "Epoch 93, Train loss: 1.42 and Train accuracy of 0.592, Test loss: 2.6 and Test accuracy of 0.395\n",
      "Iteration 36480: with minibatch training loss = 1.36 and accuracy of 0.63\n",
      "Iteration 36608: with minibatch training loss = 1.46 and accuracy of 0.56\n",
      "Iteration 36736: with minibatch training loss = 1.58 and accuracy of 0.6\n",
      "Epoch 94, Train loss: 1.4 and Train accuracy of 0.594, Test loss: 2.58 and Test accuracy of 0.405\n",
      "Iteration 36864: with minibatch training loss = 1.4 and accuracy of 0.58\n",
      "Iteration 36992: with minibatch training loss = 1.58 and accuracy of 0.57\n",
      "Iteration 37120: with minibatch training loss = 1.32 and accuracy of 0.62\n",
      "Epoch 95, Train loss: 1.37 and Train accuracy of 0.602, Test loss: 2.57 and Test accuracy of 0.399\n",
      "Iteration 37248: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 37376: with minibatch training loss = 1.37 and accuracy of 0.59\n",
      "Iteration 37504: with minibatch training loss = 1.49 and accuracy of 0.61\n",
      "Epoch 96, Train loss: 1.34 and Train accuracy of 0.611, Test loss: 2.67 and Test accuracy of 0.396\n",
      "Iteration 37632: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 37760: with minibatch training loss = 1.13 and accuracy of 0.68\n",
      "Iteration 37888: with minibatch training loss = 1.2 and accuracy of 0.62\n",
      "Epoch 97, Train loss: 1.32 and Train accuracy of 0.614, Test loss: 2.59 and Test accuracy of 0.403\n",
      "Iteration 38016: with minibatch training loss = 1.37 and accuracy of 0.64\n",
      "Iteration 38144: with minibatch training loss = 1.18 and accuracy of 0.66\n",
      "Iteration 38272: with minibatch training loss = 1.31 and accuracy of 0.57\n",
      "Epoch 98, Train loss: 1.29 and Train accuracy of 0.622, Test loss: 2.62 and Test accuracy of 0.401\n",
      "Iteration 38400: with minibatch training loss = 1.34 and accuracy of 0.65\n",
      "Iteration 38528: with minibatch training loss = 0.929 and accuracy of 0.74\n",
      "Iteration 38656: with minibatch training loss = 1.23 and accuracy of 0.58\n",
      "Epoch 99, Train loss: 1.27 and Train accuracy of 0.629, Test loss: 2.69 and Test accuracy of 0.398\n",
      "Iteration 38784: with minibatch training loss = 1.3 and accuracy of 0.59\n",
      "Iteration 38912: with minibatch training loss = 1.25 and accuracy of 0.57\n",
      "Iteration 39040: with minibatch training loss = 1.3 and accuracy of 0.64\n",
      "Epoch 100, Train loss: 1.26 and Train accuracy of 0.632, Test loss: 2.66 and Test accuracy of 0.398\n",
      "Iteration 39168: with minibatch training loss = 1.2 and accuracy of 0.59\n",
      "Iteration 39296: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 39424: with minibatch training loss = 1.38 and accuracy of 0.59\n",
      "Epoch 101, Train loss: 1.22 and Train accuracy of 0.639, Test loss: 2.7 and Test accuracy of 0.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39552: with minibatch training loss = 1 and accuracy of 0.71\n",
      "Iteration 39680: with minibatch training loss = 1.23 and accuracy of 0.64\n",
      "Iteration 39808: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Epoch 102, Train loss: 1.19 and Train accuracy of 0.646, Test loss: 2.73 and Test accuracy of 0.403\n",
      "Iteration 39936: with minibatch training loss = 0.998 and accuracy of 0.73\n",
      "Iteration 40064: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 40192: with minibatch training loss = 1.28 and accuracy of 0.59\n",
      "Epoch 103, Train loss: 1.16 and Train accuracy of 0.654, Test loss: 2.76 and Test accuracy of 0.399\n",
      "Iteration 40320: with minibatch training loss = 1.08 and accuracy of 0.68\n",
      "Iteration 40448: with minibatch training loss = 1.12 and accuracy of 0.63\n",
      "Iteration 40576: with minibatch training loss = 1.07 and accuracy of 0.68\n",
      "Epoch 104, Train loss: 1.15 and Train accuracy of 0.658, Test loss: 2.75 and Test accuracy of 0.401\n",
      "Iteration 40704: with minibatch training loss = 0.994 and accuracy of 0.72\n",
      "Iteration 40832: with minibatch training loss = 1.3 and accuracy of 0.61\n",
      "Iteration 40960: with minibatch training loss = 1.29 and accuracy of 0.63\n",
      "Epoch 105, Train loss: 1.13 and Train accuracy of 0.66, Test loss: 2.77 and Test accuracy of 0.4\n",
      "Iteration 41088: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 41216: with minibatch training loss = 1.14 and accuracy of 0.63\n",
      "Iteration 41344: with minibatch training loss = 1.08 and accuracy of 0.63\n",
      "Epoch 106, Train loss: 1.1 and Train accuracy of 0.67, Test loss: 2.72 and Test accuracy of 0.407\n",
      "Iteration 41472: with minibatch training loss = 0.936 and accuracy of 0.73\n",
      "Iteration 41600: with minibatch training loss = 1.35 and accuracy of 0.58\n",
      "Iteration 41728: with minibatch training loss = 1.14 and accuracy of 0.65\n",
      "Epoch 107, Train loss: 1.08 and Train accuracy of 0.673, Test loss: 2.83 and Test accuracy of 0.4\n",
      "Iteration 41856: with minibatch training loss = 1.19 and accuracy of 0.62\n",
      "Iteration 41984: with minibatch training loss = 1.25 and accuracy of 0.65\n",
      "Iteration 42112: with minibatch training loss = 1.28 and accuracy of 0.62\n",
      "Epoch 108, Train loss: 1.07 and Train accuracy of 0.681, Test loss: 2.81 and Test accuracy of 0.399\n",
      "Iteration 42240: with minibatch training loss = 1.12 and accuracy of 0.66\n",
      "Iteration 42368: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 42496: with minibatch training loss = 1.25 and accuracy of 0.63\n",
      "Epoch 109, Train loss: 1.04 and Train accuracy of 0.687, Test loss: 2.78 and Test accuracy of 0.404\n",
      "Iteration 42624: with minibatch training loss = 0.933 and accuracy of 0.7\n",
      "Iteration 42752: with minibatch training loss = 1.17 and accuracy of 0.65\n",
      "Iteration 42880: with minibatch training loss = 1.21 and accuracy of 0.66\n",
      "Iteration 43008: with minibatch training loss = 1.07 and accuracy of 0.66\n",
      "Epoch 110, Train loss: 1.01 and Train accuracy of 0.696, Test loss: 2.85 and Test accuracy of 0.403\n",
      "Iteration 43136: with minibatch training loss = 0.964 and accuracy of 0.7\n",
      "Iteration 43264: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 43392: with minibatch training loss = 1.03 and accuracy of 0.68\n",
      "Epoch 111, Train loss: 0.99 and Train accuracy of 0.7, Test loss: 2.8 and Test accuracy of 0.398\n",
      "Iteration 43520: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 43648: with minibatch training loss = 0.866 and accuracy of 0.74\n",
      "Iteration 43776: with minibatch training loss = 0.995 and accuracy of 0.7\n",
      "Epoch 112, Train loss: 0.981 and Train accuracy of 0.704, Test loss: 2.82 and Test accuracy of 0.405\n",
      "Iteration 43904: with minibatch training loss = 0.78 and accuracy of 0.75\n",
      "Iteration 44032: with minibatch training loss = 0.679 and accuracy of 0.79\n",
      "Iteration 44160: with minibatch training loss = 0.924 and accuracy of 0.73\n",
      "Epoch 113, Train loss: 0.955 and Train accuracy of 0.709, Test loss: 2.9 and Test accuracy of 0.402\n",
      "Iteration 44288: with minibatch training loss = 1.01 and accuracy of 0.66\n",
      "Iteration 44416: with minibatch training loss = 0.987 and accuracy of 0.74\n",
      "Iteration 44544: with minibatch training loss = 0.638 and accuracy of 0.81\n",
      "Epoch 114, Train loss: 0.941 and Train accuracy of 0.712, Test loss: 2.89 and Test accuracy of 0.401\n",
      "Iteration 44672: with minibatch training loss = 0.971 and accuracy of 0.68\n",
      "Iteration 44800: with minibatch training loss = 0.879 and accuracy of 0.71\n",
      "Iteration 44928: with minibatch training loss = 0.858 and accuracy of 0.7\n",
      "Epoch 115, Train loss: 0.919 and Train accuracy of 0.72, Test loss: 2.9 and Test accuracy of 0.402\n",
      "Iteration 45056: with minibatch training loss = 0.836 and accuracy of 0.75\n",
      "Iteration 45184: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 45312: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Epoch 116, Train loss: 0.903 and Train accuracy of 0.723, Test loss: 2.97 and Test accuracy of 0.403\n",
      "Iteration 45440: with minibatch training loss = 0.918 and accuracy of 0.7\n",
      "Iteration 45568: with minibatch training loss = 0.83 and accuracy of 0.73\n",
      "Iteration 45696: with minibatch training loss = 0.883 and accuracy of 0.72\n",
      "Epoch 117, Train loss: 0.882 and Train accuracy of 0.729, Test loss: 2.91 and Test accuracy of 0.398\n",
      "Iteration 45824: with minibatch training loss = 0.737 and accuracy of 0.73\n",
      "Iteration 45952: with minibatch training loss = 0.858 and accuracy of 0.71\n",
      "Iteration 46080: with minibatch training loss = 0.981 and accuracy of 0.7\n",
      "Epoch 118, Train loss: 0.865 and Train accuracy of 0.734, Test loss: 2.98 and Test accuracy of 0.401\n",
      "Iteration 46208: with minibatch training loss = 0.847 and accuracy of 0.73\n",
      "Iteration 46336: with minibatch training loss = 0.875 and accuracy of 0.73\n",
      "Iteration 46464: with minibatch training loss = 0.82 and accuracy of 0.75\n",
      "Epoch 119, Train loss: 0.854 and Train accuracy of 0.739, Test loss: 2.98 and Test accuracy of 0.405\n",
      "Iteration 46592: with minibatch training loss = 0.781 and accuracy of 0.74\n",
      "Iteration 46720: with minibatch training loss = 0.914 and accuracy of 0.65\n",
      "Iteration 46848: with minibatch training loss = 0.894 and accuracy of 0.71\n",
      "Epoch 120, Train loss: 0.833 and Train accuracy of 0.741, Test loss: 3.01 and Test accuracy of 0.4\n",
      "Iteration 46976: with minibatch training loss = 0.68 and accuracy of 0.82\n",
      "Iteration 47104: with minibatch training loss = 0.874 and accuracy of 0.73\n",
      "Iteration 47232: with minibatch training loss = 0.835 and accuracy of 0.76\n",
      "Epoch 121, Train loss: 0.822 and Train accuracy of 0.748, Test loss: 3.03 and Test accuracy of 0.398\n",
      "Iteration 47360: with minibatch training loss = 0.785 and accuracy of 0.76\n",
      "Iteration 47488: with minibatch training loss = 0.748 and accuracy of 0.75\n",
      "Iteration 47616: with minibatch training loss = 0.807 and accuracy of 0.75\n",
      "Epoch 122, Train loss: 0.799 and Train accuracy of 0.755, Test loss: 3.17 and Test accuracy of 0.399\n",
      "Iteration 47744: with minibatch training loss = 0.789 and accuracy of 0.73\n",
      "Iteration 47872: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 48000: with minibatch training loss = 0.774 and accuracy of 0.77\n",
      "Epoch 123, Train loss: 0.779 and Train accuracy of 0.76, Test loss: 3.14 and Test accuracy of 0.401\n",
      "Iteration 48128: with minibatch training loss = 0.831 and accuracy of 0.75\n",
      "Iteration 48256: with minibatch training loss = 0.836 and accuracy of 0.76\n",
      "Iteration 48384: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Epoch 124, Train loss: 0.764 and Train accuracy of 0.763, Test loss: 3.01 and Test accuracy of 0.404\n",
      "Iteration 48512: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 48640: with minibatch training loss = 0.839 and accuracy of 0.76\n",
      "Iteration 48768: with minibatch training loss = 0.511 and accuracy of 0.88\n",
      "Epoch 125, Train loss: 0.756 and Train accuracy of 0.764, Test loss: 3.03 and Test accuracy of 0.404\n",
      "Iteration 48896: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 49024: with minibatch training loss = 0.676 and accuracy of 0.83\n",
      "Iteration 49152: with minibatch training loss = 0.726 and accuracy of 0.76\n",
      "Epoch 126, Train loss: 0.737 and Train accuracy of 0.771, Test loss: 3.24 and Test accuracy of 0.402\n",
      "Iteration 49280: with minibatch training loss = 0.726 and accuracy of 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49408: with minibatch training loss = 0.653 and accuracy of 0.84\n",
      "Iteration 49536: with minibatch training loss = 0.491 and accuracy of 0.84\n",
      "Epoch 127, Train loss: 0.726 and Train accuracy of 0.774, Test loss: 3.05 and Test accuracy of 0.405\n",
      "Iteration 49664: with minibatch training loss = 0.687 and accuracy of 0.77\n",
      "Iteration 49792: with minibatch training loss = 0.755 and accuracy of 0.73\n",
      "Iteration 49920: with minibatch training loss = 0.655 and accuracy of 0.77\n",
      "Epoch 128, Train loss: 0.709 and Train accuracy of 0.781, Test loss: 3.05 and Test accuracy of 0.408\n",
      "Iteration 50048: with minibatch training loss = 0.558 and accuracy of 0.81\n",
      "Iteration 50176: with minibatch training loss = 0.796 and accuracy of 0.76\n",
      "Iteration 50304: with minibatch training loss = 0.732 and accuracy of 0.76\n",
      "Iteration 50432: with minibatch training loss = 0.662 and accuracy of 0.76\n",
      "Epoch 129, Train loss: 0.701 and Train accuracy of 0.782, Test loss: 3.2 and Test accuracy of 0.403\n",
      "Iteration 50560: with minibatch training loss = 0.657 and accuracy of 0.78\n",
      "Iteration 50688: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 50816: with minibatch training loss = 0.616 and accuracy of 0.84\n",
      "Epoch 130, Train loss: 0.683 and Train accuracy of 0.787, Test loss: 3.02 and Test accuracy of 0.404\n",
      "Iteration 50944: with minibatch training loss = 0.51 and accuracy of 0.83\n",
      "Iteration 51072: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 51200: with minibatch training loss = 0.836 and accuracy of 0.72\n",
      "Epoch 131, Train loss: 0.683 and Train accuracy of 0.787, Test loss: 3.09 and Test accuracy of 0.401\n",
      "Iteration 51328: with minibatch training loss = 0.751 and accuracy of 0.75\n",
      "Iteration 51456: with minibatch training loss = 0.685 and accuracy of 0.78\n",
      "Iteration 51584: with minibatch training loss = 0.577 and accuracy of 0.81\n",
      "Epoch 132, Train loss: 0.667 and Train accuracy of 0.793, Test loss: 3.26 and Test accuracy of 0.401\n",
      "Iteration 51712: with minibatch training loss = 0.598 and accuracy of 0.77\n",
      "Iteration 51840: with minibatch training loss = 0.683 and accuracy of 0.79\n",
      "Iteration 51968: with minibatch training loss = 0.573 and accuracy of 0.83\n",
      "Epoch 133, Train loss: 0.652 and Train accuracy of 0.798, Test loss: 3.19 and Test accuracy of 0.402\n",
      "Iteration 52096: with minibatch training loss = 0.68 and accuracy of 0.79\n",
      "Iteration 52224: with minibatch training loss = 0.574 and accuracy of 0.77\n",
      "Iteration 52352: with minibatch training loss = 0.682 and accuracy of 0.81\n",
      "Epoch 134, Train loss: 0.636 and Train accuracy of 0.804, Test loss: 3.23 and Test accuracy of 0.405\n",
      "Iteration 52480: with minibatch training loss = 0.57 and accuracy of 0.84\n",
      "Iteration 52608: with minibatch training loss = 0.653 and accuracy of 0.83\n",
      "Iteration 52736: with minibatch training loss = 0.754 and accuracy of 0.8\n",
      "Epoch 135, Train loss: 0.632 and Train accuracy of 0.803, Test loss: 3.21 and Test accuracy of 0.405\n",
      "Iteration 52864: with minibatch training loss = 0.686 and accuracy of 0.78\n",
      "Iteration 52992: with minibatch training loss = 0.683 and accuracy of 0.82\n",
      "Iteration 53120: with minibatch training loss = 0.673 and accuracy of 0.76\n",
      "Epoch 136, Train loss: 0.617 and Train accuracy of 0.808, Test loss: 3.3 and Test accuracy of 0.402\n",
      "Iteration 53248: with minibatch training loss = 0.598 and accuracy of 0.83\n",
      "Iteration 53376: with minibatch training loss = 0.686 and accuracy of 0.76\n",
      "Iteration 53504: with minibatch training loss = 0.613 and accuracy of 0.84\n",
      "Epoch 137, Train loss: 0.608 and Train accuracy of 0.809, Test loss: 3.17 and Test accuracy of 0.406\n",
      "Iteration 53632: with minibatch training loss = 0.544 and accuracy of 0.82\n",
      "Iteration 53760: with minibatch training loss = 0.554 and accuracy of 0.84\n",
      "Iteration 53888: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Epoch 138, Train loss: 0.592 and Train accuracy of 0.815, Test loss: 3.23 and Test accuracy of 0.407\n",
      "Iteration 54016: with minibatch training loss = 0.416 and accuracy of 0.88\n",
      "Iteration 54144: with minibatch training loss = 0.555 and accuracy of 0.82\n",
      "Iteration 54272: with minibatch training loss = 0.599 and accuracy of 0.8\n",
      "Epoch 139, Train loss: 0.588 and Train accuracy of 0.815, Test loss: 3.33 and Test accuracy of 0.405\n",
      "Iteration 54400: with minibatch training loss = 0.697 and accuracy of 0.79\n",
      "Iteration 54528: with minibatch training loss = 0.392 and accuracy of 0.87\n",
      "Iteration 54656: with minibatch training loss = 0.432 and accuracy of 0.86\n",
      "Epoch 140, Train loss: 0.573 and Train accuracy of 0.82, Test loss: 3.31 and Test accuracy of 0.406\n",
      "Iteration 54784: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 54912: with minibatch training loss = 0.542 and accuracy of 0.84\n",
      "Iteration 55040: with minibatch training loss = 0.571 and accuracy of 0.8\n",
      "Epoch 141, Train loss: 0.566 and Train accuracy of 0.824, Test loss: 3.17 and Test accuracy of 0.412\n",
      "Iteration 55168: with minibatch training loss = 0.517 and accuracy of 0.82\n",
      "Iteration 55296: with minibatch training loss = 0.483 and accuracy of 0.84\n",
      "Iteration 55424: with minibatch training loss = 0.658 and accuracy of 0.84\n",
      "Epoch 142, Train loss: 0.556 and Train accuracy of 0.825, Test loss: 3.11 and Test accuracy of 0.41\n",
      "Iteration 55552: with minibatch training loss = 0.626 and accuracy of 0.8\n",
      "Iteration 55680: with minibatch training loss = 0.483 and accuracy of 0.85\n",
      "Iteration 55808: with minibatch training loss = 0.784 and accuracy of 0.76\n",
      "Epoch 143, Train loss: 0.541 and Train accuracy of 0.829, Test loss: 3.32 and Test accuracy of 0.406\n",
      "Iteration 55936: with minibatch training loss = 0.644 and accuracy of 0.83\n",
      "Iteration 56064: with minibatch training loss = 0.48 and accuracy of 0.84\n",
      "Iteration 56192: with minibatch training loss = 0.47 and accuracy of 0.88\n",
      "Epoch 144, Train loss: 0.535 and Train accuracy of 0.833, Test loss: 3.25 and Test accuracy of 0.407\n",
      "Iteration 56320: with minibatch training loss = 0.63 and accuracy of 0.78\n",
      "Iteration 56448: with minibatch training loss = 0.417 and accuracy of 0.84\n",
      "Iteration 56576: with minibatch training loss = 0.452 and accuracy of 0.88\n",
      "Epoch 145, Train loss: 0.528 and Train accuracy of 0.834, Test loss: 3.32 and Test accuracy of 0.403\n",
      "Iteration 56704: with minibatch training loss = 0.493 and accuracy of 0.85\n",
      "Iteration 56832: with minibatch training loss = 0.532 and accuracy of 0.84\n",
      "Iteration 56960: with minibatch training loss = 0.418 and accuracy of 0.86\n",
      "Epoch 146, Train loss: 0.529 and Train accuracy of 0.834, Test loss: 3.35 and Test accuracy of 0.409\n",
      "Iteration 57088: with minibatch training loss = 0.535 and accuracy of 0.84\n",
      "Iteration 57216: with minibatch training loss = 0.514 and accuracy of 0.82\n",
      "Iteration 57344: with minibatch training loss = 0.651 and accuracy of 0.79\n",
      "Iteration 57472: with minibatch training loss = 0.52 and accuracy of 0.83\n",
      "Epoch 147, Train loss: 0.511 and Train accuracy of 0.839, Test loss: 3.28 and Test accuracy of 0.408\n",
      "Iteration 57600: with minibatch training loss = 0.491 and accuracy of 0.84\n",
      "Iteration 57728: with minibatch training loss = 0.362 and accuracy of 0.88\n",
      "Iteration 57856: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Epoch 148, Train loss: 0.507 and Train accuracy of 0.84, Test loss: 3.26 and Test accuracy of 0.407\n",
      "Iteration 57984: with minibatch training loss = 0.601 and accuracy of 0.86\n",
      "Iteration 58112: with minibatch training loss = 0.422 and accuracy of 0.9\n",
      "Iteration 58240: with minibatch training loss = 0.279 and accuracy of 0.88\n",
      "Epoch 149, Train loss: 0.499 and Train accuracy of 0.844, Test loss: 3.37 and Test accuracy of 0.41\n",
      "Iteration 58368: with minibatch training loss = 0.469 and accuracy of 0.86\n",
      "Iteration 58496: with minibatch training loss = 0.61 and accuracy of 0.84\n",
      "Iteration 58624: with minibatch training loss = 0.592 and accuracy of 0.8\n",
      "Epoch 150, Train loss: 0.491 and Train accuracy of 0.846, Test loss: 3.36 and Test accuracy of 0.404\n",
      "Iteration 58752: with minibatch training loss = 0.567 and accuracy of 0.84\n",
      "Iteration 58880: with minibatch training loss = 0.373 and accuracy of 0.88\n",
      "Iteration 59008: with minibatch training loss = 0.509 and accuracy of 0.84\n",
      "Epoch 151, Train loss: 0.483 and Train accuracy of 0.848, Test loss: 3.37 and Test accuracy of 0.408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59136: with minibatch training loss = 0.329 and accuracy of 0.88\n",
      "Iteration 59264: with minibatch training loss = 0.456 and accuracy of 0.88\n",
      "Iteration 59392: with minibatch training loss = 0.291 and accuracy of 0.91\n",
      "Epoch 152, Train loss: 0.476 and Train accuracy of 0.851, Test loss: 3.38 and Test accuracy of 0.412\n",
      "Iteration 59520: with minibatch training loss = 0.447 and accuracy of 0.86\n",
      "Iteration 59648: with minibatch training loss = 0.581 and accuracy of 0.77\n",
      "Iteration 59776: with minibatch training loss = 0.544 and accuracy of 0.77\n",
      "Epoch 153, Train loss: 0.467 and Train accuracy of 0.854, Test loss: 3.27 and Test accuracy of 0.407\n",
      "Iteration 59904: with minibatch training loss = 0.344 and accuracy of 0.89\n",
      "Iteration 60032: with minibatch training loss = 0.541 and accuracy of 0.85\n",
      "Iteration 60160: with minibatch training loss = 0.576 and accuracy of 0.8\n",
      "Epoch 154, Train loss: 0.459 and Train accuracy of 0.855, Test loss: 3.48 and Test accuracy of 0.406\n",
      "Iteration 60288: with minibatch training loss = 0.526 and accuracy of 0.82\n",
      "Iteration 60416: with minibatch training loss = 0.538 and accuracy of 0.84\n",
      "Iteration 60544: with minibatch training loss = 0.392 and accuracy of 0.88\n",
      "Epoch 155, Train loss: 0.454 and Train accuracy of 0.856, Test loss: 3.37 and Test accuracy of 0.408\n",
      "Iteration 60672: with minibatch training loss = 0.368 and accuracy of 0.9\n",
      "Iteration 60800: with minibatch training loss = 0.371 and accuracy of 0.9\n",
      "Iteration 60928: with minibatch training loss = 0.332 and accuracy of 0.9\n",
      "Epoch 156, Train loss: 0.447 and Train accuracy of 0.86, Test loss: 3.47 and Test accuracy of 0.408\n",
      "Iteration 61056: with minibatch training loss = 0.449 and accuracy of 0.86\n",
      "Iteration 61184: with minibatch training loss = 0.499 and accuracy of 0.84\n",
      "Iteration 61312: with minibatch training loss = 0.387 and accuracy of 0.86\n",
      "Epoch 157, Train loss: 0.447 and Train accuracy of 0.859, Test loss: 3.51 and Test accuracy of 0.405\n",
      "Iteration 61440: with minibatch training loss = 0.425 and accuracy of 0.84\n",
      "Iteration 61568: with minibatch training loss = 0.354 and accuracy of 0.91\n",
      "Iteration 61696: with minibatch training loss = 0.498 and accuracy of 0.84\n",
      "Epoch 158, Train loss: 0.436 and Train accuracy of 0.861, Test loss: 3.43 and Test accuracy of 0.404\n",
      "Iteration 61824: with minibatch training loss = 0.419 and accuracy of 0.87\n",
      "Iteration 61952: with minibatch training loss = 0.352 and accuracy of 0.91\n",
      "Iteration 62080: with minibatch training loss = 0.519 and accuracy of 0.84\n",
      "Epoch 159, Train loss: 0.424 and Train accuracy of 0.866, Test loss: 3.34 and Test accuracy of 0.412\n",
      "Iteration 62208: with minibatch training loss = 0.397 and accuracy of 0.88\n",
      "Iteration 62336: with minibatch training loss = 0.388 and accuracy of 0.88\n",
      "Iteration 62464: with minibatch training loss = 0.398 and accuracy of 0.88\n",
      "Epoch 160, Train loss: 0.421 and Train accuracy of 0.867, Test loss: 3.53 and Test accuracy of 0.406\n",
      "Iteration 62592: with minibatch training loss = 0.432 and accuracy of 0.9\n",
      "Iteration 62720: with minibatch training loss = 0.239 and accuracy of 0.91\n",
      "Iteration 62848: with minibatch training loss = 0.434 and accuracy of 0.84\n",
      "Epoch 161, Train loss: 0.417 and Train accuracy of 0.869, Test loss: 3.51 and Test accuracy of 0.407\n",
      "Iteration 62976: with minibatch training loss = 0.481 and accuracy of 0.87\n",
      "Iteration 63104: with minibatch training loss = 0.408 and accuracy of 0.85\n",
      "Iteration 63232: with minibatch training loss = 0.314 and accuracy of 0.91\n",
      "Epoch 162, Train loss: 0.413 and Train accuracy of 0.869, Test loss: 3.44 and Test accuracy of 0.41\n",
      "Iteration 63360: with minibatch training loss = 0.348 and accuracy of 0.89\n",
      "Iteration 63488: with minibatch training loss = 0.514 and accuracy of 0.85\n",
      "Iteration 63616: with minibatch training loss = 0.357 and accuracy of 0.89\n",
      "Epoch 163, Train loss: 0.413 and Train accuracy of 0.87, Test loss: 3.52 and Test accuracy of 0.407\n",
      "Iteration 63744: with minibatch training loss = 0.295 and accuracy of 0.93\n",
      "Iteration 63872: with minibatch training loss = 0.445 and accuracy of 0.84\n",
      "Iteration 64000: with minibatch training loss = 0.319 and accuracy of 0.91\n",
      "Epoch 164, Train loss: 0.396 and Train accuracy of 0.873, Test loss: 3.44 and Test accuracy of 0.408\n",
      "Iteration 64128: with minibatch training loss = 0.589 and accuracy of 0.8\n",
      "Iteration 64256: with minibatch training loss = 0.496 and accuracy of 0.81\n",
      "Iteration 64384: with minibatch training loss = 0.455 and accuracy of 0.84\n",
      "Iteration 64512: with minibatch training loss = 0.315 and accuracy of 0.89\n",
      "Epoch 165, Train loss: 0.4 and Train accuracy of 0.873, Test loss: 3.53 and Test accuracy of 0.411\n",
      "Iteration 64640: with minibatch training loss = 0.389 and accuracy of 0.91\n",
      "Iteration 64768: with minibatch training loss = 0.342 and accuracy of 0.89\n",
      "Iteration 64896: with minibatch training loss = 0.374 and accuracy of 0.85\n",
      "Epoch 166, Train loss: 0.386 and Train accuracy of 0.878, Test loss: 3.48 and Test accuracy of 0.407\n",
      "Iteration 65024: with minibatch training loss = 0.36 and accuracy of 0.88\n",
      "Iteration 65152: with minibatch training loss = 0.473 and accuracy of 0.88\n",
      "Iteration 65280: with minibatch training loss = 0.299 and accuracy of 0.92\n",
      "Epoch 167, Train loss: 0.39 and Train accuracy of 0.877, Test loss: 3.52 and Test accuracy of 0.408\n",
      "Iteration 65408: with minibatch training loss = 0.296 and accuracy of 0.92\n",
      "Iteration 65536: with minibatch training loss = 0.352 and accuracy of 0.88\n",
      "Iteration 65664: with minibatch training loss = 0.398 and accuracy of 0.89\n",
      "Epoch 168, Train loss: 0.381 and Train accuracy of 0.879, Test loss: 3.46 and Test accuracy of 0.408\n",
      "Iteration 65792: with minibatch training loss = 0.532 and accuracy of 0.83\n",
      "Iteration 65920: with minibatch training loss = 0.329 and accuracy of 0.91\n",
      "Iteration 66048: with minibatch training loss = 0.323 and accuracy of 0.93\n",
      "Epoch 169, Train loss: 0.376 and Train accuracy of 0.882, Test loss: 3.42 and Test accuracy of 0.41\n",
      "Iteration 66176: with minibatch training loss = 0.141 and accuracy of 0.95\n",
      "Iteration 66304: with minibatch training loss = 0.428 and accuracy of 0.85\n",
      "Iteration 66432: with minibatch training loss = 0.234 and accuracy of 0.93\n",
      "Epoch 170, Train loss: 0.382 and Train accuracy of 0.88, Test loss: 3.61 and Test accuracy of 0.412\n",
      "Iteration 66560: with minibatch training loss = 0.261 and accuracy of 0.9\n",
      "Iteration 66688: with minibatch training loss = 0.267 and accuracy of 0.92\n",
      "Iteration 66816: with minibatch training loss = 0.266 and accuracy of 0.93\n",
      "Epoch 171, Train loss: 0.37 and Train accuracy of 0.882, Test loss: 3.72 and Test accuracy of 0.405\n",
      "Iteration 66944: with minibatch training loss = 0.353 and accuracy of 0.88\n",
      "Iteration 67072: with minibatch training loss = 0.32 and accuracy of 0.91\n",
      "Iteration 67200: with minibatch training loss = 0.387 and accuracy of 0.87\n",
      "Epoch 172, Train loss: 0.363 and Train accuracy of 0.885, Test loss: 3.45 and Test accuracy of 0.41\n",
      "Iteration 67328: with minibatch training loss = 0.354 and accuracy of 0.91\n",
      "Iteration 67456: with minibatch training loss = 0.139 and accuracy of 0.96\n",
      "Iteration 67584: with minibatch training loss = 0.48 and accuracy of 0.87\n",
      "Epoch 173, Train loss: 0.351 and Train accuracy of 0.888, Test loss: 3.54 and Test accuracy of 0.41\n",
      "Iteration 67712: with minibatch training loss = 0.466 and accuracy of 0.86\n",
      "Iteration 67840: with minibatch training loss = 0.271 and accuracy of 0.9\n",
      "Iteration 67968: with minibatch training loss = 0.331 and accuracy of 0.86\n",
      "Epoch 174, Train loss: 0.353 and Train accuracy of 0.889, Test loss: 3.55 and Test accuracy of 0.408\n",
      "Iteration 68096: with minibatch training loss = 0.354 and accuracy of 0.92\n",
      "Iteration 68224: with minibatch training loss = 0.348 and accuracy of 0.88\n",
      "Iteration 68352: with minibatch training loss = 0.29 and accuracy of 0.87\n",
      "Epoch 175, Train loss: 0.354 and Train accuracy of 0.887, Test loss: 3.57 and Test accuracy of 0.409\n",
      "Iteration 68480: with minibatch training loss = 0.21 and accuracy of 0.95\n",
      "Iteration 68608: with minibatch training loss = 0.362 and accuracy of 0.88\n",
      "Iteration 68736: with minibatch training loss = 0.292 and accuracy of 0.91\n",
      "Epoch 176, Train loss: 0.338 and Train accuracy of 0.893, Test loss: 3.51 and Test accuracy of 0.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68864: with minibatch training loss = 0.247 and accuracy of 0.9\n",
      "Iteration 68992: with minibatch training loss = 0.397 and accuracy of 0.89\n",
      "Iteration 69120: with minibatch training loss = 0.28 and accuracy of 0.89\n",
      "Epoch 177, Train loss: 0.337 and Train accuracy of 0.894, Test loss: 3.53 and Test accuracy of 0.409\n",
      "Iteration 69248: with minibatch training loss = 0.38 and accuracy of 0.87\n",
      "Iteration 69376: with minibatch training loss = 0.371 and accuracy of 0.84\n",
      "Iteration 69504: with minibatch training loss = 0.394 and accuracy of 0.89\n",
      "Epoch 178, Train loss: 0.338 and Train accuracy of 0.893, Test loss: 3.61 and Test accuracy of 0.407\n",
      "Iteration 69632: with minibatch training loss = 0.268 and accuracy of 0.93\n",
      "Iteration 69760: with minibatch training loss = 0.244 and accuracy of 0.91\n",
      "Iteration 69888: with minibatch training loss = 0.431 and accuracy of 0.86\n",
      "Epoch 179, Train loss: 0.337 and Train accuracy of 0.894, Test loss: 3.57 and Test accuracy of 0.41\n",
      "Iteration 70016: with minibatch training loss = 0.358 and accuracy of 0.88\n",
      "Iteration 70144: with minibatch training loss = 0.396 and accuracy of 0.85\n",
      "Iteration 70272: with minibatch training loss = 0.463 and accuracy of 0.86\n",
      "Epoch 180, Train loss: 0.325 and Train accuracy of 0.897, Test loss: 3.59 and Test accuracy of 0.409\n",
      "Iteration 70400: with minibatch training loss = 0.27 and accuracy of 0.92\n",
      "Iteration 70528: with minibatch training loss = 0.201 and accuracy of 0.91\n",
      "Iteration 70656: with minibatch training loss = 0.266 and accuracy of 0.94\n",
      "Epoch 181, Train loss: 0.333 and Train accuracy of 0.894, Test loss: 3.58 and Test accuracy of 0.413\n",
      "Iteration 70784: with minibatch training loss = 0.258 and accuracy of 0.93\n",
      "Iteration 70912: with minibatch training loss = 0.34 and accuracy of 0.9\n",
      "Iteration 71040: with minibatch training loss = 0.297 and accuracy of 0.91\n",
      "Epoch 182, Train loss: 0.322 and Train accuracy of 0.897, Test loss: 3.6 and Test accuracy of 0.41\n",
      "Iteration 71168: with minibatch training loss = 0.423 and accuracy of 0.88\n",
      "Iteration 71296: with minibatch training loss = 0.333 and accuracy of 0.88\n",
      "Iteration 71424: with minibatch training loss = 0.354 and accuracy of 0.91\n",
      "Iteration 71552: with minibatch training loss = 0.178 and accuracy of 0.93\n",
      "Epoch 183, Train loss: 0.317 and Train accuracy of 0.9, Test loss: 3.63 and Test accuracy of 0.412\n",
      "Iteration 71680: with minibatch training loss = 0.218 and accuracy of 0.92\n",
      "Iteration 71808: with minibatch training loss = 0.316 and accuracy of 0.9\n",
      "Iteration 71936: with minibatch training loss = 0.183 and accuracy of 0.93\n",
      "Epoch 184, Train loss: 0.309 and Train accuracy of 0.903, Test loss: 3.64 and Test accuracy of 0.411\n",
      "Iteration 72064: with minibatch training loss = 0.238 and accuracy of 0.92\n",
      "Iteration 72192: with minibatch training loss = 0.479 and accuracy of 0.85\n",
      "Iteration 72320: with minibatch training loss = 0.358 and accuracy of 0.88\n",
      "Epoch 185, Train loss: 0.312 and Train accuracy of 0.901, Test loss: 3.72 and Test accuracy of 0.408\n",
      "Iteration 72448: with minibatch training loss = 0.446 and accuracy of 0.89\n",
      "Iteration 72576: with minibatch training loss = 0.427 and accuracy of 0.88\n",
      "Iteration 72704: with minibatch training loss = 0.256 and accuracy of 0.91\n",
      "Epoch 186, Train loss: 0.319 and Train accuracy of 0.899, Test loss: 3.43 and Test accuracy of 0.41\n",
      "Iteration 72832: with minibatch training loss = 0.218 and accuracy of 0.93\n",
      "Iteration 72960: with minibatch training loss = 0.292 and accuracy of 0.93\n",
      "Iteration 73088: with minibatch training loss = 0.272 and accuracy of 0.92\n",
      "Epoch 187, Train loss: 0.303 and Train accuracy of 0.906, Test loss: 3.49 and Test accuracy of 0.408\n",
      "Iteration 73216: with minibatch training loss = 0.495 and accuracy of 0.87\n",
      "Iteration 73344: with minibatch training loss = 0.342 and accuracy of 0.94\n",
      "Iteration 73472: with minibatch training loss = 0.371 and accuracy of 0.89\n",
      "Epoch 188, Train loss: 0.305 and Train accuracy of 0.904, Test loss: 3.61 and Test accuracy of 0.409\n",
      "Iteration 73600: with minibatch training loss = 0.202 and accuracy of 0.94\n",
      "Iteration 73728: with minibatch training loss = 0.155 and accuracy of 0.95\n",
      "Iteration 73856: with minibatch training loss = 0.153 and accuracy of 0.95\n",
      "Epoch 189, Train loss: 0.304 and Train accuracy of 0.904, Test loss: 3.69 and Test accuracy of 0.408\n",
      "Iteration 73984: with minibatch training loss = 0.219 and accuracy of 0.92\n",
      "Iteration 74112: with minibatch training loss = 0.265 and accuracy of 0.93\n",
      "Iteration 74240: with minibatch training loss = 0.256 and accuracy of 0.9\n",
      "Epoch 190, Train loss: 0.297 and Train accuracy of 0.904, Test loss: 3.64 and Test accuracy of 0.409\n",
      "Iteration 74368: with minibatch training loss = 0.323 and accuracy of 0.9\n",
      "Iteration 74496: with minibatch training loss = 0.337 and accuracy of 0.9\n",
      "Iteration 74624: with minibatch training loss = 0.339 and accuracy of 0.88\n",
      "Epoch 191, Train loss: 0.286 and Train accuracy of 0.908, Test loss: 3.74 and Test accuracy of 0.41\n",
      "Iteration 74752: with minibatch training loss = 0.328 and accuracy of 0.89\n",
      "Iteration 74880: with minibatch training loss = 0.209 and accuracy of 0.92\n",
      "Iteration 75008: with minibatch training loss = 0.411 and accuracy of 0.88\n",
      "Epoch 192, Train loss: 0.285 and Train accuracy of 0.909, Test loss: 3.54 and Test accuracy of 0.41\n",
      "Iteration 75136: with minibatch training loss = 0.261 and accuracy of 0.91\n",
      "Iteration 75264: with minibatch training loss = 0.471 and accuracy of 0.86\n",
      "Iteration 75392: with minibatch training loss = 0.331 and accuracy of 0.9\n",
      "Epoch 193, Train loss: 0.286 and Train accuracy of 0.909, Test loss: 3.68 and Test accuracy of 0.405\n",
      "Iteration 75520: with minibatch training loss = 0.259 and accuracy of 0.91\n",
      "Iteration 75648: with minibatch training loss = 0.23 and accuracy of 0.91\n",
      "Iteration 75776: with minibatch training loss = 0.272 and accuracy of 0.93\n",
      "Epoch 194, Train loss: 0.289 and Train accuracy of 0.909, Test loss: 3.8 and Test accuracy of 0.41\n",
      "Iteration 75904: with minibatch training loss = 0.276 and accuracy of 0.88\n",
      "Iteration 76032: with minibatch training loss = 0.256 and accuracy of 0.92\n",
      "Iteration 76160: with minibatch training loss = 0.388 and accuracy of 0.85\n",
      "Epoch 195, Train loss: 0.282 and Train accuracy of 0.912, Test loss: 3.73 and Test accuracy of 0.405\n",
      "Iteration 76288: with minibatch training loss = 0.286 and accuracy of 0.9\n",
      "Iteration 76416: with minibatch training loss = 0.214 and accuracy of 0.93\n",
      "Iteration 76544: with minibatch training loss = 0.242 and accuracy of 0.95\n",
      "Epoch 196, Train loss: 0.279 and Train accuracy of 0.913, Test loss: 3.64 and Test accuracy of 0.407\n",
      "Iteration 76672: with minibatch training loss = 0.25 and accuracy of 0.92\n",
      "Iteration 76800: with minibatch training loss = 0.35 and accuracy of 0.88\n",
      "Iteration 76928: with minibatch training loss = 0.462 and accuracy of 0.87\n",
      "Epoch 197, Train loss: 0.273 and Train accuracy of 0.913, Test loss: 3.7 and Test accuracy of 0.41\n",
      "Iteration 77056: with minibatch training loss = 0.231 and accuracy of 0.93\n",
      "Iteration 77184: with minibatch training loss = 0.167 and accuracy of 0.97\n",
      "Iteration 77312: with minibatch training loss = 0.157 and accuracy of 0.96\n",
      "Epoch 198, Train loss: 0.268 and Train accuracy of 0.915, Test loss: 3.59 and Test accuracy of 0.407\n",
      "Iteration 77440: with minibatch training loss = 0.197 and accuracy of 0.93\n",
      "Iteration 77568: with minibatch training loss = 0.19 and accuracy of 0.94\n",
      "Iteration 77696: with minibatch training loss = 0.313 and accuracy of 0.9\n",
      "Epoch 199, Train loss: 0.267 and Train accuracy of 0.915, Test loss: 3.66 and Test accuracy of 0.41\n",
      "Iteration 77824: with minibatch training loss = 0.168 and accuracy of 0.95\n",
      "Iteration 77952: with minibatch training loss = 0.323 and accuracy of 0.9\n",
      "Iteration 78080: with minibatch training loss = 0.248 and accuracy of 0.91\n",
      "Epoch 200, Train loss: 0.265 and Train accuracy of 0.916, Test loss: 3.72 and Test accuracy of 0.408\n",
      "1 day, 0:41:20.050621\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
