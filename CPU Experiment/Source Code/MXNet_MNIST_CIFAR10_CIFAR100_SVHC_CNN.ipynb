{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import LoggerYN as YN\n",
    "import scipy.io as sio\n",
    "import utilsYN as uYN\n",
    "from mxnet import nd, autograd, gluon\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(1)\n",
    "myDevice= mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "\n",
    "\n",
    "def initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    global Dataset    \n",
    "    global pbatchSize\n",
    "    global pnumClasses\n",
    "    global pEpochs\n",
    "    global pLearningRate\n",
    "    global pMomentum\n",
    "    global pWeightDecay\n",
    "    Dataset = dataset\n",
    "    pbatchSize = batchSize\n",
    "    pnumClasses = numClasses\n",
    "    pEpochs = epochs\n",
    "    pLearningRate = learningRate\n",
    "    pMomentum = momentum\n",
    "    pWeightDecay = weightDecay\n",
    "\n",
    "    \n",
    "def NormalizeData(x_train,x_test):\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "        return x_train, x_test\n",
    "    \n",
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
    "\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def loadData():\n",
    "    if(Dataset == \"mnist\"):\n",
    "        train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=transform),pbatchSize, shuffle=True)\n",
    "        test_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=transform),pbatchSize, shuffle=False)\n",
    "        return train_data, test_data\n",
    "    elif(Dataset ==  \"cifar10\"):\n",
    "        train_data = gluon.data.DataLoader(gluon.data.vision.CIFAR10(train=True, transform=transform),pbatchSize, shuffle=True)\n",
    "        test_data = gluon.data.DataLoader(gluon.data.vision.CIFAR10(train=False, transform=transform),pbatchSize, shuffle=False)\n",
    "        return train_data, test_data\n",
    "    elif(Dataset ==  \"cifar100\"):\n",
    "        train_data = gluon.data.DataLoader(gluon.data.vision.CIFAR100(train=True, transform=transform),pbatchSize, shuffle=True)\n",
    "        test_data = gluon.data.DataLoader(gluon.data.vision.CIFAR100(train=False, transform=transform),pbatchSize, shuffle=False)\n",
    "        return train_data, test_data\n",
    "    else:\n",
    "        # Add your own custom dataset\n",
    "        pass\n",
    "\n",
    "def loadDataSVHN(fname,extra=False):\n",
    "    \"\"\"Load the SVHN dataset (optionally with extra images)\n",
    "    Args:\n",
    "        fname: the path which contains the SVHN dataset .mat files\n",
    "        or you can download it from the following links\n",
    "        \n",
    "        extra (bool, optional): load extra training data\n",
    "    Returns:\n",
    "        Dataset: SVHN data (x_train, y_train), (x_test, y_test)\n",
    "    \"\"\"\n",
    "    def load_mat(fname):\n",
    "        data = sio.loadmat(fname)\n",
    "        X = data['X'].transpose(3, 0, 1, 2)\n",
    "        y = data['y'] % 10  # map label \"10\" --> \"0\"\n",
    "        return X, y\n",
    "\n",
    "    data = uYN.Dataset()\n",
    "    data.classes = np.arange(10)\n",
    "\n",
    "\n",
    "    X, y = load_mat(fname % 'train')\n",
    "    data.train_images = X\n",
    "    data.train_labels = y.reshape(-1)\n",
    "\n",
    "    X, y = load_mat(fname % 'test')\n",
    "    data.test_images = X\n",
    "    data.test_labels = y.reshape(-1)\n",
    "\n",
    "    new_x = data.train_images\n",
    "    new_y = data.train_labels\n",
    "    \n",
    "    if extra:\n",
    "        X, y = load_mat(fname % 'extra')\n",
    "        data.extra_images = X\n",
    "        data.extra_labels = y.reshape(-1)\n",
    "    \n",
    "        # Use extra dataset\n",
    "        new_x = data.extra_images\n",
    "        new_y = data.extra_labels\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test)  = (new_x,new_y),(data.test_images,data.test_labels)\n",
    "    \n",
    "    global imgRows\n",
    "    global imgCols\n",
    "    global inputShape\n",
    "    \n",
    "    imgRows = x_train.shape[1]\n",
    "    imgCols = x_train.shape[2]\n",
    "\n",
    "    try:\n",
    "        imgRGB_Dimensions = x_train.shape[3]\n",
    "    except Exception:\n",
    "        imgRGB_Dimensions = 1 #For Gray Scale Images\n",
    "\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_test = x_test.reshape(x_test.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train, x_test = NormalizeData(x_train, x_test)\n",
    "    inputShape = (imgRows, imgCols, imgRGB_Dimensions)\n",
    "    \n",
    "    \n",
    "    x_train = x_train.reshape(len(y_train), imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_train = nd.array(x_train, dtype=x_train.dtype)\n",
    "    label = nd.array(y_train,dtype=y_train.dtype)\n",
    "    \n",
    "    x_test = x_test.reshape(len(y_test), imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_test = nd.array(x_test, dtype=x_test.dtype)\n",
    "    labelTest = nd.array(y_test,dtype=y_test.dtype)\n",
    "    return x_train,label, x_test,labelTest\n",
    "\n",
    "\n",
    "def modelMNIST():\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Conv2D(channels=32, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=64, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        net.add(gluon.nn.Dropout(0.25))\n",
    "        net.add(gluon.nn.Flatten())\n",
    "        net.add(gluon.nn.Dense(128, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dropout(0.5))\n",
    "        net.add(gluon.nn.Dense(pnumClasses))\n",
    "    return net\n",
    "\n",
    "def modelCIFAR10():\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Conv2D(channels=32, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=32, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        net.add(gluon.nn.Dropout(0.25))\n",
    "        net.add(gluon.nn.Conv2D(channels=64, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=64, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        net.add(gluon.nn.Dropout(0.25))\n",
    "        net.add(gluon.nn.Flatten())\n",
    "        net.add(gluon.nn.Dense(512, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dropout(0.5))\n",
    "        net.add(gluon.nn.Dense(pnumClasses))\n",
    "    return  net\n",
    "\n",
    "def modelCIFAR100():\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Conv2D(channels=128, kernel_size=3,strides=1,padding=1, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=128, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        net.add(gluon.nn.Dropout(0.1))   \n",
    "        net.add(gluon.nn.Conv2D(channels=256, kernel_size=3,strides=1,padding=1, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=256, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        net.add(gluon.nn.Dropout(0.25))\n",
    "        net.add(gluon.nn.Conv2D(channels=512, kernel_size=3,strides=1,padding=1, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=512, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        net.add(gluon.nn.Dropout(0.5))\n",
    "        net.add(gluon.nn.Flatten())\n",
    "        net.add(gluon.nn.Dense(1024, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dropout(0.5))\n",
    "        net.add(gluon.nn.Dense(pnumClasses * 10))\n",
    "    return  net\n",
    "\n",
    "def model_SVHN():\n",
    "    \n",
    "    model_SVHN = gluon.nn.Sequential()\n",
    "    with model_SVHN.name_scope():\n",
    "        model_SVHN.add(gluon.nn.Conv2D(channels=48, kernel_size=5,strides=1,padding=2, activation='relu'))\n",
    "        model_SVHN.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        model_SVHN.add(gluon.nn.Dropout(0.2))\n",
    "        model_SVHN.add(gluon.nn.Conv2D(channels=64, kernel_size=5,strides=1,padding=2))\n",
    "        model_SVHN.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "        model_SVHN.add(gluon.nn.Activation(activation='relu'))\n",
    "        model_SVHN.add(gluon.nn.MaxPool2D(pool_size=2, strides=1))\n",
    "        model_SVHN.add(gluon.nn.Dropout(0.2))\n",
    "        model_SVHN.add(gluon.nn.Conv2D(channels=128, kernel_size=5,strides=1,padding=2))\n",
    "        model_SVHN.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "        model_SVHN.add(gluon.nn.Activation(activation='relu'))\n",
    "        model_SVHN.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        model_SVHN.add(gluon.nn.Dropout(0.2))\n",
    "        model_SVHN.add(gluon.nn.Conv2D(channels=160, kernel_size=5,strides=1,padding=2))\n",
    "        model_SVHN.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "        model_SVHN.add(gluon.nn.Activation(activation='relu'))\n",
    "        model_SVHN.add(gluon.nn.MaxPool2D(pool_size=2, strides=1))\n",
    "        model_SVHN.add(gluon.nn.Dropout(0.2)) \n",
    "        model_SVHN.add(gluon.nn.Conv2D(channels=192, kernel_size=5,strides=1,padding=2))\n",
    "        model_SVHN.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "        model_SVHN.add(gluon.nn.Activation(activation='relu'))\n",
    "        model_SVHN.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        model_SVHN.add(gluon.nn.Dropout(0.2))\n",
    "        model_SVHN.add(gluon.nn.Conv2D(channels=192, kernel_size=5,strides=1,padding=2))\n",
    "        model_SVHN.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "        model_SVHN.add(gluon.nn.Activation(activation='relu'))\n",
    "        model_SVHN.add(gluon.nn.MaxPool2D(pool_size=2, strides=1))\n",
    "        model_SVHN.add(gluon.nn.Dropout(0.2))\n",
    "        model_SVHN.add(gluon.nn.Conv2D(channels=192, kernel_size=3,strides=1,padding=2))\n",
    "        model_SVHN.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "        model_SVHN.add(gluon.nn.Activation(activation='relu'))\n",
    "        model_SVHN.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        model_SVHN.add(gluon.nn.Dropout(0.2))\n",
    "        model_SVHN.add(gluon.nn.Conv2D(channels=192, kernel_size=5,strides=1,padding=2))\n",
    "        model_SVHN.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "        model_SVHN.add(gluon.nn.Activation(activation='relu'))\n",
    "        model_SVHN.add(gluon.nn.MaxPool2D(pool_size=2, strides=1))\n",
    "        model_SVHN.add(gluon.nn.Dropout(0.2))\n",
    "        model_SVHN.add(gluon.nn.Flatten())\n",
    "        model_SVHN.add(gluon.nn.Dense(3072, activation=\"relu\"))\n",
    "        model_SVHN.add(gluon.nn.Dense(pnumClasses))\n",
    "    return model_SVHN        \n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(myDevice)\n",
    "        label = label.as_in_context(myDevice)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]\n",
    "\n",
    "def evaluate_accuracySVHN(X,Y,batchSize, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    i =0 \n",
    "    for data, label in zip(batch(X, batchSize),batch(Y, batchSize)):\n",
    "        data = data.as_in_context(myDevice)\n",
    "        label = label.as_in_context(myDevice)\n",
    "        data = np.transpose(data,(0,3,1,2))\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "        i=i+1\n",
    "    return acc.get()[1]\n",
    "        \n",
    "\n",
    "\n",
    "def RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    train_data, test_data = loadData()\n",
    "    \n",
    "    #we didn’t have to include the softmax layer \n",
    "    #because MXNet’s has an efficient function that simultaneously computes \n",
    "    #the softmax activation and cross-entropy loss\n",
    "    \n",
    "    #Get Model\n",
    "    net = modelMNIST()\n",
    "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=myDevice)\n",
    "    \n",
    "    #Optimizer settings\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learningRate, 'momentum': momentum,'wd': weightDecay})\n",
    "    \n",
    "    print(\"MNIST Training Started.....\")\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"MXNET\",\"MNIST\")\n",
    "    start = time.time()\n",
    "    for e in range(pEpochs):\n",
    "        for i, (data, label) in list(enumerate(train_data)):\n",
    "            if ((i % batchSize) == 0) or (i == 0):\n",
    "                print(\"Epoch: \"+str(e) , \" Batch (\",i,\")\")        \n",
    "            data = data.as_in_context(myDevice)\n",
    "            label = label.as_in_context(myDevice)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = gluon.loss.SoftmaxCrossEntropyLoss()(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            curr_loss = nd.mean(loss).asscalar()\n",
    "        \n",
    "        for i, (data, label) in list(enumerate(test_data)):\n",
    "            data = data.as_in_context(myDevice)\n",
    "            label = label.as_in_context(myDevice)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss2 = gluon.loss.SoftmaxCrossEntropyLoss()(output, label)\n",
    "            curr_loss2 = nd.mean(loss2).asscalar()\n",
    "            \n",
    "        print(\"Calculating metrics.....\")\n",
    "        test_accuracy = evaluate_accuracy(test_data, net)\n",
    "        train_accuracy = evaluate_accuracy(train_data, net)\n",
    "        print(\"Epoch %s. Test Loss:%s, Train Loss: %s, Train_acc: %s, Test_acc: %s\" % (e, curr_loss2,curr_loss, train_accuracy, test_accuracy))\n",
    "    end = time.time()\n",
    "    print(\"MNIST Training Finished.....\")\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "        \n",
    "def RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    train_data, test_data = loadData()\n",
    "    \n",
    "    #we didn’t have to include the softmax layer \n",
    "    #because MXNet’s has an efficient function that simultaneously computes \n",
    "    #the softmax activation and cross-entropy loss\n",
    "    \n",
    "    #Get Model\n",
    "    net = modelCIFAR10()\n",
    "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=myDevice)\n",
    "    \n",
    "    #Optimizer settings\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learningRate, 'momentum': momentum,'wd': weightDecay})\n",
    "    print(\"CIFAR10 Training Started.....\")\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"MXNET\",\"CIFAR10\")\n",
    "    start = time.time()\n",
    "    for e in range(pEpochs):\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            if ((i % batchSize) == 0) or (i == 0):\n",
    "                print(\"Epoch: \"+str(e) , \" Batch (\",i,\")\")     \n",
    "            data = data.as_in_context(myDevice)\n",
    "            label = label.as_in_context(myDevice)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = gluon.loss.SoftmaxCrossEntropyLoss()(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            curr_loss = nd.mean(loss).asscalar()\n",
    "        for i, (data, label) in list(enumerate(test_data)):\n",
    "            data = data.as_in_context(myDevice)\n",
    "            label = label.as_in_context(myDevice)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss2 = gluon.loss.SoftmaxCrossEntropyLoss()(output, label)\n",
    "            curr_loss2 = nd.mean(loss2).asscalar()\n",
    "            \n",
    "        print(\"Calculating metrics.....\")\n",
    "        test_accuracy = evaluate_accuracy(test_data, net)\n",
    "        train_accuracy = evaluate_accuracy(train_data, net)\n",
    "        print(\"Epoch %s. Test Loss:%s, Train Loss: %s, Train_acc: %s, Test_acc: %s\" % (e, curr_loss2,curr_loss, train_accuracy, test_accuracy))\n",
    "    end = time.time()\n",
    "    print(\"CIFAR10 Training Finished.....\")\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "\n",
    "\n",
    "def RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    train_data, test_data = loadData()\n",
    "\n",
    "    #we didn’t have to include the softmax layer \n",
    "    #because MXNet’s has an efficient function that simultaneously computes \n",
    "    #the softmax activation and cross-entropy loss\n",
    "    \n",
    "    #Get Model\n",
    "    net = modelCIFAR100()\n",
    "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=myDevice)\n",
    "    \n",
    "    #Optimizer settings\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learningRate, 'momentum': momentum,'wd': weightDecay})\n",
    "    print(\"CIFAR100 Training Started.....\")\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"MXNET\",\"CIFAR100\")\n",
    "    start = time.time()\n",
    "    for e in range(pEpochs):\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            if ((i % batchSize) == 0) or (i == 0):\n",
    "                print(\"Epoch: \"+str(e) , \" Batch (\",i,\")\")         \n",
    "            data = data.as_in_context(myDevice)\n",
    "            label = label.as_in_context(myDevice)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = gluon.loss.SoftmaxCrossEntropyLoss()(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            curr_loss = nd.mean(loss).asscalar()\n",
    "        for i, (data, label) in list(enumerate(test_data)):\n",
    "            data = data.as_in_context(myDevice)\n",
    "            label = label.as_in_context(myDevice)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss2 = gluon.loss.SoftmaxCrossEntropyLoss()(output, label)\n",
    "            curr_loss2 = nd.mean(loss2).asscalar()\n",
    "            \n",
    "        print(\"Calculating metrics.....\")\n",
    "        test_accuracy = evaluate_accuracy(test_data, net)\n",
    "        train_accuracy = evaluate_accuracy(train_data, net)\n",
    "        print(\"Epoch %s. Test Loss:%s, Train Loss: %s, Train_acc: %s, Test_acc: %s\" % (e, curr_loss2,curr_loss, train_accuracy, test_accuracy))\n",
    "    end = time.time()\n",
    "    print(\"CIFAR100 Training Finished.....\")\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "\n",
    "        \n",
    "def RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay,fname):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    train_data_x,train_data_y, test_data_x,test_data_y = loadDataSVHN(fname,True)\n",
    "   \n",
    "    #Get Model\n",
    "    net = model_SVHN()\n",
    "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=myDevice)\n",
    "    \n",
    "    #Optimizer settings\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learningRate, 'momentum': momentum,'wd': weightDecay})\n",
    "    \n",
    "\n",
    "    print(\"SVHN Training Started.....\")\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"MXNET\",\"SVHN\")\n",
    "    start = time.time()\n",
    "    for e in range(pEpochs):\n",
    "        i=0\n",
    "        for data, label in zip(batch(train_data_x, batchSize),batch(train_data_y, batchSize)):\n",
    "            if ((i % batchSize) == 0) or (i == 0):\n",
    "                print(\"Epoch: \"+str(e) , \" Batch (\",i,\")\") \n",
    "            data = data.as_in_context(myDevice)\n",
    "            label = label.as_in_context(myDevice)\n",
    "            data = np.transpose(data,(0,3,1,2))\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = gluon.loss.SoftmaxCrossEntropyLoss()(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            curr_loss = nd.mean(loss).asscalar()\n",
    "            i=i+1\n",
    "        for data, label in zip(batch(test_data_x, batchSize),batch(test_data_y, batchSize)):\n",
    "            data = data.as_in_context(myDevice)\n",
    "            label = label.as_in_context(myDevice)\n",
    "            data = np.transpose(data,(0,3,1,2))\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss2 = gluon.loss.SoftmaxCrossEntropyLoss()(output, label)\n",
    "            curr_loss2 = nd.mean(loss2).asscalar()\n",
    "            \n",
    "        print(\"Calculating metrics.....\")\n",
    "        test_accuracy = evaluate_accuracySVHN(test_data_x,test_data_y,batchSize, net)\n",
    "        train_accuracy = evaluate_accuracySVHN(train_data_x,train_data_y,batchSize, net)\n",
    "        print(\"Epoch %s. Test Loss:%s, Train Loss: %s, Train_acc: %s, Test_acc: %s\" % (e, curr_loss2,curr_loss, train_accuracy, test_accuracy))  \n",
    "    end = time.time()\n",
    "    print(\"SVHN Training Finished.....\")\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "\n",
    "def runModel(dataset,batchSize=128,numClasses=10,epochs=12,learningRate=0.01,momentum=0.5,weightDecay=1e-6):\n",
    "    if dataset is \"mnist\":\n",
    "        RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar10\":\n",
    "        RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar100\":\n",
    "        RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"SVHN\":\n",
    "        fname = './%s_32x32.mat'\n",
    "        RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay,fname) \n",
    "    else:\n",
    "        print(\"Choose cifar10 or mnist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #runModel(\"mnist\",epochs=1)\n",
    "    #runModel(\"cifar10\",epochs=1)\n",
    "    #runModel(\"SVHN\",epochs=1)\n",
    "    runModel(\"cifar100\",epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
