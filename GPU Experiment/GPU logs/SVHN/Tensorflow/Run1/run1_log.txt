Training
Iteration 0: with minibatch training loss = 2.45 and accuracy of 0.094
Iteration 128: with minibatch training loss = 2.26 and accuracy of 0.17
Iteration 256: with minibatch training loss = 1.76 and accuracy of 0.34
Iteration 384: with minibatch training loss = 1.11 and accuracy of 0.66
Iteration 512: with minibatch training loss = 0.885 and accuracy of 0.7
Iteration 640: with minibatch training loss = 0.673 and accuracy of 0.81
Iteration 768: with minibatch training loss = 0.646 and accuracy of 0.84
Iteration 896: with minibatch training loss = 0.422 and accuracy of 0.89
Iteration 1024: with minibatch training loss = 0.397 and accuracy of 0.88
Iteration 1152: with minibatch training loss = 0.312 and accuracy of 0.91
Iteration 1280: with minibatch training loss = 0.344 and accuracy of 0.9
Iteration 1408: with minibatch training loss = 0.341 and accuracy of 0.9
Iteration 1536: with minibatch training loss = 0.468 and accuracy of 0.89
Iteration 1664: with minibatch training loss = 0.251 and accuracy of 0.93
Iteration 1792: with minibatch training loss = 0.322 and accuracy of 0.9
Iteration 1920: with minibatch training loss = 0.233 and accuracy of 0.94
Iteration 2048: with minibatch training loss = 0.27 and accuracy of 0.91
Iteration 2176: with minibatch training loss = 0.354 and accuracy of 0.94
Iteration 2304: with minibatch training loss = 0.239 and accuracy of 0.92
Iteration 2432: with minibatch training loss = 0.153 and accuracy of 0.96
Iteration 2560: with minibatch training loss = 0.342 and accuracy of 0.89
Iteration 2688: with minibatch training loss = 0.281 and accuracy of 0.94
Iteration 2816: with minibatch training loss = 0.27 and accuracy of 0.91
Iteration 2944: with minibatch training loss = 0.293 and accuracy of 0.91
Iteration 3072: with minibatch training loss = 0.238 and accuracy of 0.93
Iteration 3200: with minibatch training loss = 0.226 and accuracy of 0.93
Iteration 3328: with minibatch training loss = 0.267 and accuracy of 0.92
Iteration 3456: with minibatch training loss = 0.289 and accuracy of 0.91
Iteration 3584: with minibatch training loss = 0.184 and accuracy of 0.93
Iteration 3712: with minibatch training loss = 0.147 and accuracy of 0.96
Iteration 3840: with minibatch training loss = 0.227 and accuracy of 0.93
Iteration 3968: with minibatch training loss = 0.261 and accuracy of 0.91
Iteration 4096: with minibatch training loss = 0.308 and accuracy of 0.91
Epoch 1, Train loss: 0.484 and Train accuracy of 0.842, Test loss: 0.302 and Test accuracy of 0.913
Iteration 4224: with minibatch training loss = 0.188 and accuracy of 0.94
Iteration 4352: with minibatch training loss = 0.135 and accuracy of 0.95
Iteration 4480: with minibatch training loss = 0.253 and accuracy of 0.95
Iteration 4608: with minibatch training loss = 0.306 and accuracy of 0.91
Iteration 4736: with minibatch training loss = 0.199 and accuracy of 0.95
Iteration 4864: with minibatch training loss = 0.262 and accuracy of 0.92
Iteration 4992: with minibatch training loss = 0.209 and accuracy of 0.93
Iteration 5120: with minibatch training loss = 0.268 and accuracy of 0.92
Iteration 5248: with minibatch training loss = 0.0745 and accuracy of 0.98
Iteration 5376: with minibatch training loss = 0.149 and accuracy of 0.95
Iteration 5504: with minibatch training loss = 0.116 and accuracy of 0.97
Iteration 5632: with minibatch training loss = 0.178 and accuracy of 0.95
Iteration 5760: with minibatch training loss = 0.139 and accuracy of 0.96
Iteration 5888: with minibatch training loss = 0.131 and accuracy of 0.96
Iteration 6016: with minibatch training loss = 0.196 and accuracy of 0.94
Iteration 6144: with minibatch training loss = 0.0755 and accuracy of 0.98
Iteration 6272: with minibatch training loss = 0.135 and accuracy of 0.95
Iteration 6400: with minibatch training loss = 0.133 and accuracy of 0.95
Iteration 6528: with minibatch training loss = 0.0536 and accuracy of 0.99
Iteration 6656: with minibatch training loss = 0.247 and accuracy of 0.95
Iteration 6784: with minibatch training loss = 0.224 and accuracy of 0.95
Iteration 6912: with minibatch training loss = 0.243 and accuracy of 0.95
Iteration 7040: with minibatch training loss = 0.208 and accuracy of 0.92
Iteration 7168: with minibatch training loss = 0.0782 and accuracy of 0.96
Iteration 7296: with minibatch training loss = 0.244 and accuracy of 0.94
Iteration 7424: with minibatch training loss = 0.119 and accuracy of 0.97
Iteration 7552: with minibatch training loss = 0.15 and accuracy of 0.94
Iteration 7680: with minibatch training loss = 0.262 and accuracy of 0.94
Iteration 7808: with minibatch training loss = 0.155 and accuracy of 0.95
Iteration 7936: with minibatch training loss = 0.0744 and accuracy of 0.98
Iteration 8064: with minibatch training loss = 0.15 and accuracy of 0.97
Iteration 8192: with minibatch training loss = 0.0452 and accuracy of 0.99
Epoch 2, Train loss: 0.168 and Train accuracy of 0.952, Test loss: 0.234 and Test accuracy of 0.935
Iteration 8320: with minibatch training loss = 0.112 and accuracy of 0.95
Iteration 8448: with minibatch training loss = 0.175 and accuracy of 0.95
Iteration 8576: with minibatch training loss = 0.172 and accuracy of 0.95
Iteration 8704: with minibatch training loss = 0.126 and accuracy of 0.95
Iteration 8832: with minibatch training loss = 0.0523 and accuracy of 0.98
Iteration 8960: with minibatch training loss = 0.262 and accuracy of 0.92
Iteration 9088: with minibatch training loss = 0.124 and accuracy of 0.95
Iteration 9216: with minibatch training loss = 0.0336 and accuracy of 1
Iteration 9344: with minibatch training loss = 0.116 and accuracy of 0.97
Iteration 9472: with minibatch training loss = 0.158 and accuracy of 0.95
Iteration 9600: with minibatch training loss = 0.127 and accuracy of 0.95
Iteration 9728: with minibatch training loss = 0.199 and accuracy of 0.94
Iteration 9856: with minibatch training loss = 0.0848 and accuracy of 0.97
Iteration 9984: with minibatch training loss = 0.209 and accuracy of 0.91
Iteration 10112: with minibatch training loss = 0.203 and accuracy of 0.94
Iteration 10240: with minibatch training loss = 0.141 and accuracy of 0.95
Iteration 10368: with minibatch training loss = 0.103 and accuracy of 0.95
Iteration 10496: with minibatch training loss = 0.236 and accuracy of 0.94
Iteration 10624: with minibatch training loss = 0.0332 and accuracy of 0.99
Iteration 10752: with minibatch training loss = 0.0333 and accuracy of 0.99
Iteration 10880: with minibatch training loss = 0.0774 and accuracy of 0.97
Iteration 11008: with minibatch training loss = 0.0798 and accuracy of 0.98
Iteration 11136: with minibatch training loss = 0.216 and accuracy of 0.93
Iteration 11264: with minibatch training loss = 0.166 and accuracy of 0.94
Iteration 11392: with minibatch training loss = 0.0322 and accuracy of 0.99
Iteration 11520: with minibatch training loss = 0.0452 and accuracy of 0.98
Iteration 11648: with minibatch training loss = 0.0687 and accuracy of 0.98
Iteration 11776: with minibatch training loss = 0.115 and accuracy of 0.98
Iteration 11904: with minibatch training loss = 0.228 and accuracy of 0.95
Iteration 12032: with minibatch training loss = 0.0764 and accuracy of 0.97
Iteration 12160: with minibatch training loss = 0.168 and accuracy of 0.94
Iteration 12288: with minibatch training loss = 0.0981 and accuracy of 0.97
Iteration 12416: with minibatch training loss = 0.234 and accuracy of 0.95
Epoch 3, Train loss: 0.131 and Train accuracy of 0.963, Test loss: 0.21 and Test accuracy of 0.944
Iteration 12544: with minibatch training loss = 0.204 and accuracy of 0.95
Iteration 12672: with minibatch training loss = 0.139 and accuracy of 0.97
Iteration 12800: with minibatch training loss = 0.13 and accuracy of 0.94
Iteration 12928: with minibatch training loss = 0.118 and accuracy of 0.96
Iteration 13056: with minibatch training loss = 0.168 and accuracy of 0.94
Iteration 13184: with minibatch training loss = 0.113 and accuracy of 0.97
Iteration 13312: with minibatch training loss = 0.0995 and accuracy of 0.98
Iteration 13440: with minibatch training loss = 0.134 and accuracy of 0.97
Iteration 13568: with minibatch training loss = 0.0609 and accuracy of 0.99
Iteration 13696: with minibatch training loss = 0.107 and accuracy of 0.98
Iteration 13824: with minibatch training loss = 0.0843 and accuracy of 0.98
Iteration 13952: with minibatch training loss = 0.129 and accuracy of 0.98
Iteration 14080: with minibatch training loss = 0.138 and accuracy of 0.95
Iteration 14208: with minibatch training loss = 0.114 and accuracy of 0.96
Iteration 14336: with minibatch training loss = 0.0719 and accuracy of 0.97
Iteration 14464: with minibatch training loss = 0.0832 and accuracy of 0.98
Iteration 14592: with minibatch training loss = 0.193 and accuracy of 0.96
Iteration 14720: with minibatch training loss = 0.195 and accuracy of 0.95
Iteration 14848: with minibatch training loss = 0.0983 and accuracy of 0.96
Iteration 14976: with minibatch training loss = 0.0669 and accuracy of 0.98
Iteration 15104: with minibatch training loss = 0.142 and accuracy of 0.97
Iteration 15232: with minibatch training loss = 0.184 and accuracy of 0.97
Iteration 15360: with minibatch training loss = 0.132 and accuracy of 0.98
Iteration 15488: with minibatch training loss = 0.208 and accuracy of 0.95
Iteration 15616: with minibatch training loss = 0.0857 and accuracy of 0.98
Iteration 15744: with minibatch training loss = 0.123 and accuracy of 0.96
Iteration 15872: with minibatch training loss = 0.143 and accuracy of 0.96
Iteration 16000: with minibatch training loss = 0.134 and accuracy of 0.96
Iteration 16128: with minibatch training loss = 0.112 and accuracy of 0.97
Iteration 16256: with minibatch training loss = 0.116 and accuracy of 0.95
Iteration 16384: with minibatch training loss = 0.0803 and accuracy of 0.98
Iteration 16512: with minibatch training loss = 0.114 and accuracy of 0.95
Epoch 4, Train loss: 0.113 and Train accuracy of 0.968, Test loss: 0.194 and Test accuracy of 0.949
Iteration 16640: with minibatch training loss = 0.142 and accuracy of 0.96
Iteration 16768: with minibatch training loss = 0.0827 and accuracy of 0.98
Iteration 16896: with minibatch training loss = 0.0539 and accuracy of 0.98
Iteration 17024: with minibatch training loss = 0.157 and accuracy of 0.96
Iteration 17152: with minibatch training loss = 0.129 and accuracy of 0.97
Iteration 17280: with minibatch training loss = 0.0603 and accuracy of 0.98
Iteration 17408: with minibatch training loss = 0.163 and accuracy of 0.93
Iteration 17536: with minibatch training loss = 0.114 and accuracy of 0.96
Iteration 17664: with minibatch training loss = 0.102 and accuracy of 0.98
Iteration 17792: with minibatch training loss = 0.0731 and accuracy of 0.98
Iteration 17920: with minibatch training loss = 0.0329 and accuracy of 0.99
Iteration 18048: with minibatch training loss = 0.0745 and accuracy of 0.99
Iteration 18176: with minibatch training loss = 0.121 and accuracy of 0.96
Iteration 18304: with minibatch training loss = 0.0802 and accuracy of 0.98
Iteration 18432: with minibatch training loss = 0.127 and accuracy of 0.95
Iteration 18560: with minibatch training loss = 0.166 and accuracy of 0.95
Iteration 18688: with minibatch training loss = 0.12 and accuracy of 0.97
Iteration 18816: with minibatch training loss = 0.0799 and accuracy of 0.97
Iteration 18944: with minibatch training loss = 0.256 and accuracy of 0.95
Iteration 19072: with minibatch training loss = 0.0833 and accuracy of 0.97
Iteration 19200: with minibatch training loss = 0.146 and accuracy of 0.95
Iteration 19328: with minibatch training loss = 0.0914 and accuracy of 0.98
Iteration 19456: with minibatch training loss = 0.179 and accuracy of 0.94
Iteration 19584: with minibatch training loss = 0.0808 and accuracy of 0.98
Iteration 19712: with minibatch training loss = 0.109 and accuracy of 0.97
Iteration 19840: with minibatch training loss = 0.182 and accuracy of 0.95
Iteration 19968: with minibatch training loss = 0.0391 and accuracy of 0.99
Iteration 20096: with minibatch training loss = 0.187 and accuracy of 0.95
Iteration 20224: with minibatch training loss = 0.084 and accuracy of 0.98
Iteration 20352: with minibatch training loss = 0.091 and accuracy of 0.97
Iteration 20480: with minibatch training loss = 0.0758 and accuracy of 0.98
Iteration 20608: with minibatch training loss = 0.0977 and accuracy of 0.98
Iteration 20736: with minibatch training loss = 0.127 and accuracy of 0.96
Epoch 5, Train loss: 0.101 and Train accuracy of 0.972, Test loss: 0.193 and Test accuracy of 0.948
Iteration 20864: with minibatch training loss = 0.173 and accuracy of 0.95
Iteration 20992: with minibatch training loss = 0.053 and accuracy of 0.99
Iteration 21120: with minibatch training loss = 0.157 and accuracy of 0.94
Iteration 21248: with minibatch training loss = 0.0703 and accuracy of 0.98
Iteration 21376: with minibatch training loss = 0.14 and accuracy of 0.95
Iteration 21504: with minibatch training loss = 0.139 and accuracy of 0.95
Iteration 21632: with minibatch training loss = 0.104 and accuracy of 0.97
Iteration 21760: with minibatch training loss = 0.0789 and accuracy of 0.98
Iteration 21888: with minibatch training loss = 0.218 and accuracy of 0.96
Iteration 22016: with minibatch training loss = 0.0797 and accuracy of 0.98
Iteration 22144: with minibatch training loss = 0.0569 and accuracy of 0.99
Iteration 22272: with minibatch training loss = 0.0286 and accuracy of 0.99
Iteration 22400: with minibatch training loss = 0.0853 and accuracy of 0.97
Iteration 22528: with minibatch training loss = 0.0998 and accuracy of 0.98
Iteration 22656: with minibatch training loss = 0.0522 and accuracy of 0.98
Iteration 22784: with minibatch training loss = 0.122 and accuracy of 0.96
Iteration 22912: with minibatch training loss = 0.146 and accuracy of 0.97
Iteration 23040: with minibatch training loss = 0.0698 and accuracy of 0.98
Iteration 23168: with minibatch training loss = 0.163 and accuracy of 0.96
Iteration 23296: with minibatch training loss = 0.128 and accuracy of 0.95
Iteration 23424: with minibatch training loss = 0.0855 and accuracy of 0.98
Iteration 23552: with minibatch training loss = 0.0483 and accuracy of 0.98
Iteration 23680: with minibatch training loss = 0.0602 and accuracy of 0.96
Iteration 23808: with minibatch training loss = 0.0857 and accuracy of 0.97
Iteration 23936: with minibatch training loss = 0.0965 and accuracy of 0.98
Iteration 24064: with minibatch training loss = 0.0739 and accuracy of 0.98
Iteration 24192: with minibatch training loss = 0.0417 and accuracy of 0.99
Iteration 24320: with minibatch training loss = 0.0347 and accuracy of 0.98
Iteration 24448: with minibatch training loss = 0.0342 and accuracy of 1
Iteration 24576: with minibatch training loss = 0.0999 and accuracy of 0.98
Iteration 24704: with minibatch training loss = 0.0625 and accuracy of 0.98
Iteration 24832: with minibatch training loss = 0.12 and accuracy of 0.98
Epoch 6, Train loss: 0.0931 and Train accuracy of 0.974, Test loss: 0.177 and Test accuracy of 0.954
Iteration 24960: with minibatch training loss = 0.0408 and accuracy of 0.98
Iteration 25088: with minibatch training loss = 0.097 and accuracy of 0.97
Iteration 25216: with minibatch training loss = 0.0961 and accuracy of 0.97
Iteration 25344: with minibatch training loss = 0.0616 and accuracy of 0.97
Iteration 25472: with minibatch training loss = 0.0452 and accuracy of 0.98
Iteration 25600: with minibatch training loss = 0.145 and accuracy of 0.97
Iteration 25728: with minibatch training loss = 0.0719 and accuracy of 0.97
Iteration 25856: with minibatch training loss = 0.0476 and accuracy of 0.98
Iteration 25984: with minibatch training loss = 0.0637 and accuracy of 0.97
Iteration 26112: with minibatch training loss = 0.0181 and accuracy of 1
Iteration 26240: with minibatch training loss = 0.0967 and accuracy of 0.98
Iteration 26368: with minibatch training loss = 0.133 and accuracy of 0.98
Iteration 26496: with minibatch training loss = 0.0988 and accuracy of 0.97
Iteration 26624: with minibatch training loss = 0.147 and accuracy of 0.95
Iteration 26752: with minibatch training loss = 0.113 and accuracy of 0.96
Iteration 26880: with minibatch training loss = 0.0866 and accuracy of 0.97
Iteration 27008: with minibatch training loss = 0.0996 and accuracy of 0.95
Iteration 27136: with minibatch training loss = 0.164 and accuracy of 0.97
Iteration 27264: with minibatch training loss = 0.0756 and accuracy of 0.97
Iteration 27392: with minibatch training loss = 0.0369 and accuracy of 0.98
Iteration 27520: with minibatch training loss = 0.11 and accuracy of 0.97
Iteration 27648: with minibatch training loss = 0.0445 and accuracy of 0.99
Iteration 27776: with minibatch training loss = 0.116 and accuracy of 0.98
Iteration 27904: with minibatch training loss = 0.0794 and accuracy of 0.95
Iteration 28032: with minibatch training loss = 0.128 and accuracy of 0.95
Iteration 28160: with minibatch training loss = 0.146 and accuracy of 0.95
Iteration 28288: with minibatch training loss = 0.0736 and accuracy of 0.97
Iteration 28416: with minibatch training loss = 0.0653 and accuracy of 0.98
Iteration 28544: with minibatch training loss = 0.085 and accuracy of 0.97
Iteration 28672: with minibatch training loss = 0.0601 and accuracy of 0.98
Iteration 28800: with minibatch training loss = 0.0769 and accuracy of 0.98
Iteration 28928: with minibatch training loss = 0.0376 and accuracy of 0.98
Epoch 7, Train loss: 0.0857 and Train accuracy of 0.976, Test loss: 0.178 and Test accuracy of 0.954
Iteration 29056: with minibatch training loss = 0.0401 and accuracy of 0.99
Iteration 29184: with minibatch training loss = 0.0774 and accuracy of 0.98
Iteration 29312: with minibatch training loss = 0.079 and accuracy of 0.96
Iteration 29440: with minibatch training loss = 0.0481 and accuracy of 0.98
Iteration 29568: with minibatch training loss = 0.0501 and accuracy of 0.98
Iteration 29696: with minibatch training loss = 0.0686 and accuracy of 0.98
Iteration 29824: with minibatch training loss = 0.0478 and accuracy of 0.98
Iteration 29952: with minibatch training loss = 0.0358 and accuracy of 0.99
Iteration 30080: with minibatch training loss = 0.122 and accuracy of 0.98
Iteration 30208: with minibatch training loss = 0.0587 and accuracy of 0.99
Iteration 30336: with minibatch training loss = 0.0498 and accuracy of 0.99
Iteration 30464: with minibatch training loss = 0.0477 and accuracy of 0.98
Iteration 30592: with minibatch training loss = 0.0622 and accuracy of 0.96
Iteration 30720: with minibatch training loss = 0.112 and accuracy of 0.96
Iteration 30848: with minibatch training loss = 0.169 and accuracy of 0.95
Iteration 30976: with minibatch training loss = 0.022 and accuracy of 0.99
Iteration 31104: with minibatch training loss = 0.127 and accuracy of 0.95
Iteration 31232: with minibatch training loss = 0.0784 and accuracy of 0.95
Iteration 31360: with minibatch training loss = 0.0814 and accuracy of 0.98
Iteration 31488: with minibatch training loss = 0.166 and accuracy of 0.96
Iteration 31616: with minibatch training loss = 0.0409 and accuracy of 0.98
Iteration 31744: with minibatch training loss = 0.0652 and accuracy of 0.98
Iteration 31872: with minibatch training loss = 0.0648 and accuracy of 0.98
Iteration 32000: with minibatch training loss = 0.0414 and accuracy of 0.99
Iteration 32128: with minibatch training loss = 0.0681 and accuracy of 0.98
Iteration 32256: with minibatch training loss = 0.103 and accuracy of 0.97
Iteration 32384: with minibatch training loss = 0.0797 and accuracy of 0.98
Iteration 32512: with minibatch training loss = 0.0655 and accuracy of 0.98
Iteration 32640: with minibatch training loss = 0.13 and accuracy of 0.98
Iteration 32768: with minibatch training loss = 0.149 and accuracy of 0.98
Iteration 32896: with minibatch training loss = 0.073 and accuracy of 0.98
Iteration 33024: with minibatch training loss = 0.139 and accuracy of 0.98
Iteration 33152: with minibatch training loss = 0.138 and accuracy of 0.96
Epoch 8, Train loss: 0.0805 and Train accuracy of 0.978, Test loss: 0.169 and Test accuracy of 0.957
Iteration 33280: with minibatch training loss = 0.128 and accuracy of 0.97
Iteration 33408: with minibatch training loss = 0.0623 and accuracy of 0.98
Iteration 33536: with minibatch training loss = 0.121 and accuracy of 0.98
Iteration 33664: with minibatch training loss = 0.0425 and accuracy of 0.98
Iteration 33792: with minibatch training loss = 0.124 and accuracy of 0.98
Iteration 33920: with minibatch training loss = 0.0849 and accuracy of 0.98
Iteration 34048: with minibatch training loss = 0.0288 and accuracy of 1
Iteration 34176: with minibatch training loss = 0.039 and accuracy of 0.99
Iteration 34304: with minibatch training loss = 0.134 and accuracy of 0.97
Iteration 34432: with minibatch training loss = 0.0662 and accuracy of 0.98
Iteration 34560: with minibatch training loss = 0.0968 and accuracy of 0.97
Iteration 34688: with minibatch training loss = 0.0651 and accuracy of 0.98
Iteration 34816: with minibatch training loss = 0.0354 and accuracy of 0.99
Iteration 34944: with minibatch training loss = 0.0652 and accuracy of 0.98
Iteration 35072: with minibatch training loss = 0.15 and accuracy of 0.96
Iteration 35200: with minibatch training loss = 0.0394 and accuracy of 0.99
Iteration 35328: with minibatch training loss = 0.0579 and accuracy of 0.98
Iteration 35456: with minibatch training loss = 0.0277 and accuracy of 0.99
Iteration 35584: with minibatch training loss = 0.132 and accuracy of 0.95
Iteration 35712: with minibatch training loss = 0.123 and accuracy of 0.98
Iteration 35840: with minibatch training loss = 0.105 and accuracy of 0.98
Iteration 35968: with minibatch training loss = 0.0858 and accuracy of 0.97
Iteration 36096: with minibatch training loss = 0.108 and accuracy of 0.97
Iteration 36224: with minibatch training loss = 0.113 and accuracy of 0.98
Iteration 36352: with minibatch training loss = 0.0156 and accuracy of 1
Iteration 36480: with minibatch training loss = 0.0269 and accuracy of 0.99
Iteration 36608: with minibatch training loss = 0.195 and accuracy of 0.95
Iteration 36736: with minibatch training loss = 0.0249 and accuracy of 0.98
Iteration 36864: with minibatch training loss = 0.124 and accuracy of 0.98
Iteration 36992: with minibatch training loss = 0.0554 and accuracy of 0.98
Iteration 37120: with minibatch training loss = 0.0321 and accuracy of 0.98
Iteration 37248: with minibatch training loss = 0.139 and accuracy of 0.96
Epoch 9, Train loss: 0.0761 and Train accuracy of 0.979, Test loss: 0.166 and Test accuracy of 0.957
Iteration 37376: with minibatch training loss = 0.0343 and accuracy of 0.99
Iteration 37504: with minibatch training loss = 0.0521 and accuracy of 0.98
Iteration 37632: with minibatch training loss = 0.0184 and accuracy of 0.99
Iteration 37760: with minibatch training loss = 0.0463 and accuracy of 0.98
Iteration 37888: with minibatch training loss = 0.102 and accuracy of 0.98
Iteration 38016: with minibatch training loss = 0.0566 and accuracy of 0.97
Iteration 38144: with minibatch training loss = 0.121 and accuracy of 0.97
Iteration 38272: with minibatch training loss = 0.185 and accuracy of 0.95
Iteration 38400: with minibatch training loss = 0.0889 and accuracy of 0.98
Iteration 38528: with minibatch training loss = 0.0322 and accuracy of 0.98
Iteration 38656: with minibatch training loss = 0.0564 and accuracy of 0.99
Iteration 38784: with minibatch training loss = 0.0278 and accuracy of 0.99
Iteration 38912: with minibatch training loss = 0.0804 and accuracy of 0.97
Iteration 39040: with minibatch training loss = 0.122 and accuracy of 0.97
Iteration 39168: with minibatch training loss = 0.0325 and accuracy of 0.98
Iteration 39296: with minibatch training loss = 0.0525 and accuracy of 0.99
Iteration 39424: with minibatch training loss = 0.11 and accuracy of 0.98
Iteration 39552: with minibatch training loss = 0.116 and accuracy of 0.98
Iteration 39680: with minibatch training loss = 0.0212 and accuracy of 0.99
Iteration 39808: with minibatch training loss = 0.127 and accuracy of 0.97
Iteration 39936: with minibatch training loss = 0.0975 and accuracy of 0.98
Iteration 40064: with minibatch training loss = 0.126 and accuracy of 0.96
Iteration 40192: with minibatch training loss = 0.108 and accuracy of 0.98
Iteration 40320: with minibatch training loss = 0.0171 and accuracy of 0.99
Iteration 40448: with minibatch training loss = 0.0335 and accuracy of 0.98
Iteration 40576: with minibatch training loss = 0.0864 and accuracy of 0.97
Iteration 40704: with minibatch training loss = 0.0402 and accuracy of 0.98
Iteration 40832: with minibatch training loss = 0.108 and accuracy of 0.98
Iteration 40960: with minibatch training loss = 0.0763 and accuracy of 0.97
Iteration 41088: with minibatch training loss = 0.036 and accuracy of 0.99
Iteration 41216: with minibatch training loss = 0.122 and accuracy of 0.98
Iteration 41344: with minibatch training loss = 0.0412 and accuracy of 0.99
Iteration 41472: with minibatch training loss = 0.0661 and accuracy of 0.98
Epoch 10, Train loss: 0.0722 and Train accuracy of 0.98, Test loss: 0.161 and Test accuracy of 0.96
Iteration 41600: with minibatch training loss = 0.1 and accuracy of 0.95
Iteration 41728: with minibatch training loss = 0.0688 and accuracy of 0.97
Iteration 41856: with minibatch training loss = 0.0528 and accuracy of 0.98
Iteration 41984: with minibatch training loss = 0.124 and accuracy of 0.98
Iteration 42112: with minibatch training loss = 0.0313 and accuracy of 0.98
Iteration 42240: with minibatch training loss = 0.0365 and accuracy of 0.99
Iteration 42368: with minibatch training loss = 0.221 and accuracy of 0.91
Iteration 42496: with minibatch training loss = 0.101 and accuracy of 0.96
Iteration 42624: with minibatch training loss = 0.0879 and accuracy of 0.98
Iteration 42752: with minibatch training loss = 0.0364 and accuracy of 0.98
Iteration 42880: with minibatch training loss = 0.0468 and accuracy of 0.98
Iteration 43008: with minibatch training loss = 0.111 and accuracy of 0.96
Iteration 43136: with minibatch training loss = 0.0366 and accuracy of 0.98
Iteration 43264: with minibatch training loss = 0.134 and accuracy of 0.95
Iteration 43392: with minibatch training loss = 0.0942 and accuracy of 0.98
Iteration 43520: with minibatch training loss = 0.0328 and accuracy of 0.99
Iteration 43648: with minibatch training loss = 0.073 and accuracy of 0.98
Iteration 43776: with minibatch training loss = 0.0891 and accuracy of 0.98
Iteration 43904: with minibatch training loss = 0.0526 and accuracy of 0.97
Iteration 44032: with minibatch training loss = 0.0241 and accuracy of 0.99
Iteration 44160: with minibatch training loss = 0.0621 and accuracy of 0.97
Iteration 44288: with minibatch training loss = 0.0284 and accuracy of 0.99
Iteration 44416: with minibatch training loss = 0.0794 and accuracy of 0.98
Iteration 44544: with minibatch training loss = 0.0138 and accuracy of 0.99
Iteration 44672: with minibatch training loss = 0.107 and accuracy of 0.97
Iteration 44800: with minibatch training loss = 0.0457 and accuracy of 0.98
Iteration 44928: with minibatch training loss = 0.0572 and accuracy of 0.98
Iteration 45056: with minibatch training loss = 0.163 and accuracy of 0.97
Iteration 45184: with minibatch training loss = 0.0184 and accuracy of 0.99
Iteration 45312: with minibatch training loss = 0.0806 and accuracy of 0.98
Iteration 45440: with minibatch training loss = 0.165 and accuracy of 0.95
Iteration 45568: with minibatch training loss = 0.032 and accuracy of 0.99
Epoch 11, Train loss: 0.0685 and Train accuracy of 0.981, Test loss: 0.158 and Test accuracy of 0.961
Iteration 45696: with minibatch training loss = 0.0247 and accuracy of 0.99
Iteration 45824: with minibatch training loss = 0.0213 and accuracy of 0.99
Iteration 45952: with minibatch training loss = 0.034 and accuracy of 0.99
Iteration 46080: with minibatch training loss = 0.0288 and accuracy of 0.99
Iteration 46208: with minibatch training loss = 0.0927 and accuracy of 0.95
Iteration 46336: with minibatch training loss = 0.0699 and accuracy of 0.98
Iteration 46464: with minibatch training loss = 0.0459 and accuracy of 0.98
Iteration 46592: with minibatch training loss = 0.0619 and accuracy of 0.98
Iteration 46720: with minibatch training loss = 0.128 and accuracy of 0.95
Iteration 46848: with minibatch training loss = 0.0289 and accuracy of 0.99
Iteration 46976: with minibatch training loss = 0.079 and accuracy of 0.97
Iteration 47104: with minibatch training loss = 0.0483 and accuracy of 0.99
Iteration 47232: with minibatch training loss = 0.0329 and accuracy of 0.99
Iteration 47360: with minibatch training loss = 0.0719 and accuracy of 0.98
Iteration 47488: with minibatch training loss = 0.0337 and accuracy of 0.99
Iteration 47616: with minibatch training loss = 0.0826 and accuracy of 0.98
Iteration 47744: with minibatch training loss = 0.119 and accuracy of 0.97
Iteration 47872: with minibatch training loss = 0.0608 and accuracy of 0.98
Iteration 48000: with minibatch training loss = 0.091 and accuracy of 0.98
Iteration 48128: with minibatch training loss = 0.0505 and accuracy of 0.98
Iteration 48256: with minibatch training loss = 0.0466 and accuracy of 0.99
Iteration 48384: with minibatch training loss = 0.127 and accuracy of 0.97
Iteration 48512: with minibatch training loss = 0.061 and accuracy of 0.98
Iteration 48640: with minibatch training loss = 0.0538 and accuracy of 0.98
Iteration 48768: with minibatch training loss = 0.0652 and accuracy of 0.98
Iteration 48896: with minibatch training loss = 0.0822 and accuracy of 0.97
Iteration 49024: with minibatch training loss = 0.0552 and accuracy of 0.98
Iteration 49152: with minibatch training loss = 0.126 and accuracy of 0.98
Iteration 49280: with minibatch training loss = 0.0413 and accuracy of 0.99
Iteration 49408: with minibatch training loss = 0.0265 and accuracy of 0.99
Iteration 49536: with minibatch training loss = 0.064 and accuracy of 0.98
Iteration 49664: with minibatch training loss = 0.0255 and accuracy of 0.98
Iteration 49792: with minibatch training loss = 0.0784 and accuracy of 0.98
Epoch 12, Train loss: 0.0658 and Train accuracy of 0.982, Test loss: 0.153 and Test accuracy of 0.962
Iteration 49920: with minibatch training loss = 0.0502 and accuracy of 0.98
Iteration 50048: with minibatch training loss = 0.0242 and accuracy of 0.99
Iteration 50176: with minibatch training loss = 0.0264 and accuracy of 0.99
Iteration 50304: with minibatch training loss = 0.0903 and accuracy of 0.98
Iteration 50432: with minibatch training loss = 0.0118 and accuracy of 1
Iteration 50560: with minibatch training loss = 0.105 and accuracy of 0.97
Iteration 50688: with minibatch training loss = 0.0888 and accuracy of 0.98
Iteration 50816: with minibatch training loss = 0.0259 and accuracy of 0.99
Iteration 50944: with minibatch training loss = 0.0583 and accuracy of 0.97
Iteration 51072: with minibatch training loss = 0.0454 and accuracy of 0.98
Iteration 51200: with minibatch training loss = 0.0259 and accuracy of 0.98
Iteration 51328: with minibatch training loss = 0.0636 and accuracy of 0.98
Iteration 51456: with minibatch training loss = 0.0462 and accuracy of 0.98
Iteration 51584: with minibatch training loss = 0.105 and accuracy of 0.97
Iteration 51712: with minibatch training loss = 0.0141 and accuracy of 0.99
Iteration 51840: with minibatch training loss = 0.0959 and accuracy of 0.97
Iteration 51968: with minibatch training loss = 0.00809 and accuracy of 1
Iteration 52096: with minibatch training loss = 0.0274 and accuracy of 0.99
Iteration 52224: with minibatch training loss = 0.0528 and accuracy of 0.98
Iteration 52352: with minibatch training loss = 0.0625 and accuracy of 0.98
Iteration 52480: with minibatch training loss = 0.0441 and accuracy of 0.98
Iteration 52608: with minibatch training loss = 0.0635 and accuracy of 0.98
Iteration 52736: with minibatch training loss = 0.0643 and accuracy of 0.98
Iteration 52864: with minibatch training loss = 0.0282 and accuracy of 0.98
Iteration 52992: with minibatch training loss = 0.0262 and accuracy of 0.99
Iteration 53120: with minibatch training loss = 0.0446 and accuracy of 0.99
Iteration 53248: with minibatch training loss = 0.0544 and accuracy of 0.98
Iteration 53376: with minibatch training loss = 0.0287 and accuracy of 0.99
Iteration 53504: with minibatch training loss = 0.0455 and accuracy of 0.98
Iteration 53632: with minibatch training loss = 0.184 and accuracy of 0.98
Iteration 53760: with minibatch training loss = 0.0128 and accuracy of 1
Iteration 53888: with minibatch training loss = 0.05 and accuracy of 0.98
Epoch 13, Train loss: 0.0631 and Train accuracy of 0.983, Test loss: 0.153 and Test accuracy of 0.961
Iteration 54016: with minibatch training loss = 0.028 and accuracy of 0.99
Iteration 54144: with minibatch training loss = 0.0145 and accuracy of 1
Iteration 54272: with minibatch training loss = 0.0598 and accuracy of 0.99
Iteration 54400: with minibatch training loss = 0.0238 and accuracy of 0.99
Iteration 54528: with minibatch training loss = 0.0479 and accuracy of 0.99
Iteration 54656: with minibatch training loss = 0.0098 and accuracy of 1
Iteration 54784: with minibatch training loss = 0.0775 and accuracy of 0.98
Iteration 54912: with minibatch training loss = 0.0194 and accuracy of 0.99
Iteration 55040: with minibatch training loss = 0.0757 and accuracy of 0.96
Iteration 55168: with minibatch training loss = 0.0308 and accuracy of 0.99
Iteration 55296: with minibatch training loss = 0.00895 and accuracy of 1
Iteration 55424: with minibatch training loss = 0.0321 and accuracy of 0.99
Iteration 55552: with minibatch training loss = 0.0272 and accuracy of 0.99
Iteration 55680: with minibatch training loss = 0.0132 and accuracy of 1
Iteration 55808: with minibatch training loss = 0.00814 and accuracy of 1
Iteration 55936: with minibatch training loss = 0.0157 and accuracy of 0.99
Iteration 56064: with minibatch training loss = 0.0202 and accuracy of 0.99
Iteration 56192: with minibatch training loss = 0.0148 and accuracy of 1
Iteration 56320: with minibatch training loss = 0.0931 and accuracy of 0.98
Iteration 56448: with minibatch training loss = 0.166 and accuracy of 0.96
Iteration 56576: with minibatch training loss = 0.123 and accuracy of 0.98
Iteration 56704: with minibatch training loss = 0.0735 and accuracy of 0.98
Iteration 56832: with minibatch training loss = 0.011 and accuracy of 1
Iteration 56960: with minibatch training loss = 0.094 and accuracy of 0.97
Iteration 57088: with minibatch training loss = 0.0431 and accuracy of 0.98
Iteration 57216: with minibatch training loss = 0.213 and accuracy of 0.96
Iteration 57344: with minibatch training loss = 0.0607 and accuracy of 0.98
Iteration 57472: with minibatch training loss = 0.0304 and accuracy of 0.98
Iteration 57600: with minibatch training loss = 0.113 and accuracy of 0.97
Iteration 57728: with minibatch training loss = 0.107 and accuracy of 0.95
Iteration 57856: with minibatch training loss = 0.0249 and accuracy of 0.99
Iteration 57984: with minibatch training loss = 0.054 and accuracy of 0.98
Epoch 14, Train loss: 0.0603 and Train accuracy of 0.984, Test loss: 0.151 and Test accuracy of 0.963
Iteration 58112: with minibatch training loss = 0.0237 and accuracy of 0.99
Iteration 58240: with minibatch training loss = 0.0331 and accuracy of 0.98
Iteration 58368: with minibatch training loss = 0.0321 and accuracy of 1
Iteration 58496: with minibatch training loss = 0.132 and accuracy of 0.98
Iteration 58624: with minibatch training loss = 0.0625 and accuracy of 0.98
Iteration 58752: with minibatch training loss = 0.153 and accuracy of 0.95
Iteration 58880: with minibatch training loss = 0.201 and accuracy of 0.97
Iteration 59008: with minibatch training loss = 0.0263 and accuracy of 0.99
Iteration 59136: with minibatch training loss = 0.0353 and accuracy of 0.99
Iteration 59264: with minibatch training loss = 0.164 and accuracy of 0.96
Iteration 59392: with minibatch training loss = 0.0205 and accuracy of 0.99
Iteration 59520: with minibatch training loss = 0.0134 and accuracy of 0.99
Iteration 59648: with minibatch training loss = 0.0337 and accuracy of 0.99
Iteration 59776: with minibatch training loss = 0.00645 and accuracy of 1
Iteration 59904: with minibatch training loss = 0.0343 and accuracy of 0.98
Iteration 60032: with minibatch training loss = 0.0216 and accuracy of 0.99
Iteration 60160: with minibatch training loss = 0.0214 and accuracy of 1
Iteration 60288: with minibatch training loss = 0.118 and accuracy of 0.97
Iteration 60416: with minibatch training loss = 0.101 and accuracy of 0.95
Iteration 60544: with minibatch training loss = 0.0242 and accuracy of 0.99
Iteration 60672: with minibatch training loss = 0.057 and accuracy of 0.99
Iteration 60800: with minibatch training loss = 0.0155 and accuracy of 0.99
Iteration 60928: with minibatch training loss = 0.0796 and accuracy of 0.98
Iteration 61056: with minibatch training loss = 0.0548 and accuracy of 0.98
Iteration 61184: with minibatch training loss = 0.0477 and accuracy of 0.99
Iteration 61312: with minibatch training loss = 0.13 and accuracy of 0.95
Iteration 61440: with minibatch training loss = 0.116 and accuracy of 0.98
Iteration 61568: with minibatch training loss = 0.0312 and accuracy of 0.99
Iteration 61696: with minibatch training loss = 0.0231 and accuracy of 1
Iteration 61824: with minibatch training loss = 0.0709 and accuracy of 0.98
Iteration 61952: with minibatch training loss = 0.0156 and accuracy of 1
Iteration 62080: with minibatch training loss = 0.158 and accuracy of 0.95
Iteration 62208: with minibatch training loss = 0.037 and accuracy of 0.99
Epoch 15, Train loss: 0.0585 and Train accuracy of 0.984, Test loss: 0.153 and Test accuracy of 0.963
Iteration 62336: with minibatch training loss = 0.0162 and accuracy of 1
Iteration 62464: with minibatch training loss = 0.0352 and accuracy of 0.98
Iteration 62592: with minibatch training loss = 0.101 and accuracy of 0.99
Iteration 62720: with minibatch training loss = 0.0791 and accuracy of 0.97
Iteration 62848: with minibatch training loss = 0.13 and accuracy of 0.97
Iteration 62976: with minibatch training loss = 0.109 and accuracy of 0.98
Iteration 63104: with minibatch training loss = 0.019 and accuracy of 0.99
Iteration 63232: with minibatch training loss = 0.00842 and accuracy of 1
Iteration 63360: with minibatch training loss = 0.168 and accuracy of 0.96
Iteration 63488: with minibatch training loss = 0.0186 and accuracy of 1
Iteration 63616: with minibatch training loss = 0.127 and accuracy of 0.98
Iteration 63744: with minibatch training loss = 0.0742 and accuracy of 0.98
Iteration 63872: with minibatch training loss = 0.0467 and accuracy of 0.98
Iteration 64000: with minibatch training loss = 0.0519 and accuracy of 0.99
Iteration 64128: with minibatch training loss = 0.0743 and accuracy of 0.98
Iteration 64256: with minibatch training loss = 0.0703 and accuracy of 0.98
Iteration 64384: with minibatch training loss = 0.0623 and accuracy of 0.98
Iteration 64512: with minibatch training loss = 0.0149 and accuracy of 1
Iteration 64640: with minibatch training loss = 0.067 and accuracy of 0.98
Iteration 64768: with minibatch training loss = 0.0521 and accuracy of 0.98
Iteration 64896: with minibatch training loss = 0.156 and accuracy of 0.94
Iteration 65024: with minibatch training loss = 0.0529 and accuracy of 0.98
Iteration 65152: with minibatch training loss = 0.0454 and accuracy of 0.98
Iteration 65280: with minibatch training loss = 0.0533 and accuracy of 0.99
Iteration 65408: with minibatch training loss = 0.096 and accuracy of 0.96
Iteration 65536: with minibatch training loss = 0.0824 and accuracy of 0.97
Iteration 65664: with minibatch training loss = 0.141 and accuracy of 0.96
Iteration 65792: with minibatch training loss = 0.00713 and accuracy of 1
Iteration 65920: with minibatch training loss = 0.035 and accuracy of 0.99
Iteration 66048: with minibatch training loss = 0.0772 and accuracy of 0.98
Iteration 66176: with minibatch training loss = 0.0274 and accuracy of 0.99
Iteration 66304: with minibatch training loss = 0.0687 and accuracy of 0.98
Epoch 16, Train loss: 0.0565 and Train accuracy of 0.985, Test loss: 0.149 and Test accuracy of 0.964
Iteration 66432: with minibatch training loss = 0.0523 and accuracy of 0.98
Iteration 66560: with minibatch training loss = 0.0161 and accuracy of 0.99
Iteration 66688: with minibatch training loss = 0.0242 and accuracy of 0.98
Iteration 66816: with minibatch training loss = 0.183 and accuracy of 0.98
Iteration 66944: with minibatch training loss = 0.078 and accuracy of 0.97
Iteration 67072: with minibatch training loss = 0.0134 and accuracy of 1
Iteration 67200: with minibatch training loss = 0.135 and accuracy of 0.98
Iteration 67328: with minibatch training loss = 0.0452 and accuracy of 0.99
Iteration 67456: with minibatch training loss = 0.084 and accuracy of 0.97
Iteration 67584: with minibatch training loss = 0.0744 and accuracy of 0.98
Iteration 67712: with minibatch training loss = 0.0224 and accuracy of 1
Iteration 67840: with minibatch training loss = 0.117 and accuracy of 0.98
Iteration 67968: with minibatch training loss = 0.0163 and accuracy of 1
Iteration 68096: with minibatch training loss = 0.0703 and accuracy of 0.97
Iteration 68224: with minibatch training loss = 0.079 and accuracy of 0.97
Iteration 68352: with minibatch training loss = 0.0253 and accuracy of 0.98
Iteration 68480: with minibatch training loss = 0.0331 and accuracy of 0.98
Iteration 68608: with minibatch training loss = 0.124 and accuracy of 0.97
Iteration 68736: with minibatch training loss = 0.085 and accuracy of 0.98
Iteration 68864: with minibatch training loss = 0.0352 and accuracy of 0.98
Iteration 68992: with minibatch training loss = 0.101 and accuracy of 0.98
Iteration 69120: with minibatch training loss = 0.066 and accuracy of 0.98
Iteration 69248: with minibatch training loss = 0.016 and accuracy of 1
Iteration 69376: with minibatch training loss = 0.0741 and accuracy of 0.97
Iteration 69504: with minibatch training loss = 0.0304 and accuracy of 0.98
Iteration 69632: with minibatch training loss = 0.0617 and accuracy of 0.98
Iteration 69760: with minibatch training loss = 0.00905 and accuracy of 1
Iteration 69888: with minibatch training loss = 0.00911 and accuracy of 1
Iteration 70016: with minibatch training loss = 0.144 and accuracy of 0.96
Iteration 70144: with minibatch training loss = 0.0128 and accuracy of 1
Iteration 70272: with minibatch training loss = 0.039 and accuracy of 0.98
Iteration 70400: with minibatch training loss = 0.111 and accuracy of 0.98
Iteration 70528: with minibatch training loss = 0.0209 and accuracy of 0.99
Epoch 17, Train loss: 0.0546 and Train accuracy of 0.985, Test loss: 0.164 and Test accuracy of 0.959
Iteration 70656: with minibatch training loss = 0.012 and accuracy of 0.99
Iteration 70784: with minibatch training loss = 0.0603 and accuracy of 0.97
Iteration 70912: with minibatch training loss = 0.0136 and accuracy of 0.99
Iteration 71040: with minibatch training loss = 0.0233 and accuracy of 0.99
Iteration 71168: with minibatch training loss = 0.078 and accuracy of 0.96
Iteration 71296: with minibatch training loss = 0.01 and accuracy of 1
Iteration 71424: with minibatch training loss = 0.0669 and accuracy of 0.98
Iteration 71552: with minibatch training loss = 0.0491 and accuracy of 0.98
Iteration 71680: with minibatch training loss = 0.166 and accuracy of 0.97
Iteration 71808: with minibatch training loss = 0.0204 and accuracy of 1
Iteration 71936: with minibatch training loss = 0.081 and accuracy of 0.98
Iteration 72064: with minibatch training loss = 0.0388 and accuracy of 0.99
Iteration 72192: with minibatch training loss = 0.0306 and accuracy of 0.99
Iteration 72320: with minibatch training loss = 0.0154 and accuracy of 1
Iteration 72448: with minibatch training loss = 0.012 and accuracy of 1
Iteration 72576: with minibatch training loss = 0.11 and accuracy of 0.97
Iteration 72704: with minibatch training loss = 0.0181 and accuracy of 0.99
Iteration 72832: with minibatch training loss = 0.0569 and accuracy of 0.97
Iteration 72960: with minibatch training loss = 0.0497 and accuracy of 0.98
Iteration 73088: with minibatch training loss = 0.0331 and accuracy of 0.98
Iteration 73216: with minibatch training loss = 0.0628 and accuracy of 0.98
Iteration 73344: with minibatch training loss = 0.0764 and accuracy of 0.98
Iteration 73472: with minibatch training loss = 0.0626 and accuracy of 0.98
Iteration 73600: with minibatch training loss = 0.132 and accuracy of 0.98
Iteration 73728: with minibatch training loss = 0.0287 and accuracy of 0.98
Iteration 73856: with minibatch training loss = 0.0411 and accuracy of 0.98
Iteration 73984: with minibatch training loss = 0.0855 and accuracy of 0.98
Iteration 74112: with minibatch training loss = 0.0194 and accuracy of 1
Iteration 74240: with minibatch training loss = 0.0394 and accuracy of 0.99
Iteration 74368: with minibatch training loss = 0.077 and accuracy of 0.98
Iteration 74496: with minibatch training loss = 0.0277 and accuracy of 1
Iteration 74624: with minibatch training loss = 0.0632 and accuracy of 0.98
Epoch 18, Train loss: 0.0527 and Train accuracy of 0.986, Test loss: 0.149 and Test accuracy of 0.964
Iteration 74752: with minibatch training loss = 0.0851 and accuracy of 0.96
Iteration 74880: with minibatch training loss = 0.0446 and accuracy of 0.98
Iteration 75008: with minibatch training loss = 0.0232 and accuracy of 1
Iteration 75136: with minibatch training loss = 0.0559 and accuracy of 0.98
Iteration 75264: with minibatch training loss = 0.0896 and accuracy of 0.99
Iteration 75392: with minibatch training loss = 0.0532 and accuracy of 0.98
Iteration 75520: with minibatch training loss = 0.0879 and accuracy of 0.98
Iteration 75648: with minibatch training loss = 0.0981 and accuracy of 0.98
Iteration 75776: with minibatch training loss = 0.0252 and accuracy of 0.98
Iteration 75904: with minibatch training loss = 0.0491 and accuracy of 0.98
Iteration 76032: with minibatch training loss = 0.0366 and accuracy of 0.99
Iteration 76160: with minibatch training loss = 0.0898 and accuracy of 0.97
Iteration 76288: with minibatch training loss = 0.042 and accuracy of 0.99
Iteration 76416: with minibatch training loss = 0.0214 and accuracy of 0.99
Iteration 76544: with minibatch training loss = 0.00895 and accuracy of 1
Iteration 76672: with minibatch training loss = 0.0878 and accuracy of 0.98
Iteration 76800: with minibatch training loss = 0.0262 and accuracy of 0.98
Iteration 76928: with minibatch training loss = 0.0302 and accuracy of 0.98
Iteration 77056: with minibatch training loss = 0.0807 and accuracy of 0.98
Iteration 77184: with minibatch training loss = 0.118 and accuracy of 0.97
Iteration 77312: with minibatch training loss = 0.0132 and accuracy of 1
Iteration 77440: with minibatch training loss = 0.0631 and accuracy of 0.99
Iteration 77568: with minibatch training loss = 0.0384 and accuracy of 0.98
Iteration 77696: with minibatch training loss = 0.0402 and accuracy of 0.98
Iteration 77824: with minibatch training loss = 0.0733 and accuracy of 0.99
Iteration 77952: with minibatch training loss = 0.0139 and accuracy of 1
Iteration 78080: with minibatch training loss = 0.0626 and accuracy of 0.96
Iteration 78208: with minibatch training loss = 0.0343 and accuracy of 0.98
Iteration 78336: with minibatch training loss = 0.0519 and accuracy of 0.98
Iteration 78464: with minibatch training loss = 0.0192 and accuracy of 0.99
Iteration 78592: with minibatch training loss = 0.0711 and accuracy of 0.99
Iteration 78720: with minibatch training loss = 0.0229 and accuracy of 0.99
Iteration 78848: with minibatch training loss = 0.0118 and accuracy of 1
Epoch 19, Train loss: 0.0513 and Train accuracy of 0.986, Test loss: 0.153 and Test accuracy of 0.963
Iteration 78976: with minibatch training loss = 0.137 and accuracy of 0.98
Iteration 79104: with minibatch training loss = 0.0115 and accuracy of 1
Iteration 79232: with minibatch training loss = 0.298 and accuracy of 0.95
Iteration 79360: with minibatch training loss = 0.067 and accuracy of 0.99
Iteration 79488: with minibatch training loss = 0.0395 and accuracy of 0.98
Iteration 79616: with minibatch training loss = 0.0829 and accuracy of 0.98
Iteration 79744: with minibatch training loss = 0.0302 and accuracy of 0.99
Iteration 79872: with minibatch training loss = 0.0795 and accuracy of 0.98
Iteration 80000: with minibatch training loss = 0.0424 and accuracy of 0.98
Iteration 80128: with minibatch training loss = 0.0304 and accuracy of 0.99
Iteration 80256: with minibatch training loss = 0.0443 and accuracy of 0.98
Iteration 80384: with minibatch training loss = 0.0301 and accuracy of 0.99
Iteration 80512: with minibatch training loss = 0.0243 and accuracy of 0.99
Iteration 80640: with minibatch training loss = 0.00625 and accuracy of 1
Iteration 80768: with minibatch training loss = 0.0211 and accuracy of 0.98
Iteration 80896: with minibatch training loss = 0.142 and accuracy of 0.98
Iteration 81024: with minibatch training loss = 0.0151 and accuracy of 1
Iteration 81152: with minibatch training loss = 0.0365 and accuracy of 0.98
Iteration 81280: with minibatch training loss = 0.0598 and accuracy of 0.99
Iteration 81408: with minibatch training loss = 0.00908 and accuracy of 1
Iteration 81536: with minibatch training loss = 0.0478 and accuracy of 0.98
Iteration 81664: with minibatch training loss = 0.012 and accuracy of 1
Iteration 81792: with minibatch training loss = 0.0838 and accuracy of 0.98
Iteration 81920: with minibatch training loss = 0.0393 and accuracy of 0.98
Iteration 82048: with minibatch training loss = 0.0945 and accuracy of 0.97
Iteration 82176: with minibatch training loss = 0.111 and accuracy of 0.99
Iteration 82304: with minibatch training loss = 0.0642 and accuracy of 0.98
Iteration 82432: with minibatch training loss = 0.0362 and accuracy of 0.98
Iteration 82560: with minibatch training loss = 0.0525 and accuracy of 0.98
Iteration 82688: with minibatch training loss = 0.015 and accuracy of 0.99
Iteration 82816: with minibatch training loss = 0.00473 and accuracy of 1
Iteration 82944: with minibatch training loss = 0.0308 and accuracy of 0.98
Epoch 20, Train loss: 0.05 and Train accuracy of 0.987, Test loss: 0.15 and Test accuracy of 0.965
Iteration 83072: with minibatch training loss = 0.00451 and accuracy of 1
Iteration 83200: with minibatch training loss = 0.0153 and accuracy of 0.99
Iteration 83328: with minibatch training loss = 0.117 and accuracy of 0.97
Iteration 83456: with minibatch training loss = 0.0162 and accuracy of 0.99
Iteration 83584: with minibatch training loss = 0.0468 and accuracy of 0.98
Iteration 83712: with minibatch training loss = 0.0128 and accuracy of 0.99
Iteration 83840: with minibatch training loss = 0.0192 and accuracy of 1
Iteration 83968: with minibatch training loss = 0.068 and accuracy of 0.99
Iteration 84096: with minibatch training loss = 0.0256 and accuracy of 0.99
Iteration 84224: with minibatch training loss = 0.00405 and accuracy of 1
Iteration 84352: with minibatch training loss = 0.0421 and accuracy of 0.98
Iteration 84480: with minibatch training loss = 0.0309 and accuracy of 0.99
Iteration 84608: with minibatch training loss = 0.0268 and accuracy of 0.99
Iteration 84736: with minibatch training loss = 0.0532 and accuracy of 0.99
Iteration 84864: with minibatch training loss = 0.0103 and accuracy of 1
Iteration 84992: with minibatch training loss = 0.0769 and accuracy of 0.99
Iteration 85120: with minibatch training loss = 0.0823 and accuracy of 0.98
Iteration 85248: with minibatch training loss = 0.0144 and accuracy of 0.99
Iteration 85376: with minibatch training loss = 0.119 and accuracy of 0.98
Iteration 85504: with minibatch training loss = 0.0445 and accuracy of 0.99
Iteration 85632: with minibatch training loss = 0.0856 and accuracy of 0.97
Iteration 85760: with minibatch training loss = 0.0367 and accuracy of 0.99
Iteration 85888: with minibatch training loss = 0.0643 and accuracy of 0.98
Iteration 86016: with minibatch training loss = 0.0219 and accuracy of 0.98
Iteration 86144: with minibatch training loss = 0.05 and accuracy of 0.99
Iteration 86272: with minibatch training loss = 0.0354 and accuracy of 0.99
Iteration 86400: with minibatch training loss = 0.00315 and accuracy of 1
Iteration 86528: with minibatch training loss = 0.0455 and accuracy of 0.99
Iteration 86656: with minibatch training loss = 0.0385 and accuracy of 0.98
Iteration 86784: with minibatch training loss = 0.0567 and accuracy of 0.98
Iteration 86912: with minibatch training loss = 0.0425 and accuracy of 0.97
Iteration 87040: with minibatch training loss = 0.0437 and accuracy of 0.98
Epoch 21, Train loss: 0.049 and Train accuracy of 0.987, Test loss: 0.156 and Test accuracy of 0.962
Iteration 87168: with minibatch training loss = 0.1 and accuracy of 0.98
Iteration 87296: with minibatch training loss = 0.0336 and accuracy of 0.99
Iteration 87424: with minibatch training loss = 0.00545 and accuracy of 1
Iteration 87552: with minibatch training loss = 0.0208 and accuracy of 0.99
Iteration 87680: with minibatch training loss = 0.0744 and accuracy of 0.98
Iteration 87808: with minibatch training loss = 0.0523 and accuracy of 0.97
Iteration 87936: with minibatch training loss = 0.0149 and accuracy of 0.99
Iteration 88064: with minibatch training loss = 0.0684 and accuracy of 0.98
Iteration 88192: with minibatch training loss = 0.0624 and accuracy of 0.98
Iteration 88320: with minibatch training loss = 0.0121 and accuracy of 1
Iteration 88448: with minibatch training loss = 0.0125 and accuracy of 1
Iteration 88576: with minibatch training loss = 0.0373 and accuracy of 0.98
Iteration 88704: with minibatch training loss = 0.082 and accuracy of 0.99
Iteration 88832: with minibatch training loss = 0.0885 and accuracy of 0.97
Iteration 88960: with minibatch training loss = 0.146 and accuracy of 0.98
Iteration 89088: with minibatch training loss = 0.0399 and accuracy of 0.99
Iteration 89216: with minibatch training loss = 0.0743 and accuracy of 0.99
Iteration 89344: with minibatch training loss = 0.044 and accuracy of 0.98
Iteration 89472: with minibatch training loss = 0.0198 and accuracy of 0.99
Iteration 89600: with minibatch training loss = 0.0311 and accuracy of 0.99
Iteration 89728: with minibatch training loss = 0.0356 and accuracy of 0.99
Iteration 89856: with minibatch training loss = 0.0777 and accuracy of 0.98
Iteration 89984: with minibatch training loss = 0.0911 and accuracy of 0.98
Iteration 90112: with minibatch training loss = 0.209 and accuracy of 0.97
Iteration 90240: with minibatch training loss = 0.00538 and accuracy of 1
Iteration 90368: with minibatch training loss = 0.0404 and accuracy of 0.98
Iteration 90496: with minibatch training loss = 0.0184 and accuracy of 1
Iteration 90624: with minibatch training loss = 0.0634 and accuracy of 0.98
Iteration 90752: with minibatch training loss = 0.0239 and accuracy of 0.99
Iteration 90880: with minibatch training loss = 0.0637 and accuracy of 0.98
Iteration 91008: with minibatch training loss = 0.116 and accuracy of 0.98
Iteration 91136: with minibatch training loss = 0.0877 and accuracy of 0.98
Iteration 91264: with minibatch training loss = 0.0897 and accuracy of 0.98
Epoch 22, Train loss: 0.048 and Train accuracy of 0.987, Test loss: 0.149 and Test accuracy of 0.965
Iteration 91392: with minibatch training loss = 0.00635 and accuracy of 1
Iteration 91520: with minibatch training loss = 0.0148 and accuracy of 0.99
Iteration 91648: with minibatch training loss = 0.00747 and accuracy of 1
Iteration 91776: with minibatch training loss = 0.0562 and accuracy of 0.98
Iteration 91904: with minibatch training loss = 0.0264 and accuracy of 1
Iteration 92032: with minibatch training loss = 0.0686 and accuracy of 0.98
Iteration 92160: with minibatch training loss = 0.0657 and accuracy of 0.99
Iteration 92288: with minibatch training loss = 0.00967 and accuracy of 1
Iteration 92416: with minibatch training loss = 0.028 and accuracy of 0.98
Iteration 92544: with minibatch training loss = 0.00948 and accuracy of 1
Iteration 92672: with minibatch training loss = 0.0105 and accuracy of 1
Iteration 92800: with minibatch training loss = 0.0688 and accuracy of 0.98
Iteration 92928: with minibatch training loss = 0.0653 and accuracy of 0.98
Iteration 93056: with minibatch training loss = 0.071 and accuracy of 0.98
Iteration 93184: with minibatch training loss = 0.0212 and accuracy of 0.99
Iteration 93312: with minibatch training loss = 0.0414 and accuracy of 0.99
Iteration 93440: with minibatch training loss = 0.076 and accuracy of 0.98
Iteration 93568: with minibatch training loss = 0.178 and accuracy of 0.97
Iteration 93696: with minibatch training loss = 0.0458 and accuracy of 0.99
Iteration 93824: with minibatch training loss = 0.00515 and accuracy of 1
Iteration 93952: with minibatch training loss = 0.0172 and accuracy of 1
Iteration 94080: with minibatch training loss = 0.0366 and accuracy of 0.98
Iteration 94208: with minibatch training loss = 0.00574 and accuracy of 1
Iteration 94336: with minibatch training loss = 0.00931 and accuracy of 1
Iteration 94464: with minibatch training loss = 0.012 and accuracy of 1
Iteration 94592: with minibatch training loss = 0.0602 and accuracy of 0.98
Iteration 94720: with minibatch training loss = 0.0946 and accuracy of 0.98
Iteration 94848: with minibatch training loss = 0.0354 and accuracy of 0.99
Iteration 94976: with minibatch training loss = 0.0216 and accuracy of 0.99
Iteration 95104: with minibatch training loss = 0.026 and accuracy of 0.99
Iteration 95232: with minibatch training loss = 0.0167 and accuracy of 0.99
Iteration 95360: with minibatch training loss = 0.0555 and accuracy of 0.98
Epoch 23, Train loss: 0.0458 and Train accuracy of 0.988, Test loss: 0.154 and Test accuracy of 0.963
Iteration 95488: with minibatch training loss = 0.0543 and accuracy of 0.98
Iteration 95616: with minibatch training loss = 0.0713 and accuracy of 0.98
Iteration 95744: with minibatch training loss = 0.0638 and accuracy of 0.98
Iteration 95872: with minibatch training loss = 0.0269 and accuracy of 0.98
Iteration 96000: with minibatch training loss = 0.131 and accuracy of 0.98
Iteration 96128: with minibatch training loss = 0.0686 and accuracy of 0.98
Iteration 96256: with minibatch training loss = 0.0528 and accuracy of 0.99
Iteration 96384: with minibatch training loss = 0.0339 and accuracy of 0.99
Iteration 96512: with minibatch training loss = 0.0108 and accuracy of 1
Iteration 96640: with minibatch training loss = 0.00903 and accuracy of 1
Iteration 96768: with minibatch training loss = 0.0816 and accuracy of 0.97
Iteration 96896: with minibatch training loss = 0.0256 and accuracy of 0.99
Iteration 97024: with minibatch training loss = 0.0321 and accuracy of 0.99
Iteration 97152: with minibatch training loss = 0.0161 and accuracy of 0.99
Iteration 97280: with minibatch training loss = 0.051 and accuracy of 0.98
Iteration 97408: with minibatch training loss = 0.0508 and accuracy of 0.98
Iteration 97536: with minibatch training loss = 0.00324 and accuracy of 1
Iteration 97664: with minibatch training loss = 0.0543 and accuracy of 0.98
Iteration 97792: with minibatch training loss = 0.0461 and accuracy of 0.98
Iteration 97920: with minibatch training loss = 0.0624 and accuracy of 0.97
Iteration 98048: with minibatch training loss = 0.00443 and accuracy of 1
Iteration 98176: with minibatch training loss = 0.081 and accuracy of 0.97
Iteration 98304: with minibatch training loss = 0.0739 and accuracy of 0.97
Iteration 98432: with minibatch training loss = 0.0347 and accuracy of 0.98
Iteration 98560: with minibatch training loss = 0.0329 and accuracy of 0.98
Iteration 98688: with minibatch training loss = 0.0264 and accuracy of 0.99
Iteration 98816: with minibatch training loss = 0.106 and accuracy of 0.98
Iteration 98944: with minibatch training loss = 0.0521 and accuracy of 0.99
Iteration 99072: with minibatch training loss = 0.012 and accuracy of 1
Iteration 99200: with minibatch training loss = 0.0227 and accuracy of 0.98
Iteration 99328: with minibatch training loss = 0.0327 and accuracy of 0.98
Iteration 99456: with minibatch training loss = 0.0486 and accuracy of 0.98
Iteration 99584: with minibatch training loss = 0.123 and accuracy of 0.97
Epoch 24, Train loss: 0.0454 and Train accuracy of 0.988, Test loss: 0.154 and Test accuracy of 0.964
Iteration 99712: with minibatch training loss = 0.037 and accuracy of 0.98
Iteration 99840: with minibatch training loss = 0.0407 and accuracy of 0.98
Iteration 99968: with minibatch training loss = 0.0236 and accuracy of 0.99
Iteration 100096: with minibatch training loss = 0.0102 and accuracy of 0.99
Iteration 100224: with minibatch training loss = 0.0219 and accuracy of 0.99
Iteration 100352: with minibatch training loss = 0.0597 and accuracy of 0.98
Iteration 100480: with minibatch training loss = 0.0231 and accuracy of 0.99
Iteration 100608: with minibatch training loss = 0.0139 and accuracy of 1
Iteration 100736: with minibatch training loss = 0.0896 and accuracy of 0.98
Iteration 100864: with minibatch training loss = 0.0103 and accuracy of 0.99
Iteration 100992: with minibatch training loss = 0.0396 and accuracy of 0.98
Iteration 101120: with minibatch training loss = 0.0635 and accuracy of 0.98
Iteration 101248: with minibatch training loss = 0.0184 and accuracy of 1
Iteration 101376: with minibatch training loss = 0.0706 and accuracy of 0.97
Iteration 101504: with minibatch training loss = 0.00649 and accuracy of 1
Iteration 101632: with minibatch training loss = 0.0169 and accuracy of 0.99
Iteration 101760: with minibatch training loss = 0.0268 and accuracy of 0.99
Iteration 101888: with minibatch training loss = 0.00944 and accuracy of 1
Iteration 102016: with minibatch training loss = 0.0953 and accuracy of 0.98
Iteration 102144: with minibatch training loss = 0.027 and accuracy of 0.98
Iteration 102272: with minibatch training loss = 0.0181 and accuracy of 0.99
Iteration 102400: with minibatch training loss = 0.0799 and accuracy of 0.98
Iteration 102528: with minibatch training loss = 0.0715 and accuracy of 0.99
Iteration 102656: with minibatch training loss = 0.0194 and accuracy of 1
Iteration 102784: with minibatch training loss = 0.0762 and accuracy of 0.99
Iteration 102912: with minibatch training loss = 0.00591 and accuracy of 1
Iteration 103040: with minibatch training loss = 0.168 and accuracy of 0.95
Iteration 103168: with minibatch training loss = 0.138 and accuracy of 0.97
Iteration 103296: with minibatch training loss = 0.0602 and accuracy of 0.97
Iteration 103424: with minibatch training loss = 0.0699 and accuracy of 0.98
Iteration 103552: with minibatch training loss = 0.0905 and accuracy of 0.99
Iteration 103680: with minibatch training loss = 0.0768 and accuracy of 0.98
Epoch 25, Train loss: 0.0442 and Train accuracy of 0.988, Test loss: 0.148 and Test accuracy of 0.965
Iteration 103808: with minibatch training loss = 0.0309 and accuracy of 0.98
Iteration 103936: with minibatch training loss = 0.0229 and accuracy of 0.99
Iteration 104064: with minibatch training loss = 0.0803 and accuracy of 0.98
Iteration 104192: with minibatch training loss = 0.0211 and accuracy of 0.99
Iteration 104320: with minibatch training loss = 0.00583 and accuracy of 1
Iteration 104448: with minibatch training loss = 0.0948 and accuracy of 0.99
Iteration 104576: with minibatch training loss = 0.0103 and accuracy of 1
Iteration 104704: with minibatch training loss = 0.0112 and accuracy of 1
Iteration 104832: with minibatch training loss = 0.0249 and accuracy of 0.98
Iteration 104960: with minibatch training loss = 0.028 and accuracy of 0.98
Iteration 105088: with minibatch training loss = 0.168 and accuracy of 0.96
Iteration 105216: with minibatch training loss = 0.0327 and accuracy of 0.99
Iteration 105344: with minibatch training loss = 0.0217 and accuracy of 1
Iteration 105472: with minibatch training loss = 0.0277 and accuracy of 0.99
Iteration 105600: with minibatch training loss = 0.0968 and accuracy of 0.98
Iteration 105728: with minibatch training loss = 0.00202 and accuracy of 1
Iteration 105856: with minibatch training loss = 0.0336 and accuracy of 0.98
Iteration 105984: with minibatch training loss = 0.0757 and accuracy of 0.98
Iteration 106112: with minibatch training loss = 0.00331 and accuracy of 1
Iteration 106240: with minibatch training loss = 0.0182 and accuracy of 0.99
Iteration 106368: with minibatch training loss = 0.122 and accuracy of 0.98
Iteration 106496: with minibatch training loss = 0.0186 and accuracy of 0.99
Iteration 106624: with minibatch training loss = 0.0336 and accuracy of 0.99
Iteration 106752: with minibatch training loss = 0.0178 and accuracy of 0.99
Iteration 106880: with minibatch training loss = 0.00207 and accuracy of 1
Iteration 107008: with minibatch training loss = 0.0284 and accuracy of 0.99
Iteration 107136: with minibatch training loss = 0.11 and accuracy of 0.98
Iteration 107264: with minibatch training loss = 0.0295 and accuracy of 0.98
Iteration 107392: with minibatch training loss = 0.019 and accuracy of 0.99
Iteration 107520: with minibatch training loss = 0.0257 and accuracy of 0.98
Iteration 107648: with minibatch training loss = 0.00839 and accuracy of 1
Iteration 107776: with minibatch training loss = 0.0227 and accuracy of 0.99
Epoch 26, Train loss: 0.0432 and Train accuracy of 0.988, Test loss: 0.145 and Test accuracy of 0.966
Iteration 107904: with minibatch training loss = 0.0423 and accuracy of 0.99
Iteration 108032: with minibatch training loss = 0.0518 and accuracy of 0.98
Iteration 108160: with minibatch training loss = 0.0403 and accuracy of 0.98
Iteration 108288: with minibatch training loss = 0.0497 and accuracy of 0.98
Iteration 108416: with minibatch training loss = 0.0683 and accuracy of 0.98
Iteration 108544: with minibatch training loss = 0.0267 and accuracy of 0.99
Iteration 108672: with minibatch training loss = 0.0516 and accuracy of 0.98
Iteration 108800: with minibatch training loss = 0.0148 and accuracy of 0.99
Iteration 108928: with minibatch training loss = 0.0206 and accuracy of 0.99
Iteration 109056: with minibatch training loss = 0.0297 and accuracy of 0.98
Iteration 109184: with minibatch training loss = 0.0514 and accuracy of 0.99
Iteration 109312: with minibatch training loss = 0.0714 and accuracy of 0.98
Iteration 109440: with minibatch training loss = 0.0188 and accuracy of 0.99
Iteration 109568: with minibatch training loss = 0.0162 and accuracy of 0.99
Iteration 109696: with minibatch training loss = 0.023 and accuracy of 0.99
Iteration 109824: with minibatch training loss = 0.0145 and accuracy of 1
Iteration 109952: with minibatch training loss = 0.019 and accuracy of 1
Iteration 110080: with minibatch training loss = 0.0288 and accuracy of 0.98
Iteration 110208: with minibatch training loss = 0.0588 and accuracy of 0.98
Iteration 110336: with minibatch training loss = 0.0865 and accuracy of 0.97
Iteration 110464: with minibatch training loss = 0.0455 and accuracy of 0.99
Iteration 110592: with minibatch training loss = 0.0895 and accuracy of 0.97
Iteration 110720: with minibatch training loss = 0.0404 and accuracy of 0.99
Iteration 110848: with minibatch training loss = 0.00311 and accuracy of 1
Iteration 110976: with minibatch training loss = 0.0941 and accuracy of 0.98
Iteration 111104: with minibatch training loss = 0.0178 and accuracy of 0.99
Iteration 111232: with minibatch training loss = 0.107 and accuracy of 0.99
Iteration 111360: with minibatch training loss = 0.00424 and accuracy of 1
Iteration 111488: with minibatch training loss = 0.03 and accuracy of 1
Iteration 111616: with minibatch training loss = 0.0283 and accuracy of 0.98
Iteration 111744: with minibatch training loss = 0.0431 and accuracy of 0.98
Iteration 111872: with minibatch training loss = 0.0267 and accuracy of 0.99
Iteration 112000: with minibatch training loss = 0.0181 and accuracy of 0.99
Epoch 27, Train loss: 0.0419 and Train accuracy of 0.989, Test loss: 0.147 and Test accuracy of 0.966
Iteration 112128: with minibatch training loss = 0.0328 and accuracy of 0.99
Iteration 112256: with minibatch training loss = 0.0183 and accuracy of 0.99
Iteration 112384: with minibatch training loss = 0.0932 and accuracy of 0.98
Iteration 112512: with minibatch training loss = 0.0719 and accuracy of 0.98
Iteration 112640: with minibatch training loss = 0.0278 and accuracy of 0.99
Iteration 112768: with minibatch training loss = 0.094 and accuracy of 0.98
Iteration 112896: with minibatch training loss = 0.0206 and accuracy of 0.99
Iteration 113024: with minibatch training loss = 0.0847 and accuracy of 0.98
Iteration 113152: with minibatch training loss = 0.0313 and accuracy of 0.98
Iteration 113280: with minibatch training loss = 0.0154 and accuracy of 1
Iteration 113408: with minibatch training loss = 0.062 and accuracy of 0.98
Iteration 113536: with minibatch training loss = 0.00509 and accuracy of 1
Iteration 113664: with minibatch training loss = 0.0658 and accuracy of 0.98
Iteration 113792: with minibatch training loss = 0.0299 and accuracy of 0.99
Iteration 113920: with minibatch training loss = 0.0033 and accuracy of 1
Iteration 114048: with minibatch training loss = 0.0266 and accuracy of 0.99
Iteration 114176: with minibatch training loss = 0.0187 and accuracy of 0.99
Iteration 114304: with minibatch training loss = 0.0453 and accuracy of 0.98
Iteration 114432: with minibatch training loss = 0.0276 and accuracy of 0.98
Iteration 114560: with minibatch training loss = 0.089 and accuracy of 0.99
Iteration 114688: with minibatch training loss = 0.0549 and accuracy of 0.98
Iteration 114816: with minibatch training loss = 0.00555 and accuracy of 1
Iteration 114944: with minibatch training loss = 0.042 and accuracy of 0.98
Iteration 115072: with minibatch training loss = 0.00486 and accuracy of 1
Iteration 115200: with minibatch training loss = 0.00582 and accuracy of 1
Iteration 115328: with minibatch training loss = 0.0922 and accuracy of 0.99
Iteration 115456: with minibatch training loss = 0.0127 and accuracy of 0.99
Iteration 115584: with minibatch training loss = 0.0176 and accuracy of 1
Iteration 115712: with minibatch training loss = 0.03 and accuracy of 0.99
Iteration 115840: with minibatch training loss = 0.0111 and accuracy of 0.99
Iteration 115968: with minibatch training loss = 0.0116 and accuracy of 1
Iteration 116096: with minibatch training loss = 0.0828 and accuracy of 0.98
Epoch 28, Train loss: 0.0412 and Train accuracy of 0.989, Test loss: 0.15 and Test accuracy of 0.966
Iteration 116224: with minibatch training loss = 0.0731 and accuracy of 0.97
Iteration 116352: with minibatch training loss = 0.0742 and accuracy of 0.99
Iteration 116480: with minibatch training loss = 0.0139 and accuracy of 0.99
Iteration 116608: with minibatch training loss = 0.0399 and accuracy of 0.99
Iteration 116736: with minibatch training loss = 0.00914 and accuracy of 1
Iteration 116864: with minibatch training loss = 0.0342 and accuracy of 0.98
Iteration 116992: with minibatch training loss = 0.0346 and accuracy of 0.98
Iteration 117120: with minibatch training loss = 0.105 and accuracy of 0.98
Iteration 117248: with minibatch training loss = 0.0675 and accuracy of 0.98
Iteration 117376: with minibatch training loss = 0.0822 and accuracy of 0.98
Iteration 117504: with minibatch training loss = 0.0157 and accuracy of 1
Iteration 117632: with minibatch training loss = 0.017 and accuracy of 0.99
Iteration 117760: with minibatch training loss = 0.0178 and accuracy of 1
Iteration 117888: with minibatch training loss = 0.0242 and accuracy of 0.99
Iteration 118016: with minibatch training loss = 0.0862 and accuracy of 0.97
Iteration 118144: with minibatch training loss = 0.113 and accuracy of 0.98
Iteration 118272: with minibatch training loss = 0.0401 and accuracy of 0.98
Iteration 118400: with minibatch training loss = 0.0403 and accuracy of 0.99
Iteration 118528: with minibatch training loss = 0.104 and accuracy of 0.98
Iteration 118656: with minibatch training loss = 0.0268 and accuracy of 0.99
Iteration 118784: with minibatch training loss = 0.0134 and accuracy of 1
Iteration 118912: with minibatch training loss = 0.0872 and accuracy of 0.98
Iteration 119040: with minibatch training loss = 0.0719 and accuracy of 0.99
Iteration 119168: with minibatch training loss = 0.0194 and accuracy of 0.99
Iteration 119296: with minibatch training loss = 0.0221 and accuracy of 0.99
Iteration 119424: with minibatch training loss = 0.107 and accuracy of 0.99
Iteration 119552: with minibatch training loss = 0.0258 and accuracy of 0.98
Iteration 119680: with minibatch training loss = 0.00721 and accuracy of 1
Iteration 119808: with minibatch training loss = 0.133 and accuracy of 0.97
Iteration 119936: with minibatch training loss = 0.0388 and accuracy of 0.99
Iteration 120064: with minibatch training loss = 0.0986 and accuracy of 0.98
Iteration 120192: with minibatch training loss = 0.0132 and accuracy of 0.99
Iteration 120320: with minibatch training loss = 0.0396 and accuracy of 0.98
Epoch 29, Train loss: 0.0401 and Train accuracy of 0.989, Test loss: 0.146 and Test accuracy of 0.967
Iteration 120448: with minibatch training loss = 0.0228 and accuracy of 0.99
Iteration 120576: with minibatch training loss = 0.00835 and accuracy of 1
Iteration 120704: with minibatch training loss = 0.0531 and accuracy of 0.98
Iteration 120832: with minibatch training loss = 0.0693 and accuracy of 0.98
Iteration 120960: with minibatch training loss = 0.0216 and accuracy of 0.99
Iteration 121088: with minibatch training loss = 0.032 and accuracy of 0.99
Iteration 121216: with minibatch training loss = 0.0621 and accuracy of 0.99
Iteration 121344: with minibatch training loss = 0.0283 and accuracy of 0.99
Iteration 121472: with minibatch training loss = 0.0275 and accuracy of 0.98
Iteration 121600: with minibatch training loss = 0.0285 and accuracy of 0.99
Iteration 121728: with minibatch training loss = 0.046 and accuracy of 0.99
Iteration 121856: with minibatch training loss = 0.0642 and accuracy of 0.97
Iteration 121984: with minibatch training loss = 0.0213 and accuracy of 0.99
Iteration 122112: with minibatch training loss = 0.00671 and accuracy of 1
Iteration 122240: with minibatch training loss = 0.0176 and accuracy of 0.99
Iteration 122368: with minibatch training loss = 0.00862 and accuracy of 1
Iteration 122496: with minibatch training loss = 0.0356 and accuracy of 0.98
Iteration 122624: with minibatch training loss = 0.0369 and accuracy of 0.99
Iteration 122752: with minibatch training loss = 0.00676 and accuracy of 1
Iteration 122880: with minibatch training loss = 0.12 and accuracy of 0.97
Iteration 123008: with minibatch training loss = 0.008 and accuracy of 1
Iteration 123136: with minibatch training loss = 0.054 and accuracy of 0.97
Iteration 123264: with minibatch training loss = 0.0458 and accuracy of 0.98
Iteration 123392: with minibatch training loss = 0.0368 and accuracy of 0.99
Iteration 123520: with minibatch training loss = 0.0297 and accuracy of 0.99
Iteration 123648: with minibatch training loss = 0.0771 and accuracy of 0.98
Iteration 123776: with minibatch training loss = 0.0349 and accuracy of 0.99
Iteration 123904: with minibatch training loss = 0.0147 and accuracy of 0.99
Iteration 124032: with minibatch training loss = 0.0425 and accuracy of 0.99
Iteration 124160: with minibatch training loss = 0.0544 and accuracy of 0.98
Iteration 124288: with minibatch training loss = 0.00906 and accuracy of 1
Iteration 124416: with minibatch training loss = 0.0157 and accuracy of 0.99
Epoch 30, Train loss: 0.0394 and Train accuracy of 0.989, Test loss: 0.145 and Test accuracy of 0.966
Iteration 124544: with minibatch training loss = 0.0758 and accuracy of 0.98
Iteration 124672: with minibatch training loss = 0.0234 and accuracy of 0.99
Iteration 124800: with minibatch training loss = 0.0398 and accuracy of 0.98
Iteration 124928: with minibatch training loss = 0.0699 and accuracy of 0.99
Iteration 125056: with minibatch training loss = 0.0641 and accuracy of 0.99
Iteration 125184: with minibatch training loss = 0.119 and accuracy of 0.97
Iteration 125312: with minibatch training loss = 0.0436 and accuracy of 0.99
Iteration 125440: with minibatch training loss = 0.00679 and accuracy of 1
Iteration 125568: with minibatch training loss = 0.0319 and accuracy of 0.98
Iteration 125696: with minibatch training loss = 0.0478 and accuracy of 0.98
Iteration 125824: with minibatch training loss = 0.027 and accuracy of 0.99
Iteration 125952: with minibatch training loss = 0.0166 and accuracy of 0.99
Iteration 126080: with minibatch training loss = 0.149 and accuracy of 0.97
Iteration 126208: with minibatch training loss = 0.0526 and accuracy of 0.98
Iteration 126336: with minibatch training loss = 0.0115 and accuracy of 1
Iteration 126464: with minibatch training loss = 0.0282 and accuracy of 0.98
Iteration 126592: with minibatch training loss = 0.0473 and accuracy of 0.98
Iteration 126720: with minibatch training loss = 0.00978 and accuracy of 1
Iteration 126848: with minibatch training loss = 0.0116 and accuracy of 0.99
Iteration 126976: with minibatch training loss = 0.053 and accuracy of 0.98
Iteration 127104: with minibatch training loss = 0.106 and accuracy of 0.98
Iteration 127232: with minibatch training loss = 0.0386 and accuracy of 0.98
Iteration 127360: with minibatch training loss = 0.0323 and accuracy of 0.99
Iteration 127488: with minibatch training loss = 0.059 and accuracy of 0.98
Iteration 127616: with minibatch training loss = 0.00408 and accuracy of 1
Iteration 127744: with minibatch training loss = 0.0159 and accuracy of 1
Iteration 127872: with minibatch training loss = 0.00377 and accuracy of 1
Iteration 128000: with minibatch training loss = 0.0414 and accuracy of 0.98
Iteration 128128: with minibatch training loss = 0.00724 and accuracy of 1
Iteration 128256: with minibatch training loss = 0.0263 and accuracy of 0.99
Iteration 128384: with minibatch training loss = 0.0575 and accuracy of 0.98
Iteration 128512: with minibatch training loss = 0.00266 and accuracy of 1
Iteration 128640: with minibatch training loss = 0.0104 and accuracy of 1
Epoch 31, Train loss: 0.0384 and Train accuracy of 0.99, Test loss: 0.15 and Test accuracy of 0.966
Iteration 128768: with minibatch training loss = 0.0129 and accuracy of 1
Iteration 128896: with minibatch training loss = 0.0179 and accuracy of 0.99
Iteration 129024: with minibatch training loss = 0.0455 and accuracy of 0.98
Iteration 129152: with minibatch training loss = 0.00399 and accuracy of 1
Iteration 129280: with minibatch training loss = 0.00569 and accuracy of 1
Iteration 129408: with minibatch training loss = 0.00886 and accuracy of 1
Iteration 129536: with minibatch training loss = 0.031 and accuracy of 0.98
Iteration 129664: with minibatch training loss = 0.0634 and accuracy of 0.98
Iteration 129792: with minibatch training loss = 0.0263 and accuracy of 0.99
Iteration 129920: with minibatch training loss = 0.0419 and accuracy of 0.99
Iteration 130048: with minibatch training loss = 0.00431 and accuracy of 1
Iteration 130176: with minibatch training loss = 0.0433 and accuracy of 0.99
Iteration 130304: with minibatch training loss = 0.043 and accuracy of 0.98
Iteration 130432: with minibatch training loss = 0.0231 and accuracy of 0.99
Iteration 130560: with minibatch training loss = 0.0363 and accuracy of 0.99
Iteration 130688: with minibatch training loss = 0.109 and accuracy of 0.98
Iteration 130816: with minibatch training loss = 0.0206 and accuracy of 0.99
Iteration 130944: with minibatch training loss = 0.0262 and accuracy of 0.99
Iteration 131072: with minibatch training loss = 0.00792 and accuracy of 1
Iteration 131200: with minibatch training loss = 0.00391 and accuracy of 1
Iteration 131328: with minibatch training loss = 0.0231 and accuracy of 0.99
Iteration 131456: with minibatch training loss = 0.106 and accuracy of 0.98
Iteration 131584: with minibatch training loss = 0.00883 and accuracy of 1
Iteration 131712: with minibatch training loss = 0.00838 and accuracy of 1
Iteration 131840: with minibatch training loss = 0.0352 and accuracy of 0.99
Iteration 131968: with minibatch training loss = 0.0257 and accuracy of 0.99
Iteration 132096: with minibatch training loss = 0.00539 and accuracy of 1
Iteration 132224: with minibatch training loss = 0.0117 and accuracy of 0.99
Iteration 132352: with minibatch training loss = 0.155 and accuracy of 0.98
Iteration 132480: with minibatch training loss = 0.128 and accuracy of 0.97
Iteration 132608: with minibatch training loss = 0.00466 and accuracy of 1
Iteration 132736: with minibatch training loss = 0.0691 and accuracy of 0.98
Epoch 32, Train loss: 0.0384 and Train accuracy of 0.99, Test loss: 0.151 and Test accuracy of 0.965
Iteration 132864: with minibatch training loss = 0.00286 and accuracy of 1
Iteration 132992: with minibatch training loss = 0.00565 and accuracy of 1
Iteration 133120: with minibatch training loss = 0.0366 and accuracy of 0.98
Iteration 133248: with minibatch training loss = 0.0326 and accuracy of 0.99
Iteration 133376: with minibatch training loss = 0.0246 and accuracy of 0.98
Iteration 133504: with minibatch training loss = 0.0199 and accuracy of 0.99
Iteration 133632: with minibatch training loss = 0.00833 and accuracy of 1
Iteration 133760: with minibatch training loss = 0.0235 and accuracy of 0.99
Iteration 133888: with minibatch training loss = 0.129 and accuracy of 0.98
Iteration 134016: with minibatch training loss = 0.00816 and accuracy of 1
Iteration 134144: with minibatch training loss = 0.00909 and accuracy of 1
Iteration 134272: with minibatch training loss = 0.0449 and accuracy of 0.98
Iteration 134400: with minibatch training loss = 0.0138 and accuracy of 0.99
Iteration 134528: with minibatch training loss = 0.00611 and accuracy of 1
Iteration 134656: with minibatch training loss = 0.0315 and accuracy of 0.99
Iteration 134784: with minibatch training loss = 0.00252 and accuracy of 1
Iteration 134912: with minibatch training loss = 0.0398 and accuracy of 0.99
Iteration 135040: with minibatch training loss = 0.0169 and accuracy of 0.99
Iteration 135168: with minibatch training loss = 0.0333 and accuracy of 0.99
Iteration 135296: with minibatch training loss = 0.0348 and accuracy of 0.98
Iteration 135424: with minibatch training loss = 0.152 and accuracy of 0.94
Iteration 135552: with minibatch training loss = 0.00666 and accuracy of 1
Iteration 135680: with minibatch training loss = 0.0272 and accuracy of 0.99
Iteration 135808: with minibatch training loss = 0.0395 and accuracy of 0.99
Iteration 135936: with minibatch training loss = 0.0999 and accuracy of 0.98
Iteration 136064: with minibatch training loss = 0.0813 and accuracy of 0.99
Iteration 136192: with minibatch training loss = 0.0575 and accuracy of 0.99
Iteration 136320: with minibatch training loss = 0.00492 and accuracy of 1
Iteration 136448: with minibatch training loss = 0.0101 and accuracy of 0.99
Iteration 136576: with minibatch training loss = 0.0352 and accuracy of 0.99
Iteration 136704: with minibatch training loss = 0.0115 and accuracy of 1
Iteration 136832: with minibatch training loss = 0.0102 and accuracy of 1
Epoch 33, Train loss: 0.0368 and Train accuracy of 0.99, Test loss: 0.151 and Test accuracy of 0.967
Iteration 136960: with minibatch training loss = 0.013 and accuracy of 1
Iteration 137088: with minibatch training loss = 0.127 and accuracy of 0.97
Iteration 137216: with minibatch training loss = 0.0188 and accuracy of 0.99
Iteration 137344: with minibatch training loss = 0.0866 and accuracy of 0.98
Iteration 137472: with minibatch training loss = 0.0268 and accuracy of 0.98
Iteration 137600: with minibatch training loss = 0.00693 and accuracy of 1
Iteration 137728: with minibatch training loss = 0.0212 and accuracy of 0.99
Iteration 137856: with minibatch training loss = 0.00902 and accuracy of 1
Iteration 137984: with minibatch training loss = 0.00768 and accuracy of 1
Iteration 138112: with minibatch training loss = 0.0603 and accuracy of 0.98
Iteration 138240: with minibatch training loss = 0.0349 and accuracy of 0.99
Iteration 138368: with minibatch training loss = 0.00779 and accuracy of 1
Iteration 138496: with minibatch training loss = 0.0247 and accuracy of 0.99
Iteration 138624: with minibatch training loss = 0.0189 and accuracy of 0.99
Iteration 138752: with minibatch training loss = 0.0203 and accuracy of 0.99
Iteration 138880: with minibatch training loss = 0.00318 and accuracy of 1
Iteration 139008: with minibatch training loss = 0.0158 and accuracy of 0.99
Iteration 139136: with minibatch training loss = 0.112 and accuracy of 0.98
Iteration 139264: with minibatch training loss = 0.133 and accuracy of 0.98
Iteration 139392: with minibatch training loss = 0.0401 and accuracy of 0.99
Iteration 139520: with minibatch training loss = 0.104 and accuracy of 0.99
Iteration 139648: with minibatch training loss = 0.0377 and accuracy of 0.98
Iteration 139776: with minibatch training loss = 0.0138 and accuracy of 0.99
Iteration 139904: with minibatch training loss = 0.0169 and accuracy of 0.99
Iteration 140032: with minibatch training loss = 0.00384 and accuracy of 1
Iteration 140160: with minibatch training loss = 0.0556 and accuracy of 0.98
Iteration 140288: with minibatch training loss = 0.0426 and accuracy of 0.99
Iteration 140416: with minibatch training loss = 0.0148 and accuracy of 0.99
Iteration 140544: with minibatch training loss = 0.0157 and accuracy of 1
Iteration 140672: with minibatch training loss = 0.0146 and accuracy of 0.99
Iteration 140800: with minibatch training loss = 0.0407 and accuracy of 0.99
Iteration 140928: with minibatch training loss = 0.0325 and accuracy of 0.99
Iteration 141056: with minibatch training loss = 0.0303 and accuracy of 0.99
Epoch 34, Train loss: 0.0362 and Train accuracy of 0.99, Test loss: 0.151 and Test accuracy of 0.966
Iteration 141184: with minibatch training loss = 0.027 and accuracy of 0.99
Iteration 141312: with minibatch training loss = 0.0109 and accuracy of 1
Iteration 141440: with minibatch training loss = 0.0215 and accuracy of 0.98
Iteration 141568: with minibatch training loss = 0.0515 and accuracy of 0.99
Iteration 141696: with minibatch training loss = 0.0294 and accuracy of 0.99
Iteration 141824: with minibatch training loss = 0.0387 and accuracy of 0.98
Iteration 141952: with minibatch training loss = 0.00686 and accuracy of 1
Iteration 142080: with minibatch training loss = 0.00836 and accuracy of 1
Iteration 142208: with minibatch training loss = 0.005 and accuracy of 1
Iteration 142336: with minibatch training loss = 0.113 and accuracy of 0.98
Iteration 142464: with minibatch training loss = 0.0502 and accuracy of 0.98
Iteration 142592: with minibatch training loss = 0.0095 and accuracy of 1
Iteration 142720: with minibatch training loss = 0.0526 and accuracy of 0.98
Iteration 142848: with minibatch training loss = 0.0481 and accuracy of 0.99
Iteration 142976: with minibatch training loss = 0.0111 and accuracy of 1
Iteration 143104: with minibatch training loss = 0.0212 and accuracy of 0.99
Iteration 143232: with minibatch training loss = 0.013 and accuracy of 1
Iteration 143360: with minibatch training loss = 0.017 and accuracy of 1
Iteration 143488: with minibatch training loss = 0.0529 and accuracy of 0.99
Iteration 143616: with minibatch training loss = 0.00546 and accuracy of 1
Iteration 143744: with minibatch training loss = 0.0941 and accuracy of 0.98
Iteration 143872: with minibatch training loss = 0.0199 and accuracy of 0.99
Iteration 144000: with minibatch training loss = 0.0285 and accuracy of 0.99
Iteration 144128: with minibatch training loss = 0.0822 and accuracy of 0.99
Iteration 144256: with minibatch training loss = 0.0147 and accuracy of 0.99
Iteration 144384: with minibatch training loss = 0.0331 and accuracy of 0.99
Iteration 144512: with minibatch training loss = 0.0706 and accuracy of 0.98
Iteration 144640: with minibatch training loss = 0.0356 and accuracy of 0.98
Iteration 144768: with minibatch training loss = 0.0176 and accuracy of 0.98
Iteration 144896: with minibatch training loss = 0.00686 and accuracy of 1
Iteration 145024: with minibatch training loss = 0.0101 and accuracy of 1
Iteration 145152: with minibatch training loss = 0.0309 and accuracy of 0.99
Epoch 35, Train loss: 0.0363 and Train accuracy of 0.99, Test loss: 0.161 and Test accuracy of 0.963
Iteration 145280: with minibatch training loss = 0.00907 and accuracy of 1
Iteration 145408: with minibatch training loss = 0.00751 and accuracy of 1
Iteration 145536: with minibatch training loss = 0.0088 and accuracy of 0.99
Iteration 145664: with minibatch training loss = 0.0274 and accuracy of 0.99
Iteration 145792: with minibatch training loss = 0.206 and accuracy of 0.96
Iteration 145920: with minibatch training loss = 0.0319 and accuracy of 0.99
Iteration 146048: with minibatch training loss = 0.0123 and accuracy of 0.99
Iteration 146176: with minibatch training loss = 0.083 and accuracy of 0.98
Iteration 146304: with minibatch training loss = 0.0169 and accuracy of 1
Iteration 146432: with minibatch training loss = 0.00644 and accuracy of 1
Iteration 146560: with minibatch training loss = 0.0412 and accuracy of 0.99
Iteration 146688: with minibatch training loss = 0.0558 and accuracy of 0.99
Iteration 146816: with minibatch training loss = 0.0119 and accuracy of 1
Iteration 146944: with minibatch training loss = 0.0186 and accuracy of 0.99
Iteration 147072: with minibatch training loss = 0.129 and accuracy of 0.98
Iteration 147200: with minibatch training loss = 0.0145 and accuracy of 1
Iteration 147328: with minibatch training loss = 0.00443 and accuracy of 1
Iteration 147456: with minibatch training loss = 0.00449 and accuracy of 1
Iteration 147584: with minibatch training loss = 0.0128 and accuracy of 0.99
Iteration 147712: with minibatch training loss = 0.0271 and accuracy of 0.99
Iteration 147840: with minibatch training loss = 0.00303 and accuracy of 1
Iteration 147968: with minibatch training loss = 0.0846 and accuracy of 0.98
Iteration 148096: with minibatch training loss = 0.044 and accuracy of 0.99
Iteration 148224: with minibatch training loss = 0.0213 and accuracy of 0.99
Iteration 148352: with minibatch training loss = 0.0184 and accuracy of 0.99
Iteration 148480: with minibatch training loss = 0.056 and accuracy of 0.99
Iteration 148608: with minibatch training loss = 0.00275 and accuracy of 1
Iteration 148736: with minibatch training loss = 0.039 and accuracy of 0.99
Iteration 148864: with minibatch training loss = 0.0225 and accuracy of 0.98
Iteration 148992: with minibatch training loss = 0.00597 and accuracy of 1
Iteration 149120: with minibatch training loss = 0.0346 and accuracy of 0.99
Iteration 149248: with minibatch training loss = 0.109 and accuracy of 0.98
Iteration 149376: with minibatch training loss = 0.00788 and accuracy of 1
Epoch 36, Train loss: 0.0347 and Train accuracy of 0.991, Test loss: 0.154 and Test accuracy of 0.965
Iteration 149504: with minibatch training loss = 0.0446 and accuracy of 0.99
Iteration 149632: with minibatch training loss = 0.0425 and accuracy of 0.98
Iteration 149760: with minibatch training loss = 0.00642 and accuracy of 1
Iteration 149888: with minibatch training loss = 0.0367 and accuracy of 0.99
Iteration 150016: with minibatch training loss = 0.0758 and accuracy of 0.98
Iteration 150144: with minibatch training loss = 0.0865 and accuracy of 0.98
Iteration 150272: with minibatch training loss = 0.0493 and accuracy of 0.99
Iteration 150400: with minibatch training loss = 0.0392 and accuracy of 0.98
Iteration 150528: with minibatch training loss = 0.106 and accuracy of 0.98
Iteration 150656: with minibatch training loss = 0.0124 and accuracy of 1
Iteration 150784: with minibatch training loss = 0.0576 and accuracy of 0.98
Iteration 150912: with minibatch training loss = 0.0668 and accuracy of 0.99
Iteration 151040: with minibatch training loss = 0.124 and accuracy of 0.98
Iteration 151168: with minibatch training loss = 0.0165 and accuracy of 0.99
Iteration 151296: with minibatch training loss = 0.0897 and accuracy of 0.98
Iteration 151424: with minibatch training loss = 0.00508 and accuracy of 1
Iteration 151552: with minibatch training loss = 0.0382 and accuracy of 0.98
Iteration 151680: with minibatch training loss = 0.0187 and accuracy of 1
Iteration 151808: with minibatch training loss = 0.0498 and accuracy of 0.98
Iteration 151936: with minibatch training loss = 0.0117 and accuracy of 1
Iteration 152064: with minibatch training loss = 0.0124 and accuracy of 0.99
Iteration 152192: with minibatch training loss = 0.0822 and accuracy of 0.98
Iteration 152320: with minibatch training loss = 0.00442 and accuracy of 1
Iteration 152448: with minibatch training loss = 0.116 and accuracy of 0.98
Iteration 152576: with minibatch training loss = 0.0123 and accuracy of 0.99
Iteration 152704: with minibatch training loss = 0.00691 and accuracy of 1
Iteration 152832: with minibatch training loss = 0.013 and accuracy of 0.99
Iteration 152960: with minibatch training loss = 0.00394 and accuracy of 1
Iteration 153088: with minibatch training loss = 0.0546 and accuracy of 0.98
Iteration 153216: with minibatch training loss = 0.104 and accuracy of 0.98
Iteration 153344: with minibatch training loss = 0.0133 and accuracy of 0.99
Iteration 153472: with minibatch training loss = 0.00647 and accuracy of 1
Epoch 37, Train loss: 0.0337 and Train accuracy of 0.991, Test loss: 0.156 and Test accuracy of 0.966
Iteration 153600: with minibatch training loss = 0.0067 and accuracy of 1
Iteration 153728: with minibatch training loss = 0.00698 and accuracy of 1
Iteration 153856: with minibatch training loss = 0.0307 and accuracy of 0.98
Iteration 153984: with minibatch training loss = 0.00315 and accuracy of 1
Iteration 154112: with minibatch training loss = 0.0267 and accuracy of 0.98
Iteration 154240: with minibatch training loss = 0.00623 and accuracy of 1
Iteration 154368: with minibatch training loss = 0.0325 and accuracy of 0.99
Iteration 154496: with minibatch training loss = 0.0149 and accuracy of 0.99
Iteration 154624: with minibatch training loss = 0.0102 and accuracy of 1
Iteration 154752: with minibatch training loss = 0.0406 and accuracy of 0.99
Iteration 154880: with minibatch training loss = 0.0312 and accuracy of 0.99
Iteration 155008: with minibatch training loss = 0.00198 and accuracy of 1
Iteration 155136: with minibatch training loss = 0.0359 and accuracy of 0.99
Iteration 155264: with minibatch training loss = 0.00608 and accuracy of 1
Iteration 155392: with minibatch training loss = 0.0208 and accuracy of 0.99
Iteration 155520: with minibatch training loss = 0.105 and accuracy of 0.98
Iteration 155648: with minibatch training loss = 0.0235 and accuracy of 0.99
Iteration 155776: with minibatch training loss = 0.00697 and accuracy of 1
Iteration 155904: with minibatch training loss = 0.0365 and accuracy of 0.99
Iteration 156032: with minibatch training loss = 0.0407 and accuracy of 0.99
Iteration 156160: with minibatch training loss = 0.00619 and accuracy of 1
Iteration 156288: with minibatch training loss = 0.0065 and accuracy of 1
Iteration 156416: with minibatch training loss = 0.0911 and accuracy of 0.98
Iteration 156544: with minibatch training loss = 0.0281 and accuracy of 0.98
Iteration 156672: with minibatch training loss = 0.0359 and accuracy of 0.99
Iteration 156800: with minibatch training loss = 0.00338 and accuracy of 1
Iteration 156928: with minibatch training loss = 0.00857 and accuracy of 1
Iteration 157056: with minibatch training loss = 0.00514 and accuracy of 1
Iteration 157184: with minibatch training loss = 0.0402 and accuracy of 0.99
Iteration 157312: with minibatch training loss = 0.0565 and accuracy of 0.99
Iteration 157440: with minibatch training loss = 0.0204 and accuracy of 1
Iteration 157568: with minibatch training loss = 0.00222 and accuracy of 1
Iteration 157696: with minibatch training loss = 0.0366 and accuracy of 0.99
Epoch 38, Train loss: 0.034 and Train accuracy of 0.991, Test loss: 0.154 and Test accuracy of 0.966
Iteration 157824: with minibatch training loss = 0.00699 and accuracy of 1
Iteration 157952: with minibatch training loss = 0.0725 and accuracy of 0.98
Iteration 158080: with minibatch training loss = 0.00536 and accuracy of 1
Iteration 158208: with minibatch training loss = 0.0451 and accuracy of 0.98
Iteration 158336: with minibatch training loss = 0.0502 and accuracy of 0.98
Iteration 158464: with minibatch training loss = 0.00407 and accuracy of 1
Iteration 158592: with minibatch training loss = 0.0055 and accuracy of 1
Iteration 158720: with minibatch training loss = 0.00353 and accuracy of 1
Iteration 158848: with minibatch training loss = 0.0179 and accuracy of 0.99
Iteration 158976: with minibatch training loss = 0.00944 and accuracy of 1
Iteration 159104: with minibatch training loss = 0.00707 and accuracy of 1
Iteration 159232: with minibatch training loss = 0.0328 and accuracy of 0.99
Iteration 159360: with minibatch training loss = 0.00904 and accuracy of 1
Iteration 159488: with minibatch training loss = 0.00361 and accuracy of 1
Iteration 159616: with minibatch training loss = 0.00633 and accuracy of 1
Iteration 159744: with minibatch training loss = 0.00262 and accuracy of 1
Iteration 159872: with minibatch training loss = 0.03 and accuracy of 0.98
Iteration 160000: with minibatch training loss = 0.058 and accuracy of 0.98
Iteration 160128: with minibatch training loss = 0.0205 and accuracy of 0.99
Iteration 160256: with minibatch training loss = 0.0178 and accuracy of 0.99
Iteration 160384: with minibatch training loss = 0.106 and accuracy of 0.97
Iteration 160512: with minibatch training loss = 0.00586 and accuracy of 1
Iteration 160640: with minibatch training loss = 0.0153 and accuracy of 0.99
Iteration 160768: with minibatch training loss = 0.0422 and accuracy of 0.98
Iteration 160896: with minibatch training loss = 0.00849 and accuracy of 1
Iteration 161024: with minibatch training loss = 0.0061 and accuracy of 1
Iteration 161152: with minibatch training loss = 0.0414 and accuracy of 0.98
Iteration 161280: with minibatch training loss = 0.0429 and accuracy of 0.98
Iteration 161408: with minibatch training loss = 0.00498 and accuracy of 1
Iteration 161536: with minibatch training loss = 0.0562 and accuracy of 0.99
Iteration 161664: with minibatch training loss = 0.0504 and accuracy of 0.98
Iteration 161792: with minibatch training loss = 0.0304 and accuracy of 0.98
Epoch 39, Train loss: 0.0329 and Train accuracy of 0.991, Test loss: 0.163 and Test accuracy of 0.963
Iteration 161920: with minibatch training loss = 0.0173 and accuracy of 0.99
Iteration 162048: with minibatch training loss = 0.0745 and accuracy of 0.98
Iteration 162176: with minibatch training loss = 0.0346 and accuracy of 0.99
Iteration 162304: with minibatch training loss = 0.012 and accuracy of 0.99
Iteration 162432: with minibatch training loss = 0.0569 and accuracy of 0.99
Iteration 162560: with minibatch training loss = 0.00989 and accuracy of 1
Iteration 162688: with minibatch training loss = 0.0692 and accuracy of 0.98
Iteration 162816: with minibatch training loss = 0.0193 and accuracy of 0.99
Iteration 162944: with minibatch training loss = 0.00417 and accuracy of 1
Iteration 163072: with minibatch training loss = 0.0282 and accuracy of 0.99
Iteration 163200: with minibatch training loss = 0.0169 and accuracy of 0.99
Iteration 163328: with minibatch training loss = 0.0058 and accuracy of 1
Iteration 163456: with minibatch training loss = 0.0846 and accuracy of 0.97
Iteration 163584: with minibatch training loss = 0.0157 and accuracy of 0.99
Iteration 163712: with minibatch training loss = 0.0944 and accuracy of 0.98
Iteration 163840: with minibatch training loss = 0.0173 and accuracy of 0.99
Iteration 163968: with minibatch training loss = 0.057 and accuracy of 0.98
Iteration 164096: with minibatch training loss = 0.0294 and accuracy of 0.99
Iteration 164224: with minibatch training loss = 0.0466 and accuracy of 0.98
Iteration 164352: with minibatch training loss = 0.0289 and accuracy of 0.99
Iteration 164480: with minibatch training loss = 0.00515 and accuracy of 1
Iteration 164608: with minibatch training loss = 0.0122 and accuracy of 1
Iteration 164736: with minibatch training loss = 0.0399 and accuracy of 0.98
Iteration 164864: with minibatch training loss = 0.00834 and accuracy of 1
Iteration 164992: with minibatch training loss = 0.00151 and accuracy of 1
Iteration 165120: with minibatch training loss = 0.00362 and accuracy of 1
Iteration 165248: with minibatch training loss = 0.0108 and accuracy of 0.99
Iteration 165376: with minibatch training loss = 0.0246 and accuracy of 0.99
Iteration 165504: with minibatch training loss = 0.0147 and accuracy of 1
Iteration 165632: with minibatch training loss = 0.0425 and accuracy of 0.98
Iteration 165760: with minibatch training loss = 0.00331 and accuracy of 1
Iteration 165888: with minibatch training loss = 0.0046 and accuracy of 1
Epoch 40, Train loss: 0.0329 and Train accuracy of 0.991, Test loss: 0.154 and Test accuracy of 0.966
Iteration 166016: with minibatch training loss = 0.0394 and accuracy of 0.98
Iteration 166144: with minibatch training loss = 0.0479 and accuracy of 0.99
Iteration 166272: with minibatch training loss = 0.00943 and accuracy of 1
Iteration 166400: with minibatch training loss = 0.00825 and accuracy of 1
Iteration 166528: with minibatch training loss = 0.0146 and accuracy of 0.99
Iteration 166656: with minibatch training loss = 0.0203 and accuracy of 0.98
Iteration 166784: with minibatch training loss = 0.00891 and accuracy of 1
Iteration 166912: with minibatch training loss = 0.0534 and accuracy of 0.98
Iteration 167040: with minibatch training loss = 0.00747 and accuracy of 1
Iteration 167168: with minibatch training loss = 0.0457 and accuracy of 0.99
Iteration 167296: with minibatch training loss = 0.0362 and accuracy of 0.99
Iteration 167424: with minibatch training loss = 0.0226 and accuracy of 0.98
Iteration 167552: with minibatch training loss = 0.0477 and accuracy of 0.99
Iteration 167680: with minibatch training loss = 0.0232 and accuracy of 0.99
Iteration 167808: with minibatch training loss = 0.00517 and accuracy of 1
Iteration 167936: with minibatch training loss = 0.00741 and accuracy of 1
Iteration 168064: with minibatch training loss = 0.0276 and accuracy of 0.99
Iteration 168192: with minibatch training loss = 0.0868 and accuracy of 0.98
Iteration 168320: with minibatch training loss = 0.00639 and accuracy of 1
Iteration 168448: with minibatch training loss = 0.0862 and accuracy of 0.99
Iteration 168576: with minibatch training loss = 0.129 and accuracy of 0.97
Iteration 168704: with minibatch training loss = 0.021 and accuracy of 0.99
Iteration 168832: with minibatch training loss = 0.00526 and accuracy of 1
Iteration 168960: with minibatch training loss = 0.0255 and accuracy of 0.99
Iteration 169088: with minibatch training loss = 0.0169 and accuracy of 0.99
Iteration 169216: with minibatch training loss = 0.00472 and accuracy of 1
Iteration 169344: with minibatch training loss = 0.127 and accuracy of 0.98
Iteration 169472: with minibatch training loss = 0.00658 and accuracy of 1
Iteration 169600: with minibatch training loss = 0.00488 and accuracy of 1
Iteration 169728: with minibatch training loss = 0.0355 and accuracy of 0.99
Iteration 169856: with minibatch training loss = 0.0195 and accuracy of 0.99
Iteration 169984: with minibatch training loss = 0.0443 and accuracy of 0.99
Iteration 170112: with minibatch training loss = 0.00314 and accuracy of 1
Epoch 41, Train loss: 0.0326 and Train accuracy of 0.991, Test loss: 0.153 and Test accuracy of 0.967
Iteration 170240: with minibatch training loss = 0.00193 and accuracy of 1
Iteration 170368: with minibatch training loss = 0.00669 and accuracy of 1
Iteration 170496: with minibatch training loss = 0.0273 and accuracy of 0.98
Iteration 170624: with minibatch training loss = 0.0817 and accuracy of 0.97
Iteration 170752: with minibatch training loss = 0.0181 and accuracy of 1
Iteration 170880: with minibatch training loss = 0.0134 and accuracy of 0.99
Iteration 171008: with minibatch training loss = 0.0169 and accuracy of 0.99
Iteration 171136: with minibatch training loss = 0.0607 and accuracy of 0.99
Iteration 171264: with minibatch training loss = 0.0436 and accuracy of 0.98
Iteration 171392: with minibatch training loss = 0.00482 and accuracy of 1
Iteration 171520: with minibatch training loss = 0.0328 and accuracy of 0.99
Iteration 171648: with minibatch training loss = 0.0333 and accuracy of 0.98
Iteration 171776: with minibatch training loss = 0.00297 and accuracy of 1
Iteration 171904: with minibatch training loss = 0.0187 and accuracy of 0.99
Iteration 172032: with minibatch training loss = 0.0268 and accuracy of 0.99
Iteration 172160: with minibatch training loss = 0.0202 and accuracy of 0.99
Iteration 172288: with minibatch training loss = 0.0157 and accuracy of 0.99
Iteration 172416: with minibatch training loss = 0.0118 and accuracy of 1
Iteration 172544: with minibatch training loss = 0.0266 and accuracy of 0.99
Iteration 172672: with minibatch training loss = 0.0342 and accuracy of 0.98
Iteration 172800: with minibatch training loss = 0.00667 and accuracy of 1
Iteration 172928: with minibatch training loss = 0.0276 and accuracy of 0.99
Iteration 173056: with minibatch training loss = 0.0265 and accuracy of 0.99
Iteration 173184: with minibatch training loss = 0.0437 and accuracy of 0.99
Iteration 173312: with minibatch training loss = 0.0103 and accuracy of 1
Iteration 173440: with minibatch training loss = 0.0315 and accuracy of 0.98
Iteration 173568: with minibatch training loss = 0.0569 and accuracy of 0.98
Iteration 173696: with minibatch training loss = 0.0103 and accuracy of 1
Iteration 173824: with minibatch training loss = 0.00805 and accuracy of 1
Iteration 173952: with minibatch training loss = 0.0206 and accuracy of 0.99
Iteration 174080: with minibatch training loss = 0.00436 and accuracy of 1
Iteration 174208: with minibatch training loss = 0.0614 and accuracy of 0.99
Epoch 42, Train loss: 0.0314 and Train accuracy of 0.992, Test loss: 0.153 and Test accuracy of 0.967
Iteration 174336: with minibatch training loss = 0.0737 and accuracy of 0.98
Iteration 174464: with minibatch training loss = 0.0161 and accuracy of 1
Iteration 174592: with minibatch training loss = 0.0303 and accuracy of 0.99
Iteration 174720: with minibatch training loss = 0.0339 and accuracy of 0.98
Iteration 174848: with minibatch training loss = 0.0453 and accuracy of 0.98
Iteration 174976: with minibatch training loss = 0.00431 and accuracy of 1
Iteration 175104: with minibatch training loss = 0.00616 and accuracy of 1
Iteration 175232: with minibatch training loss = 0.00539 and accuracy of 1
Iteration 175360: with minibatch training loss = 0.0673 and accuracy of 0.98
Iteration 175488: with minibatch training loss = 0.0487 and accuracy of 0.98
Iteration 175616: with minibatch training loss = 0.028 and accuracy of 0.99
Iteration 175744: with minibatch training loss = 0.0114 and accuracy of 0.99
Iteration 175872: with minibatch training loss = 0.0302 and accuracy of 0.98
Iteration 176000: with minibatch training loss = 0.0848 and accuracy of 0.98
Iteration 176128: with minibatch training loss = 0.0252 and accuracy of 0.98
Iteration 176256: with minibatch training loss = 0.0484 and accuracy of 0.98
Iteration 176384: with minibatch training loss = 0.0291 and accuracy of 0.98
Iteration 176512: with minibatch training loss = 0.0702 and accuracy of 0.99
Iteration 176640: with minibatch training loss = 0.0066 and accuracy of 1
Iteration 176768: with minibatch training loss = 0.0264 and accuracy of 0.99
Iteration 176896: with minibatch training loss = 0.0356 and accuracy of 0.98
Iteration 177024: with minibatch training loss = 0.0326 and accuracy of 0.98
Iteration 177152: with minibatch training loss = 0.00297 and accuracy of 1
Iteration 177280: with minibatch training loss = 0.0582 and accuracy of 0.98
Iteration 177408: with minibatch training loss = 0.0341 and accuracy of 0.99
Iteration 177536: with minibatch training loss = 0.0909 and accuracy of 0.98
Iteration 177664: with minibatch training loss = 0.0297 and accuracy of 0.99
Iteration 177792: with minibatch training loss = 0.0103 and accuracy of 1
Iteration 177920: with minibatch training loss = 0.0053 and accuracy of 1
Iteration 178048: with minibatch training loss = 0.053 and accuracy of 0.98
Iteration 178176: with minibatch training loss = 0.0947 and accuracy of 0.98
Iteration 178304: with minibatch training loss = 0.019 and accuracy of 0.99
Iteration 178432: with minibatch training loss = 0.0273 and accuracy of 0.99
Epoch 43, Train loss: 0.0311 and Train accuracy of 0.992, Test loss: 0.154 and Test accuracy of 0.967
Iteration 178560: with minibatch training loss = 0.0401 and accuracy of 0.98
Iteration 178688: with minibatch training loss = 0.0863 and accuracy of 0.99
Iteration 178816: with minibatch training loss = 0.0472 and accuracy of 0.98
Iteration 178944: with minibatch training loss = 0.00376 and accuracy of 1
Iteration 179072: with minibatch training loss = 0.0482 and accuracy of 0.99
Iteration 179200: with minibatch training loss = 0.0152 and accuracy of 0.99
Iteration 179328: with minibatch training loss = 0.02 and accuracy of 0.99
Iteration 179456: with minibatch training loss = 0.0339 and accuracy of 0.98
Iteration 179584: with minibatch training loss = 0.0342 and accuracy of 0.98
Iteration 179712: with minibatch training loss = 0.0155 and accuracy of 0.99
Iteration 179840: with minibatch training loss = 0.0269 and accuracy of 0.99
Iteration 179968: with minibatch training loss = 0.00366 and accuracy of 1
Iteration 180096: with minibatch training loss = 0.026 and accuracy of 0.99
Iteration 180224: with minibatch training loss = 0.0986 and accuracy of 0.97
Iteration 180352: with minibatch training loss = 0.0312 and accuracy of 0.98
Iteration 180480: with minibatch training loss = 0.0517 and accuracy of 0.98
Iteration 180608: with minibatch training loss = 0.0918 and accuracy of 0.98
Iteration 180736: with minibatch training loss = 0.00491 and accuracy of 1
Iteration 180864: with minibatch training loss = 0.094 and accuracy of 0.98
Iteration 180992: with minibatch training loss = 0.00311 and accuracy of 1
Iteration 181120: with minibatch training loss = 0.0169 and accuracy of 0.99
Iteration 181248: with minibatch training loss = 0.00939 and accuracy of 0.99
Iteration 181376: with minibatch training loss = 0.00726 and accuracy of 1
Iteration 181504: with minibatch training loss = 0.018 and accuracy of 0.99
Iteration 181632: with minibatch training loss = 0.0485 and accuracy of 0.98
Iteration 181760: with minibatch training loss = 0.0846 and accuracy of 0.98
Iteration 181888: with minibatch training loss = 0.0204 and accuracy of 0.99
Iteration 182016: with minibatch training loss = 0.0369 and accuracy of 0.99
Iteration 182144: with minibatch training loss = 0.0223 and accuracy of 0.99
Iteration 182272: with minibatch training loss = 0.139 and accuracy of 0.98
Iteration 182400: with minibatch training loss = 0.111 and accuracy of 0.98
Iteration 182528: with minibatch training loss = 0.06 and accuracy of 0.98
Epoch 44, Train loss: 0.0305 and Train accuracy of 0.992, Test loss: 0.155 and Test accuracy of 0.967
Iteration 182656: with minibatch training loss = 0.0801 and accuracy of 0.99
Iteration 182784: with minibatch training loss = 0.0303 and accuracy of 0.99
Iteration 182912: with minibatch training loss = 0.0669 and accuracy of 0.97
Iteration 183040: with minibatch training loss = 0.00359 and accuracy of 1
Iteration 183168: with minibatch training loss = 0.0246 and accuracy of 0.99
Iteration 183296: with minibatch training loss = 0.00493 and accuracy of 1
Iteration 183424: with minibatch training loss = 0.00652 and accuracy of 1
Iteration 183552: with minibatch training loss = 0.021 and accuracy of 0.98
Iteration 183680: with minibatch training loss = 0.0276 and accuracy of 0.99
Iteration 183808: with minibatch training loss = 0.0157 and accuracy of 0.99
Iteration 183936: with minibatch training loss = 0.0137 and accuracy of 1
Iteration 184064: with minibatch training loss = 0.011 and accuracy of 0.99
Iteration 184192: with minibatch training loss = 0.00709 and accuracy of 1
Iteration 184320: with minibatch training loss = 0.00263 and accuracy of 1
Iteration 184448: with minibatch training loss = 0.0516 and accuracy of 0.99
Iteration 184576: with minibatch training loss = 0.0249 and accuracy of 0.99
Iteration 184704: with minibatch training loss = 0.019 and accuracy of 0.99
Iteration 184832: with minibatch training loss = 0.0176 and accuracy of 0.98
Iteration 184960: with minibatch training loss = 0.0689 and accuracy of 0.98
Iteration 185088: with minibatch training loss = 0.0222 and accuracy of 0.99
Iteration 185216: with minibatch training loss = 0.0115 and accuracy of 1
Iteration 185344: with minibatch training loss = 0.0326 and accuracy of 0.98
Iteration 185472: with minibatch training loss = 0.0472 and accuracy of 0.99
Iteration 185600: with minibatch training loss = 0.114 and accuracy of 0.99
Iteration 185728: with minibatch training loss = 0.00884 and accuracy of 1
Iteration 185856: with minibatch training loss = 0.0192 and accuracy of 1
Iteration 185984: with minibatch training loss = 0.00314 and accuracy of 1
Iteration 186112: with minibatch training loss = 0.0488 and accuracy of 0.98
Iteration 186240: with minibatch training loss = 0.00726 and accuracy of 1
Iteration 186368: with minibatch training loss = 0.0165 and accuracy of 1
Iteration 186496: with minibatch training loss = 0.00609 and accuracy of 1
Iteration 186624: with minibatch training loss = 0.0815 and accuracy of 0.98
Epoch 45, Train loss: 0.0306 and Train accuracy of 0.992, Test loss: 0.154 and Test accuracy of 0.966
Iteration 186752: with minibatch training loss = 0.0249 and accuracy of 0.99
Iteration 186880: with minibatch training loss = 0.095 and accuracy of 0.97
Iteration 187008: with minibatch training loss = 0.0154 and accuracy of 0.99
Iteration 187136: with minibatch training loss = 0.066 and accuracy of 0.98
Iteration 187264: with minibatch training loss = 0.0257 and accuracy of 0.98
Iteration 187392: with minibatch training loss = 0.0259 and accuracy of 0.99
Iteration 187520: with minibatch training loss = 0.00871 and accuracy of 1
Iteration 187648: with minibatch training loss = 0.00664 and accuracy of 1
Iteration 187776: with minibatch training loss = 0.0117 and accuracy of 1
Iteration 187904: with minibatch training loss = 0.0332 and accuracy of 0.99
Iteration 188032: with minibatch training loss = 0.00421 and accuracy of 1
Iteration 188160: with minibatch training loss = 0.0855 and accuracy of 0.98
Iteration 188288: with minibatch training loss = 0.0159 and accuracy of 0.99
Iteration 188416: with minibatch training loss = 0.0159 and accuracy of 0.99
Iteration 188544: with minibatch training loss = 0.0243 and accuracy of 0.99
Iteration 188672: with minibatch training loss = 0.034 and accuracy of 0.98
Iteration 188800: with minibatch training loss = 0.145 and accuracy of 0.98
Iteration 188928: with minibatch training loss = 0.0271 and accuracy of 0.99
Iteration 189056: with minibatch training loss = 0.109 and accuracy of 0.99
Iteration 189184: with minibatch training loss = 0.0192 and accuracy of 0.99
Iteration 189312: with minibatch training loss = 0.0148 and accuracy of 1
Iteration 189440: with minibatch training loss = 0.0221 and accuracy of 0.98
Iteration 189568: with minibatch training loss = 0.0449 and accuracy of 0.98
Iteration 189696: with minibatch training loss = 0.0486 and accuracy of 0.98
Iteration 189824: with minibatch training loss = 0.00366 and accuracy of 1
Iteration 189952: with minibatch training loss = 0.0196 and accuracy of 0.98
Iteration 190080: with minibatch training loss = 0.0109 and accuracy of 1
Iteration 190208: with minibatch training loss = 0.0193 and accuracy of 0.99
Iteration 190336: with minibatch training loss = 0.0353 and accuracy of 0.99
Iteration 190464: with minibatch training loss = 0.00694 and accuracy of 1
Iteration 190592: with minibatch training loss = 0.00163 and accuracy of 1
Iteration 190720: with minibatch training loss = 0.12 and accuracy of 0.98
Iteration 190848: with minibatch training loss = 0.0726 and accuracy of 0.98
Epoch 46, Train loss: 0.0299 and Train accuracy of 0.992, Test loss: 0.157 and Test accuracy of 0.968
Iteration 190976: with minibatch training loss = 0.0862 and accuracy of 0.98
Iteration 191104: with minibatch training loss = 0.00944 and accuracy of 1
Iteration 191232: with minibatch training loss = 0.0237 and accuracy of 0.99
Iteration 191360: with minibatch training loss = 0.0198 and accuracy of 0.99
Iteration 191488: with minibatch training loss = 0.0131 and accuracy of 0.99
Iteration 191616: with minibatch training loss = 0.0226 and accuracy of 0.99
Iteration 191744: with minibatch training loss = 0.00537 and accuracy of 1
Iteration 191872: with minibatch training loss = 0.0169 and accuracy of 0.99
Iteration 192000: with minibatch training loss = 0.00371 and accuracy of 1
Iteration 192128: with minibatch training loss = 0.00264 and accuracy of 1
Iteration 192256: with minibatch training loss = 0.00723 and accuracy of 1
Iteration 192384: with minibatch training loss = 0.00615 and accuracy of 1
Iteration 192512: with minibatch training loss = 0.104 and accuracy of 0.98
Iteration 192640: with minibatch training loss = 0.00796 and accuracy of 1
Iteration 192768: with minibatch training loss = 0.0105 and accuracy of 1
Iteration 192896: with minibatch training loss = 0.00843 and accuracy of 0.99
Iteration 193024: with minibatch training loss = 0.0154 and accuracy of 1
Iteration 193152: with minibatch training loss = 0.0318 and accuracy of 0.99
Iteration 193280: with minibatch training loss = 0.0899 and accuracy of 0.99
Iteration 193408: with minibatch training loss = 0.0037 and accuracy of 1
Iteration 193536: with minibatch training loss = 0.00815 and accuracy of 1
Iteration 193664: with minibatch training loss = 0.136 and accuracy of 0.97
Iteration 193792: with minibatch training loss = 0.015 and accuracy of 0.99
Iteration 193920: with minibatch training loss = 0.00261 and accuracy of 1
Iteration 194048: with minibatch training loss = 0.0356 and accuracy of 0.98
Iteration 194176: with minibatch training loss = 0.0117 and accuracy of 0.99
Iteration 194304: with minibatch training loss = 0.0882 and accuracy of 0.99
Iteration 194432: with minibatch training loss = 0.0265 and accuracy of 0.99
Iteration 194560: with minibatch training loss = 0.00371 and accuracy of 1
Iteration 194688: with minibatch training loss = 0.0314 and accuracy of 0.98
Iteration 194816: with minibatch training loss = 0.0789 and accuracy of 0.99
Iteration 194944: with minibatch training loss = 0.0518 and accuracy of 0.98
Epoch 47, Train loss: 0.0299 and Train accuracy of 0.992, Test loss: 0.154 and Test accuracy of 0.967
Iteration 195072: with minibatch training loss = 0.0288 and accuracy of 0.99
Iteration 195200: with minibatch training loss = 0.0466 and accuracy of 0.98
Iteration 195328: with minibatch training loss = 0.00127 and accuracy of 1
Iteration 195456: with minibatch training loss = 0.0847 and accuracy of 0.98
Iteration 195584: with minibatch training loss = 0.0986 and accuracy of 0.99
Iteration 195712: with minibatch training loss = 0.00271 and accuracy of 1
Iteration 195840: with minibatch training loss = 0.0708 and accuracy of 0.97
Iteration 195968: with minibatch training loss = 0.03 and accuracy of 0.98
Iteration 196096: with minibatch training loss = 0.0096 and accuracy of 1
Iteration 196224: with minibatch training loss = 0.00602 and accuracy of 1
Iteration 196352: with minibatch training loss = 0.0145 and accuracy of 0.99
Iteration 196480: with minibatch training loss = 0.0166 and accuracy of 0.99
Iteration 196608: with minibatch training loss = 0.0139 and accuracy of 0.99
Iteration 196736: with minibatch training loss = 0.0658 and accuracy of 0.98
Iteration 196864: with minibatch training loss = 0.00553 and accuracy of 1
Iteration 196992: with minibatch training loss = 0.01 and accuracy of 1
Iteration 197120: with minibatch training loss = 0.0299 and accuracy of 0.99
Iteration 197248: with minibatch training loss = 0.0401 and accuracy of 0.99
Iteration 197376: with minibatch training loss = 0.0101 and accuracy of 1
Iteration 197504: with minibatch training loss = 0.0339 and accuracy of 0.98
Iteration 197632: with minibatch training loss = 0.00365 and accuracy of 1
Iteration 197760: with minibatch training loss = 0.00831 and accuracy of 1
Iteration 197888: with minibatch training loss = 0.0907 and accuracy of 0.98
Iteration 198016: with minibatch training loss = 0.00282 and accuracy of 1
Iteration 198144: with minibatch training loss = 0.0781 and accuracy of 0.98
Iteration 198272: with minibatch training loss = 0.00361 and accuracy of 1
Iteration 198400: with minibatch training loss = 0.0365 and accuracy of 0.98
Iteration 198528: with minibatch training loss = 0.00462 and accuracy of 1
Iteration 198656: with minibatch training loss = 0.0333 and accuracy of 0.99
Iteration 198784: with minibatch training loss = 0.0243 and accuracy of 0.99
Iteration 198912: with minibatch training loss = 0.0183 and accuracy of 0.99
Iteration 199040: with minibatch training loss = 0.0272 and accuracy of 0.99
Iteration 199168: with minibatch training loss = 0.0181 and accuracy of 0.99
Epoch 48, Train loss: 0.0288 and Train accuracy of 0.992, Test loss: 0.155 and Test accuracy of 0.968
Iteration 199296: with minibatch training loss = 0.0196 and accuracy of 0.99
Iteration 199424: with minibatch training loss = 0.00217 and accuracy of 1
Iteration 199552: with minibatch training loss = 0.00823 and accuracy of 1
Iteration 199680: with minibatch training loss = 0.0477 and accuracy of 0.99
Iteration 199808: with minibatch training loss = 0.0244 and accuracy of 0.98
Iteration 199936: with minibatch training loss = 0.0616 and accuracy of 0.97
Iteration 200064: with minibatch training loss = 0.00519 and accuracy of 1
Iteration 200192: with minibatch training loss = 0.0184 and accuracy of 1
Iteration 200320: with minibatch training loss = 0.00779 and accuracy of 1
Iteration 200448: with minibatch training loss = 0.023 and accuracy of 0.98
Iteration 200576: with minibatch training loss = 0.0184 and accuracy of 0.99
Iteration 200704: with minibatch training loss = 0.0492 and accuracy of 0.98
Iteration 200832: with minibatch training loss = 0.00649 and accuracy of 1
Iteration 200960: with minibatch training loss = 0.00258 and accuracy of 1
Iteration 201088: with minibatch training loss = 0.0156 and accuracy of 0.99
Iteration 201216: with minibatch training loss = 0.0993 and accuracy of 0.98
Iteration 201344: with minibatch training loss = 0.00523 and accuracy of 1
Iteration 201472: with minibatch training loss = 0.00522 and accuracy of 1
Iteration 201600: with minibatch training loss = 0.0104 and accuracy of 0.99
Iteration 201728: with minibatch training loss = 0.0207 and accuracy of 0.99
Iteration 201856: with minibatch training loss = 0.0417 and accuracy of 0.99
Iteration 201984: with minibatch training loss = 0.019 and accuracy of 0.99
Iteration 202112: with minibatch training loss = 0.00476 and accuracy of 1
Iteration 202240: with minibatch training loss = 0.0283 and accuracy of 0.99
Iteration 202368: with minibatch training loss = 0.0882 and accuracy of 0.98
Iteration 202496: with minibatch training loss = 0.00298 and accuracy of 1
Iteration 202624: with minibatch training loss = 0.00591 and accuracy of 1
Iteration 202752: with minibatch training loss = 0.0155 and accuracy of 0.99
Iteration 202880: with minibatch training loss = 0.0379 and accuracy of 0.99
Iteration 203008: with minibatch training loss = 0.00935 and accuracy of 1
Iteration 203136: with minibatch training loss = 0.0452 and accuracy of 0.98
Iteration 203264: with minibatch training loss = 0.0362 and accuracy of 0.98
Epoch 49, Train loss: 0.0283 and Train accuracy of 0.993, Test loss: 0.159 and Test accuracy of 0.966
Iteration 203392: with minibatch training loss = 0.0117 and accuracy of 0.99
Iteration 203520: with minibatch training loss = 0.0595 and accuracy of 0.99
Iteration 203648: with minibatch training loss = 0.0103 and accuracy of 0.99
Iteration 203776: with minibatch training loss = 0.0178 and accuracy of 0.99
Iteration 203904: with minibatch training loss = 0.04 and accuracy of 0.98
Iteration 204032: with minibatch training loss = 0.0292 and accuracy of 0.98
Iteration 204160: with minibatch training loss = 0.00158 and accuracy of 1
Iteration 204288: with minibatch training loss = 0.00648 and accuracy of 1
Iteration 204416: with minibatch training loss = 0.0279 and accuracy of 0.99
Iteration 204544: with minibatch training loss = 0.026 and accuracy of 0.99
Iteration 204672: with minibatch training loss = 0.0268 and accuracy of 0.98
Iteration 204800: with minibatch training loss = 0.0356 and accuracy of 0.99
Iteration 204928: with minibatch training loss = 0.00276 and accuracy of 1
Iteration 205056: with minibatch training loss = 0.00922 and accuracy of 0.99
Iteration 205184: with minibatch training loss = 0.00528 and accuracy of 1
Iteration 205312: with minibatch training loss = 0.0102 and accuracy of 1
Iteration 205440: with minibatch training loss = 0.00195 and accuracy of 1
Iteration 205568: with minibatch training loss = 0.00709 and accuracy of 1
Iteration 205696: with minibatch training loss = 0.00318 and accuracy of 1
Iteration 205824: with minibatch training loss = 0.0167 and accuracy of 0.99
Iteration 205952: with minibatch training loss = 0.00405 and accuracy of 1
Iteration 206080: with minibatch training loss = 0.0151 and accuracy of 0.99
Iteration 206208: with minibatch training loss = 0.00331 and accuracy of 1
Iteration 206336: with minibatch training loss = 0.0129 and accuracy of 0.99
Iteration 206464: with minibatch training loss = 0.0505 and accuracy of 0.99
Iteration 206592: with minibatch training loss = 0.00386 and accuracy of 1
Iteration 206720: with minibatch training loss = 0.00252 and accuracy of 1
Iteration 206848: with minibatch training loss = 0.00528 and accuracy of 1
Iteration 206976: with minibatch training loss = 0.135 and accuracy of 0.98
Iteration 207104: with minibatch training loss = 0.00378 and accuracy of 1
Iteration 207232: with minibatch training loss = 0.0219 and accuracy of 0.98
Iteration 207360: with minibatch training loss = 0.00364 and accuracy of 1
Iteration 207488: with minibatch training loss = 0.00433 and accuracy of 1
Epoch 50, Train loss: 0.0281 and Train accuracy of 0.993, Test loss: 0.157 and Test accuracy of 0.968
Iteration 207616: with minibatch training loss = 0.0204 and accuracy of 0.99
Iteration 207744: with minibatch training loss = 0.0143 and accuracy of 0.99
Iteration 207872: with minibatch training loss = 0.00668 and accuracy of 1
Iteration 208000: with minibatch training loss = 0.0644 and accuracy of 0.99
Iteration 208128: with minibatch training loss = 0.00637 and accuracy of 1
Iteration 208256: with minibatch training loss = 0.0154 and accuracy of 0.99
Iteration 208384: with minibatch training loss = 0.0057 and accuracy of 1
Iteration 208512: with minibatch training loss = 0.0178 and accuracy of 0.99
Iteration 208640: with minibatch training loss = 0.0217 and accuracy of 0.99
Iteration 208768: with minibatch training loss = 0.00335 and accuracy of 1
Iteration 208896: with minibatch training loss = 0.0221 and accuracy of 0.99
Iteration 209024: with minibatch training loss = 0.0356 and accuracy of 0.98
Iteration 209152: with minibatch training loss = 0.00493 and accuracy of 1
Iteration 209280: with minibatch training loss = 0.0576 and accuracy of 0.98
Iteration 209408: with minibatch training loss = 0.0357 and accuracy of 0.99
Iteration 209536: with minibatch training loss = 0.00793 and accuracy of 1
Iteration 209664: with minibatch training loss = 0.0332 and accuracy of 0.99
Iteration 209792: with minibatch training loss = 0.014 and accuracy of 0.99
Iteration 209920: with minibatch training loss = 0.0626 and accuracy of 0.98
Iteration 210048: with minibatch training loss = 0.00277 and accuracy of 1
Iteration 210176: with minibatch training loss = 0.0159 and accuracy of 0.99
Iteration 210304: with minibatch training loss = 0.0173 and accuracy of 0.99
Iteration 210432: with minibatch training loss = 0.00689 and accuracy of 1
Iteration 210560: with minibatch training loss = 0.00704 and accuracy of 1
Iteration 210688: with minibatch training loss = 0.00558 and accuracy of 1
Iteration 210816: with minibatch training loss = 0.0325 and accuracy of 0.99
Iteration 210944: with minibatch training loss = 0.0237 and accuracy of 0.99
Iteration 211072: with minibatch training loss = 0.0262 and accuracy of 0.99
Iteration 211200: with minibatch training loss = 0.0291 and accuracy of 0.98
Iteration 211328: with minibatch training loss = 0.00224 and accuracy of 1
Iteration 211456: with minibatch training loss = 0.00665 and accuracy of 1
Iteration 211584: with minibatch training loss = 0.0366 and accuracy of 0.98
Epoch 51, Train loss: 0.0276 and Train accuracy of 0.993, Test loss: 0.157 and Test accuracy of 0.967
Iteration 211712: with minibatch training loss = 0.015 and accuracy of 0.99
Iteration 211840: with minibatch training loss = 0.0561 and accuracy of 0.99
Iteration 211968: with minibatch training loss = 0.0207 and accuracy of 0.99
Iteration 212096: with minibatch training loss = 0.00978 and accuracy of 0.99
Iteration 212224: with minibatch training loss = 0.00698 and accuracy of 1
Iteration 212352: with minibatch training loss = 0.00626 and accuracy of 1
Iteration 212480: with minibatch training loss = 0.0182 and accuracy of 0.98
Iteration 212608: with minibatch training loss = 0.0048 and accuracy of 1
Iteration 212736: with minibatch training loss = 0.046 and accuracy of 0.99
Iteration 212864: with minibatch training loss = 0.00349 and accuracy of 1
Iteration 212992: with minibatch training loss = 0.0076 and accuracy of 1
Iteration 213120: with minibatch training loss = 0.0279 and accuracy of 0.99
Iteration 213248: with minibatch training loss = 0.0815 and accuracy of 0.98
Iteration 213376: with minibatch training loss = 0.0123 and accuracy of 0.99
Iteration 213504: with minibatch training loss = 0.00401 and accuracy of 1
Iteration 213632: with minibatch training loss = 0.00216 and accuracy of 1
Iteration 213760: with minibatch training loss = 0.0343 and accuracy of 0.99
Iteration 213888: with minibatch training loss = 0.00369 and accuracy of 1
Iteration 214016: with minibatch training loss = 0.0173 and accuracy of 0.99
Iteration 214144: with minibatch training loss = 0.00793 and accuracy of 1
Iteration 214272: with minibatch training loss = 0.0139 and accuracy of 0.99
Iteration 214400: with minibatch training loss = 0.0709 and accuracy of 0.97
Iteration 214528: with minibatch training loss = 0.00901 and accuracy of 1
Iteration 214656: with minibatch training loss = 0.0306 and accuracy of 0.98
Iteration 214784: with minibatch training loss = 0.0114 and accuracy of 1
Iteration 214912: with minibatch training loss = 0.0173 and accuracy of 0.99
Iteration 215040: with minibatch training loss = 0.0213 and accuracy of 0.99
Iteration 215168: with minibatch training loss = 0.0105 and accuracy of 1
Iteration 215296: with minibatch training loss = 0.00156 and accuracy of 1
Iteration 215424: with minibatch training loss = 0.00797 and accuracy of 1
Iteration 215552: with minibatch training loss = 0.0135 and accuracy of 0.99
Iteration 215680: with minibatch training loss = 0.00367 and accuracy of 1
Epoch 52, Train loss: 0.0275 and Train accuracy of 0.993, Test loss: 0.158 and Test accuracy of 0.967
Iteration 215808: with minibatch training loss = 0.0278 and accuracy of 0.99
Iteration 215936: with minibatch training loss = 0.00245 and accuracy of 1
Iteration 216064: with minibatch training loss = 0.00251 and accuracy of 1
Iteration 216192: with minibatch training loss = 0.00191 and accuracy of 1
Iteration 216320: with minibatch training loss = 0.0476 and accuracy of 0.99
Iteration 216448: with minibatch training loss = 0.00577 and accuracy of 1
Iteration 216576: with minibatch training loss = 0.038 and accuracy of 0.99
Iteration 216704: with minibatch training loss = 0.0281 and accuracy of 0.99
Iteration 216832: with minibatch training loss = 0.00529 and accuracy of 1
Iteration 216960: with minibatch training loss = 0.00894 and accuracy of 0.99
Iteration 217088: with minibatch training loss = 0.00129 and accuracy of 1
Iteration 217216: with minibatch training loss = 0.0276 and accuracy of 0.99
Iteration 217344: with minibatch training loss = 0.0509 and accuracy of 0.98
Iteration 217472: with minibatch training loss = 0.00365 and accuracy of 1
Iteration 217600: with minibatch training loss = 0.00313 and accuracy of 1
Iteration 217728: with minibatch training loss = 0.0139 and accuracy of 0.99
Iteration 217856: with minibatch training loss = 0.018 and accuracy of 1
Iteration 217984: with minibatch training loss = 0.00991 and accuracy of 1
Iteration 218112: with minibatch training loss = 0.025 and accuracy of 0.98
Iteration 218240: with minibatch training loss = 0.0138 and accuracy of 1
Iteration 218368: with minibatch training loss = 0.0113 and accuracy of 0.99
Iteration 218496: with minibatch training loss = 0.00529 and accuracy of 1
Iteration 218624: with minibatch training loss = 0.0278 and accuracy of 0.98
Iteration 218752: with minibatch training loss = 0.013 and accuracy of 1
Iteration 218880: with minibatch training loss = 0.00388 and accuracy of 1
Iteration 219008: with minibatch training loss = 0.0589 and accuracy of 0.99
Iteration 219136: with minibatch training loss = 0.00233 and accuracy of 1
Iteration 219264: with minibatch training loss = 0.0599 and accuracy of 0.98
Iteration 219392: with minibatch training loss = 0.00353 and accuracy of 1
Iteration 219520: with minibatch training loss = 0.00472 and accuracy of 1
Iteration 219648: with minibatch training loss = 0.0273 and accuracy of 0.98
Iteration 219776: with minibatch training loss = 0.00412 and accuracy of 1
Iteration 219904: with minibatch training loss = 0.0137 and accuracy of 0.99
Epoch 53, Train loss: 0.0267 and Train accuracy of 0.993, Test loss: 0.157 and Test accuracy of 0.967
Iteration 220032: with minibatch training loss = 0.00443 and accuracy of 1
Iteration 220160: with minibatch training loss = 0.0251 and accuracy of 0.99
Iteration 220288: with minibatch training loss = 0.0707 and accuracy of 0.97
Iteration 220416: with minibatch training loss = 0.0556 and accuracy of 0.98
Iteration 220544: with minibatch training loss = 0.0112 and accuracy of 1
Iteration 220672: with minibatch training loss = 0.0595 and accuracy of 0.98
Iteration 220800: with minibatch training loss = 0.00676 and accuracy of 1
Iteration 220928: with minibatch training loss = 0.0551 and accuracy of 0.98
Iteration 221056: with minibatch training loss = 0.0133 and accuracy of 0.99
Iteration 221184: with minibatch training loss = 0.0298 and accuracy of 0.98
Iteration 221312: with minibatch training loss = 0.017 and accuracy of 0.99
Iteration 221440: with minibatch training loss = 0.00227 and accuracy of 1
Iteration 221568: with minibatch training loss = 0.0123 and accuracy of 1
Iteration 221696: with minibatch training loss = 0.0265 and accuracy of 0.99
Iteration 221824: with minibatch training loss = 0.0393 and accuracy of 0.99
Iteration 221952: with minibatch training loss = 0.0493 and accuracy of 0.98
Iteration 222080: with minibatch training loss = 0.0634 and accuracy of 0.98
Iteration 222208: with minibatch training loss = 0.053 and accuracy of 0.98
Iteration 222336: with minibatch training loss = 0.0231 and accuracy of 0.99
Iteration 222464: with minibatch training loss = 0.0379 and accuracy of 0.99
Iteration 222592: with minibatch training loss = 0.12 and accuracy of 0.98
Iteration 222720: with minibatch training loss = 0.00144 and accuracy of 1
Iteration 222848: with minibatch training loss = 0.0134 and accuracy of 1
Iteration 222976: with minibatch training loss = 0.0629 and accuracy of 0.99
Iteration 223104: with minibatch training loss = 0.0397 and accuracy of 0.99
Iteration 223232: with minibatch training loss = 0.00288 and accuracy of 1
Iteration 223360: with minibatch training loss = 0.015 and accuracy of 0.99
Iteration 223488: with minibatch training loss = 0.00353 and accuracy of 1
Iteration 223616: with minibatch training loss = 0.0609 and accuracy of 0.99
Iteration 223744: with minibatch training loss = 0.0236 and accuracy of 0.99
Iteration 223872: with minibatch training loss = 0.00199 and accuracy of 1
Iteration 224000: with minibatch training loss = 0.0079 and accuracy of 0.99
Epoch 54, Train loss: 0.0264 and Train accuracy of 0.993, Test loss: 0.163 and Test accuracy of 0.967
Iteration 224128: with minibatch training loss = 0.0118 and accuracy of 0.99
Iteration 224256: with minibatch training loss = 0.00289 and accuracy of 1
Iteration 224384: with minibatch training loss = 0.042 and accuracy of 0.99
Iteration 224512: with minibatch training loss = 0.00504 and accuracy of 1
Iteration 224640: with minibatch training loss = 0.00185 and accuracy of 1
Iteration 224768: with minibatch training loss = 0.011 and accuracy of 1
Iteration 224896: with minibatch training loss = 0.022 and accuracy of 0.99
Iteration 225024: with minibatch training loss = 0.00543 and accuracy of 1
Iteration 225152: with minibatch training loss = 0.0171 and accuracy of 0.99
Iteration 225280: with minibatch training loss = 0.00834 and accuracy of 0.99
Iteration 225408: with minibatch training loss = 0.00708 and accuracy of 1
Iteration 225536: with minibatch training loss = 0.0598 and accuracy of 0.98
Iteration 225664: with minibatch training loss = 0.0228 and accuracy of 0.99
Iteration 225792: with minibatch training loss = 0.00798 and accuracy of 1
Iteration 225920: with minibatch training loss = 0.00217 and accuracy of 1
Iteration 226048: with minibatch training loss = 0.00288 and accuracy of 1
Iteration 226176: with minibatch training loss = 0.0117 and accuracy of 0.99
Iteration 226304: with minibatch training loss = 0.023 and accuracy of 0.99
Iteration 226432: with minibatch training loss = 0.00824 and accuracy of 1
Iteration 226560: with minibatch training loss = 0.0824 and accuracy of 0.99
Iteration 226688: with minibatch training loss = 0.00253 and accuracy of 1
Iteration 226816: with minibatch training loss = 0.00703 and accuracy of 1
Iteration 226944: with minibatch training loss = 0.00248 and accuracy of 1
Iteration 227072: with minibatch training loss = 0.00547 and accuracy of 1
Iteration 227200: with minibatch training loss = 0.016 and accuracy of 0.99
Iteration 227328: with minibatch training loss = 0.0875 and accuracy of 0.98
Iteration 227456: with minibatch training loss = 0.0317 and accuracy of 0.98
Iteration 227584: with minibatch training loss = 0.00832 and accuracy of 0.99
Iteration 227712: with minibatch training loss = 0.00235 and accuracy of 1
Iteration 227840: with minibatch training loss = 0.0311 and accuracy of 0.99
Iteration 227968: with minibatch training loss = 0.0215 and accuracy of 0.99
Iteration 228096: with minibatch training loss = 0.00716 and accuracy of 1
Iteration 228224: with minibatch training loss = 0.00802 and accuracy of 1
Epoch 55, Train loss: 0.0265 and Train accuracy of 0.993, Test loss: 0.161 and Test accuracy of 0.967
Iteration 228352: with minibatch training loss = 0.0636 and accuracy of 0.98
Iteration 228480: with minibatch training loss = 0.00479 and accuracy of 1
Iteration 228608: with minibatch training loss = 0.0606 and accuracy of 0.99
Iteration 228736: with minibatch training loss = 0.00199 and accuracy of 1
Iteration 228864: with minibatch training loss = 0.0111 and accuracy of 0.99
Iteration 228992: with minibatch training loss = 0.0723 and accuracy of 0.98
Iteration 229120: with minibatch training loss = 0.0112 and accuracy of 1
Iteration 229248: with minibatch training loss = 0.0169 and accuracy of 0.99
Iteration 229376: with minibatch training loss = 0.00225 and accuracy of 1
Iteration 229504: with minibatch training loss = 0.035 and accuracy of 0.98
Iteration 229632: with minibatch training loss = 0.0416 and accuracy of 0.99
Iteration 229760: with minibatch training loss = 0.0645 and accuracy of 0.98
Iteration 229888: with minibatch training loss = 0.0815 and accuracy of 0.99
Iteration 230016: with minibatch training loss = 0.00608 and accuracy of 1
Iteration 230144: with minibatch training loss = 0.021 and accuracy of 0.99
Iteration 230272: with minibatch training loss = 0.0285 and accuracy of 0.99
Iteration 230400: with minibatch training loss = 0.0762 and accuracy of 0.99
Iteration 230528: with minibatch training loss = 0.0451 and accuracy of 0.99
Iteration 230656: with minibatch training loss = 0.112 and accuracy of 0.98
Iteration 230784: with minibatch training loss = 0.00156 and accuracy of 1
Iteration 230912: with minibatch training loss = 0.00434 and accuracy of 1
Iteration 231040: with minibatch training loss = 0.0204 and accuracy of 0.99
Iteration 231168: with minibatch training loss = 0.0336 and accuracy of 0.99
Iteration 231296: with minibatch training loss = 0.0164 and accuracy of 0.99
Iteration 231424: with minibatch training loss = 0.00675 and accuracy of 1
Iteration 231552: with minibatch training loss = 0.00274 and accuracy of 1
Iteration 231680: with minibatch training loss = 0.106 and accuracy of 0.99
Iteration 231808: with minibatch training loss = 0.0165 and accuracy of 0.99
Iteration 231936: with minibatch training loss = 0.0454 and accuracy of 0.98
Iteration 232064: with minibatch training loss = 0.0757 and accuracy of 0.99
Iteration 232192: with minibatch training loss = 0.0706 and accuracy of 0.99
Iteration 232320: with minibatch training loss = 0.051 and accuracy of 0.98
Epoch 56, Train loss: 0.0259 and Train accuracy of 0.993, Test loss: 0.171 and Test accuracy of 0.965
Iteration 232448: with minibatch training loss = 0.00665 and accuracy of 1
Iteration 232576: with minibatch training loss = 0.108 and accuracy of 0.98
Iteration 232704: with minibatch training loss = 0.00678 and accuracy of 1
Iteration 232832: with minibatch training loss = 0.0025 and accuracy of 1
Iteration 232960: with minibatch training loss = 0.0375 and accuracy of 0.99
Iteration 233088: with minibatch training loss = 0.0125 and accuracy of 0.99
Iteration 233216: with minibatch training loss = 0.0415 and accuracy of 0.99
Iteration 233344: with minibatch training loss = 0.00532 and accuracy of 1
Iteration 233472: with minibatch training loss = 0.0201 and accuracy of 0.99
Iteration 233600: with minibatch training loss = 0.0154 and accuracy of 0.99
Iteration 233728: with minibatch training loss = 0.0125 and accuracy of 0.99
Iteration 233856: with minibatch training loss = 0.0648 and accuracy of 0.98
Iteration 233984: with minibatch training loss = 0.0822 and accuracy of 0.98
Iteration 234112: with minibatch training loss = 0.0417 and accuracy of 0.99
Iteration 234240: with minibatch training loss = 0.00206 and accuracy of 1
Iteration 234368: with minibatch training loss = 0.0353 and accuracy of 0.99
Iteration 234496: with minibatch training loss = 0.113 and accuracy of 0.98
Iteration 234624: with minibatch training loss = 0.0131 and accuracy of 0.99
Iteration 234752: with minibatch training loss = 0.00222 and accuracy of 1
Iteration 234880: with minibatch training loss = 0.0207 and accuracy of 0.99
Iteration 235008: with minibatch training loss = 0.0133 and accuracy of 1
Iteration 235136: with minibatch training loss = 0.00194 and accuracy of 1
Iteration 235264: with minibatch training loss = 0.0214 and accuracy of 0.99
Iteration 235392: with minibatch training loss = 0.0262 and accuracy of 0.98
Iteration 235520: with minibatch training loss = 0.0056 and accuracy of 1
Iteration 235648: with minibatch training loss = 0.0121 and accuracy of 0.99
Iteration 235776: with minibatch training loss = 0.0809 and accuracy of 0.98
Iteration 235904: with minibatch training loss = 0.00157 and accuracy of 1
Iteration 236032: with minibatch training loss = 0.0035 and accuracy of 1
Iteration 236160: with minibatch training loss = 0.0335 and accuracy of 0.99
Iteration 236288: with minibatch training loss = 0.0119 and accuracy of 1
Iteration 236416: with minibatch training loss = 0.0621 and accuracy of 0.99
Iteration 236544: with minibatch training loss = 0.0535 and accuracy of 0.99
Epoch 57, Train loss: 0.0253 and Train accuracy of 0.993, Test loss: 0.167 and Test accuracy of 0.966
Iteration 236672: with minibatch training loss = 0.0047 and accuracy of 1
Iteration 236800: with minibatch training loss = 0.00543 and accuracy of 1
Iteration 236928: with minibatch training loss = 0.0673 and accuracy of 0.99
Iteration 237056: with minibatch training loss = 0.0979 and accuracy of 0.99
Iteration 237184: with minibatch training loss = 0.0241 and accuracy of 0.98
Iteration 237312: with minibatch training loss = 0.00956 and accuracy of 1
Iteration 237440: with minibatch training loss = 0.0658 and accuracy of 0.99
Iteration 237568: with minibatch training loss = 0.00534 and accuracy of 1
Iteration 237696: with minibatch training loss = 0.00284 and accuracy of 1
Iteration 237824: with minibatch training loss = 0.0173 and accuracy of 0.99
Iteration 237952: with minibatch training loss = 0.00759 and accuracy of 1
Iteration 238080: with minibatch training loss = 0.0442 and accuracy of 0.99
Iteration 238208: with minibatch training loss = 0.00429 and accuracy of 1
Iteration 238336: with minibatch training loss = 0.0104 and accuracy of 1
Iteration 238464: with minibatch training loss = 0.00148 and accuracy of 1
Iteration 238592: with minibatch training loss = 0.00316 and accuracy of 1
Iteration 238720: with minibatch training loss = 0.0038 and accuracy of 1
Iteration 238848: with minibatch training loss = 0.0254 and accuracy of 0.99
Iteration 238976: with minibatch training loss = 0.0155 and accuracy of 0.99
Iteration 239104: with minibatch training loss = 0.0101 and accuracy of 1
Iteration 239232: with minibatch training loss = 0.00964 and accuracy of 0.99
Iteration 239360: with minibatch training loss = 0.0423 and accuracy of 0.99
Iteration 239488: with minibatch training loss = 0.0905 and accuracy of 0.98
Iteration 239616: with minibatch training loss = 0.00915 and accuracy of 0.99
Iteration 239744: with minibatch training loss = 0.034 and accuracy of 0.99
Iteration 239872: with minibatch training loss = 0.0146 and accuracy of 0.99
Iteration 240000: with minibatch training loss = 0.00402 and accuracy of 1
Iteration 240128: with minibatch training loss = 0.00514 and accuracy of 1
Iteration 240256: with minibatch training loss = 0.0487 and accuracy of 0.98
Iteration 240384: with minibatch training loss = 0.0125 and accuracy of 0.99
Iteration 240512: with minibatch training loss = 0.0573 and accuracy of 0.98
Iteration 240640: with minibatch training loss = 0.012 and accuracy of 0.99
Epoch 58, Train loss: 0.0255 and Train accuracy of 0.993, Test loss: 0.158 and Test accuracy of 0.968
Iteration 240768: with minibatch training loss = 0.00695 and accuracy of 1
Iteration 240896: with minibatch training loss = 0.0598 and accuracy of 0.99
Iteration 241024: with minibatch training loss = 0.0162 and accuracy of 0.99
Iteration 241152: with minibatch training loss = 0.00913 and accuracy of 1
Iteration 241280: with minibatch training loss = 0.00749 and accuracy of 1
Iteration 241408: with minibatch training loss = 0.0037 and accuracy of 1
Iteration 241536: with minibatch training loss = 0.00384 and accuracy of 1
Iteration 241664: with minibatch training loss = 0.00607 and accuracy of 1
Iteration 241792: with minibatch training loss = 0.0466 and accuracy of 0.99
Iteration 241920: with minibatch training loss = 0.0012 and accuracy of 1
Iteration 242048: with minibatch training loss = 0.0397 and accuracy of 0.99
Iteration 242176: with minibatch training loss = 0.141 and accuracy of 0.98
Iteration 242304: with minibatch training loss = 0.00567 and accuracy of 1
Iteration 242432: with minibatch training loss = 0.00136 and accuracy of 1
Iteration 242560: with minibatch training loss = 0.0847 and accuracy of 0.98
Iteration 242688: with minibatch training loss = 0.147 and accuracy of 0.98
Iteration 242816: with minibatch training loss = 0.0671 and accuracy of 0.98
Iteration 242944: with minibatch training loss = 0.0728 and accuracy of 0.99
Iteration 243072: with minibatch training loss = 0.0179 and accuracy of 0.99
Iteration 243200: with minibatch training loss = 0.00757 and accuracy of 1
Iteration 243328: with minibatch training loss = 0.0799 and accuracy of 0.98
Iteration 243456: with minibatch training loss = 0.00267 and accuracy of 1
Iteration 243584: with minibatch training loss = 0.00327 and accuracy of 1
Iteration 243712: with minibatch training loss = 0.00125 and accuracy of 1
Iteration 243840: with minibatch training loss = 0.0323 and accuracy of 0.98
Iteration 243968: with minibatch training loss = 0.0104 and accuracy of 0.99
Iteration 244096: with minibatch training loss = 0.0293 and accuracy of 0.98
Iteration 244224: with minibatch training loss = 0.00146 and accuracy of 1
Iteration 244352: with minibatch training loss = 0.0739 and accuracy of 0.99
Iteration 244480: with minibatch training loss = 0.0356 and accuracy of 0.98
Iteration 244608: with minibatch training loss = 0.00354 and accuracy of 1
Iteration 244736: with minibatch training loss = 0.0439 and accuracy of 0.98
Epoch 59, Train loss: 0.0249 and Train accuracy of 0.993, Test loss: 0.161 and Test accuracy of 0.968
Iteration 244864: with minibatch training loss = 0.00538 and accuracy of 1
Iteration 244992: with minibatch training loss = 0.0661 and accuracy of 0.99
Iteration 245120: with minibatch training loss = 0.00723 and accuracy of 1
Iteration 245248: with minibatch training loss = 0.00349 and accuracy of 1
Iteration 245376: with minibatch training loss = 0.01 and accuracy of 0.99
Iteration 245504: with minibatch training loss = 0.0165 and accuracy of 0.99
Iteration 245632: with minibatch training loss = 0.0224 and accuracy of 0.99
Iteration 245760: with minibatch training loss = 0.00251 and accuracy of 1
Iteration 245888: with minibatch training loss = 0.0176 and accuracy of 0.99
Iteration 246016: with minibatch training loss = 0.00157 and accuracy of 1
Iteration 246144: with minibatch training loss = 0.00119 and accuracy of 1
Iteration 246272: with minibatch training loss = 0.0439 and accuracy of 0.98
Iteration 246400: with minibatch training loss = 0.00152 and accuracy of 1
Iteration 246528: with minibatch training loss = 0.0504 and accuracy of 0.99
Iteration 246656: with minibatch training loss = 0.0206 and accuracy of 0.99
Iteration 246784: with minibatch training loss = 0.00511 and accuracy of 1
Iteration 246912: with minibatch training loss = 0.00371 and accuracy of 1
Iteration 247040: with minibatch training loss = 0.00667 and accuracy of 1
Iteration 247168: with minibatch training loss = 0.00182 and accuracy of 1
Iteration 247296: with minibatch training loss = 0.00161 and accuracy of 1
Iteration 247424: with minibatch training loss = 0.0417 and accuracy of 0.99
Iteration 247552: with minibatch training loss = 0.00294 and accuracy of 1
Iteration 247680: with minibatch training loss = 0.0442 and accuracy of 0.98
Iteration 247808: with minibatch training loss = 0.00222 and accuracy of 1
Iteration 247936: with minibatch training loss = 0.00849 and accuracy of 1
Iteration 248064: with minibatch training loss = 0.00507 and accuracy of 1
Iteration 248192: with minibatch training loss = 0.00151 and accuracy of 1
Iteration 248320: with minibatch training loss = 0.0225 and accuracy of 0.98
Iteration 248448: with minibatch training loss = 0.0333 and accuracy of 0.99
Iteration 248576: with minibatch training loss = 0.00888 and accuracy of 1
Iteration 248704: with minibatch training loss = 0.0113 and accuracy of 0.99
Iteration 248832: with minibatch training loss = 0.0063 and accuracy of 1
Iteration 248960: with minibatch training loss = 0.0438 and accuracy of 0.99
Epoch 60, Train loss: 0.0244 and Train accuracy of 0.993, Test loss: 0.159 and Test accuracy of 0.968
Iteration 249088: with minibatch training loss = 0.0772 and accuracy of 0.98
Iteration 249216: with minibatch training loss = 0.0196 and accuracy of 0.99
Iteration 249344: with minibatch training loss = 0.0359 and accuracy of 0.98
Iteration 249472: with minibatch training loss = 0.00229 and accuracy of 1
Iteration 249600: with minibatch training loss = 0.00173 and accuracy of 1
Iteration 249728: with minibatch training loss = 0.0291 and accuracy of 0.99
Iteration 249856: with minibatch training loss = 0.0142 and accuracy of 0.99
Iteration 249984: with minibatch training loss = 0.0409 and accuracy of 0.99
Iteration 250112: with minibatch training loss = 0.0196 and accuracy of 0.99
Iteration 250240: with minibatch training loss = 0.0306 and accuracy of 0.99
Iteration 250368: with minibatch training loss = 0.0106 and accuracy of 1
Iteration 250496: with minibatch training loss = 0.00364 and accuracy of 1
Iteration 250624: with minibatch training loss = 0.0322 and accuracy of 0.99
Iteration 250752: with minibatch training loss = 0.0199 and accuracy of 0.99
Iteration 250880: with minibatch training loss = 0.0509 and accuracy of 0.98
Iteration 251008: with minibatch training loss = 0.00602 and accuracy of 1
Iteration 251136: with minibatch training loss = 0.0123 and accuracy of 1
Iteration 251264: with minibatch training loss = 0.0168 and accuracy of 0.99
Iteration 251392: with minibatch training loss = 0.00543 and accuracy of 1
Iteration 251520: with minibatch training loss = 0.11 and accuracy of 0.98
Iteration 251648: with minibatch training loss = 0.009 and accuracy of 1
Iteration 251776: with minibatch training loss = 0.0528 and accuracy of 0.98
Iteration 251904: with minibatch training loss = 0.0335 and accuracy of 0.98
Iteration 252032: with minibatch training loss = 0.011 and accuracy of 0.99
Iteration 252160: with minibatch training loss = 0.0479 and accuracy of 0.99
Iteration 252288: with minibatch training loss = 0.00681 and accuracy of 1
Iteration 252416: with minibatch training loss = 0.0127 and accuracy of 1
Iteration 252544: with minibatch training loss = 0.012 and accuracy of 0.99
Iteration 252672: with minibatch training loss = 0.0651 and accuracy of 0.98
Iteration 252800: with minibatch training loss = 0.00747 and accuracy of 1
Iteration 252928: with minibatch training loss = 0.0131 and accuracy of 0.99
Iteration 253056: with minibatch training loss = 0.012 and accuracy of 1
Epoch 61, Train loss: 0.0252 and Train accuracy of 0.993, Test loss: 0.16 and Test accuracy of 0.967
Iteration 253184: with minibatch training loss = 0.013 and accuracy of 0.99
Iteration 253312: with minibatch training loss = 0.0719 and accuracy of 0.98
Iteration 253440: with minibatch training loss = 0.00351 and accuracy of 1
Iteration 253568: with minibatch training loss = 0.0792 and accuracy of 0.98
Iteration 253696: with minibatch training loss = 0.0611 and accuracy of 0.98
Iteration 253824: with minibatch training loss = 0.0213 and accuracy of 0.99
Iteration 253952: with minibatch training loss = 0.017 and accuracy of 0.99
Iteration 254080: with minibatch training loss = 0.022 and accuracy of 0.98
Iteration 254208: with minibatch training loss = 0.00969 and accuracy of 0.99
Iteration 254336: with minibatch training loss = 0.00127 and accuracy of 1
Iteration 254464: with minibatch training loss = 0.167 and accuracy of 0.98
Iteration 254592: with minibatch training loss = 0.0662 and accuracy of 0.98
Iteration 254720: with minibatch training loss = 0.0695 and accuracy of 0.98
Iteration 254848: with minibatch training loss = 0.00573 and accuracy of 1
Iteration 254976: with minibatch training loss = 0.0294 and accuracy of 0.99
Iteration 255104: with minibatch training loss = 0.0584 and accuracy of 0.98
Iteration 255232: with minibatch training loss = 0.00185 and accuracy of 1
Iteration 255360: with minibatch training loss = 0.0311 and accuracy of 0.98
Iteration 255488: with minibatch training loss = 0.0145 and accuracy of 1
Iteration 255616: with minibatch training loss = 0.049 and accuracy of 0.97
Iteration 255744: with minibatch training loss = 0.0532 and accuracy of 0.99
Iteration 255872: with minibatch training loss = 0.0151 and accuracy of 0.99
Iteration 256000: with minibatch training loss = 0.00616 and accuracy of 1
Iteration 256128: with minibatch training loss = 0.0125 and accuracy of 0.99
Iteration 256256: with minibatch training loss = 0.0132 and accuracy of 0.99
Iteration 256384: with minibatch training loss = 0.00362 and accuracy of 1
Iteration 256512: with minibatch training loss = 0.0445 and accuracy of 0.99
Iteration 256640: with minibatch training loss = 0.0392 and accuracy of 0.99
Iteration 256768: with minibatch training loss = 0.0109 and accuracy of 1
Iteration 256896: with minibatch training loss = 0.00857 and accuracy of 1
Iteration 257024: with minibatch training loss = 0.00713 and accuracy of 1
Iteration 257152: with minibatch training loss = 0.00464 and accuracy of 1
Iteration 257280: with minibatch training loss = 0.0177 and accuracy of 0.99
Epoch 62, Train loss: 0.0246 and Train accuracy of 0.993, Test loss: 0.163 and Test accuracy of 0.966
Iteration 257408: with minibatch training loss = 0.00174 and accuracy of 1
Iteration 257536: with minibatch training loss = 0.00687 and accuracy of 1
Iteration 257664: with minibatch training loss = 0.0492 and accuracy of 0.98
Iteration 257792: with minibatch training loss = 0.00492 and accuracy of 1
Iteration 257920: with minibatch training loss = 0.0159 and accuracy of 0.99
Iteration 258048: with minibatch training loss = 0.000895 and accuracy of 1
Iteration 258176: with minibatch training loss = 0.124 and accuracy of 0.97
Iteration 258304: with minibatch training loss = 0.0032 and accuracy of 1
Iteration 258432: with minibatch training loss = 0.025 and accuracy of 0.99
Iteration 258560: with minibatch training loss = 0.0182 and accuracy of 0.98
Iteration 258688: with minibatch training loss = 0.0117 and accuracy of 1
Iteration 258816: with minibatch training loss = 0.0286 and accuracy of 0.99
Iteration 258944: with minibatch training loss = 0.0763 and accuracy of 0.98
Iteration 259072: with minibatch training loss = 0.0206 and accuracy of 0.99
Iteration 259200: with minibatch training loss = 0.0129 and accuracy of 0.99
Iteration 259328: with minibatch training loss = 0.00647 and accuracy of 1
Iteration 259456: with minibatch training loss = 0.0382 and accuracy of 0.99
Iteration 259584: with minibatch training loss = 0.0018 and accuracy of 1
Iteration 259712: with minibatch training loss = 0.0019 and accuracy of 1
Iteration 259840: with minibatch training loss = 0.00581 and accuracy of 1
Iteration 259968: with minibatch training loss = 0.0605 and accuracy of 0.99
Iteration 260096: with minibatch training loss = 0.00465 and accuracy of 1
Iteration 260224: with minibatch training loss = 0.00523 and accuracy of 1
Iteration 260352: with minibatch training loss = 0.000945 and accuracy of 1
Iteration 260480: with minibatch training loss = 0.00304 and accuracy of 1
Iteration 260608: with minibatch training loss = 0.00532 and accuracy of 1
Iteration 260736: with minibatch training loss = 0.00877 and accuracy of 0.99
Iteration 260864: with minibatch training loss = 0.00671 and accuracy of 1
Iteration 260992: with minibatch training loss = 0.0316 and accuracy of 0.98
Iteration 261120: with minibatch training loss = 0.0273 and accuracy of 0.99
Iteration 261248: with minibatch training loss = 0.0212 and accuracy of 0.98
Iteration 261376: with minibatch training loss = 0.00123 and accuracy of 1
Epoch 63, Train loss: 0.0237 and Train accuracy of 0.994, Test loss: 0.164 and Test accuracy of 0.967
Iteration 261504: with minibatch training loss = 0.0209 and accuracy of 0.99
Iteration 261632: with minibatch training loss = 0.0651 and accuracy of 0.99
Iteration 261760: with minibatch training loss = 0.0276 and accuracy of 0.99
Iteration 261888: with minibatch training loss = 0.00979 and accuracy of 0.99
Iteration 262016: with minibatch training loss = 0.00968 and accuracy of 0.99
Iteration 262144: with minibatch training loss = 0.0195 and accuracy of 0.99
Iteration 262272: with minibatch training loss = 0.00916 and accuracy of 0.99
Iteration 262400: with minibatch training loss = 0.0733 and accuracy of 0.99
Iteration 262528: with minibatch training loss = 0.0673 and accuracy of 0.97
Iteration 262656: with minibatch training loss = 0.01 and accuracy of 1
Iteration 262784: with minibatch training loss = 0.0183 and accuracy of 0.99
Iteration 262912: with minibatch training loss = 0.0231 and accuracy of 0.99
Iteration 263040: with minibatch training loss = 0.00217 and accuracy of 1
Iteration 263168: with minibatch training loss = 0.0283 and accuracy of 0.99
Iteration 263296: with minibatch training loss = 0.00501 and accuracy of 1
Iteration 263424: with minibatch training loss = 0.00411 and accuracy of 1
Iteration 263552: with minibatch training loss = 0.00901 and accuracy of 0.99
Iteration 263680: with minibatch training loss = 0.0174 and accuracy of 0.99
Iteration 263808: with minibatch training loss = 0.00588 and accuracy of 1
Iteration 263936: with minibatch training loss = 0.0382 and accuracy of 0.99
Iteration 264064: with minibatch training loss = 0.0466 and accuracy of 0.98
Iteration 264192: with minibatch training loss = 0.0022 and accuracy of 1
Iteration 264320: with minibatch training loss = 0.0231 and accuracy of 0.98
Iteration 264448: with minibatch training loss = 0.00216 and accuracy of 1
Iteration 264576: with minibatch training loss = 0.0265 and accuracy of 0.99
Iteration 264704: with minibatch training loss = 0.0181 and accuracy of 0.99
Iteration 264832: with minibatch training loss = 0.011 and accuracy of 1
Iteration 264960: with minibatch training loss = 0.0159 and accuracy of 0.99
Iteration 265088: with minibatch training loss = 0.00186 and accuracy of 1
Iteration 265216: with minibatch training loss = 0.00759 and accuracy of 1
Iteration 265344: with minibatch training loss = 0.00214 and accuracy of 1
Iteration 265472: with minibatch training loss = 0.00555 and accuracy of 1
Epoch 64, Train loss: 0.0237 and Train accuracy of 0.994, Test loss: 0.162 and Test accuracy of 0.967
Iteration 265600: with minibatch training loss = 0.00648 and accuracy of 1
Iteration 265728: with minibatch training loss = 0.0843 and accuracy of 0.98
Iteration 265856: with minibatch training loss = 0.00112 and accuracy of 1
Iteration 265984: with minibatch training loss = 0.00954 and accuracy of 1
Iteration 266112: with minibatch training loss = 0.0137 and accuracy of 0.99
Iteration 266240: with minibatch training loss = 0.00401 and accuracy of 1
Iteration 266368: with minibatch training loss = 0.0103 and accuracy of 0.99
Iteration 266496: with minibatch training loss = 0.00334 and accuracy of 1
Iteration 266624: with minibatch training loss = 0.00348 and accuracy of 1
Iteration 266752: with minibatch training loss = 0.00993 and accuracy of 1
Iteration 266880: with minibatch training loss = 0.0136 and accuracy of 0.99
Iteration 267008: with minibatch training loss = 0.00656 and accuracy of 1
Iteration 267136: with minibatch training loss = 0.0122 and accuracy of 0.99
Iteration 267264: with minibatch training loss = 0.0175 and accuracy of 0.99
Iteration 267392: with minibatch training loss = 0.0206 and accuracy of 0.99
Iteration 267520: with minibatch training loss = 0.00148 and accuracy of 1
Iteration 267648: with minibatch training loss = 0.01 and accuracy of 1
Iteration 267776: with minibatch training loss = 0.0638 and accuracy of 0.99
Iteration 267904: with minibatch training loss = 0.0147 and accuracy of 0.99
Iteration 268032: with minibatch training loss = 0.00181 and accuracy of 1
Iteration 268160: with minibatch training loss = 0.00348 and accuracy of 1
Iteration 268288: with minibatch training loss = 0.0197 and accuracy of 0.99
Iteration 268416: with minibatch training loss = 0.00729 and accuracy of 1
Iteration 268544: with minibatch training loss = 0.0204 and accuracy of 0.99
Iteration 268672: with minibatch training loss = 0.029 and accuracy of 0.98
Iteration 268800: with minibatch training loss = 0.00807 and accuracy of 1
Iteration 268928: with minibatch training loss = 0.00435 and accuracy of 1
Iteration 269056: with minibatch training loss = 0.0315 and accuracy of 0.99
Iteration 269184: with minibatch training loss = 0.00585 and accuracy of 1
Iteration 269312: with minibatch training loss = 0.00178 and accuracy of 1
Iteration 269440: with minibatch training loss = 0.00776 and accuracy of 1
Iteration 269568: with minibatch training loss = 0.00284 and accuracy of 1
Iteration 269696: with minibatch training loss = 0.046 and accuracy of 0.99
Epoch 65, Train loss: 0.0227 and Train accuracy of 0.994, Test loss: 0.16 and Test accuracy of 0.968
Iteration 269824: with minibatch training loss = 0.0234 and accuracy of 0.99
Iteration 269952: with minibatch training loss = 0.0624 and accuracy of 0.98
Iteration 270080: with minibatch training loss = 0.0775 and accuracy of 0.99
Iteration 270208: with minibatch training loss = 0.0162 and accuracy of 0.98
Iteration 270336: with minibatch training loss = 0.024 and accuracy of 0.99
Iteration 270464: with minibatch training loss = 0.0659 and accuracy of 0.98
Iteration 270592: with minibatch training loss = 0.0495 and accuracy of 0.99
Iteration 270720: with minibatch training loss = 0.0777 and accuracy of 0.99
Iteration 270848: with minibatch training loss = 0.00375 and accuracy of 1
Iteration 270976: with minibatch training loss = 0.00537 and accuracy of 1
Iteration 271104: with minibatch training loss = 0.0202 and accuracy of 0.99
Iteration 271232: with minibatch training loss = 0.0023 and accuracy of 1
Iteration 271360: with minibatch training loss = 0.0177 and accuracy of 0.99
Iteration 271488: with minibatch training loss = 0.00266 and accuracy of 1
Iteration 271616: with minibatch training loss = 0.031 and accuracy of 0.98
Iteration 271744: with minibatch training loss = 0.0108 and accuracy of 1
Iteration 271872: with minibatch training loss = 0.013 and accuracy of 1
Iteration 272000: with minibatch training loss = 0.0137 and accuracy of 0.99
Iteration 272128: with minibatch training loss = 0.0659 and accuracy of 0.98
Iteration 272256: with minibatch training loss = 0.0515 and accuracy of 0.98
Iteration 272384: with minibatch training loss = 0.0958 and accuracy of 0.98
Iteration 272512: with minibatch training loss = 0.04 and accuracy of 0.98
Iteration 272640: with minibatch training loss = 0.0244 and accuracy of 0.98
Iteration 272768: with minibatch training loss = 0.0133 and accuracy of 0.99
Iteration 272896: with minibatch training loss = 0.131 and accuracy of 0.97
Iteration 273024: with minibatch training loss = 0.011 and accuracy of 0.99
Iteration 273152: with minibatch training loss = 0.0173 and accuracy of 0.99
Iteration 273280: with minibatch training loss = 0.00384 and accuracy of 1
Iteration 273408: with minibatch training loss = 0.0131 and accuracy of 1
Iteration 273536: with minibatch training loss = 0.00411 and accuracy of 1
Iteration 273664: with minibatch training loss = 0.00168 and accuracy of 1
Iteration 273792: with minibatch training loss = 0.00255 and accuracy of 1
Epoch 66, Train loss: 0.0234 and Train accuracy of 0.994, Test loss: 0.163 and Test accuracy of 0.967
Iteration 273920: with minibatch training loss = 0.0154 and accuracy of 1
Iteration 274048: with minibatch training loss = 0.0476 and accuracy of 0.99
Iteration 274176: with minibatch training loss = 0.0165 and accuracy of 0.99
Iteration 274304: with minibatch training loss = 0.0228 and accuracy of 0.99
Iteration 274432: with minibatch training loss = 0.00303 and accuracy of 1
Iteration 274560: with minibatch training loss = 0.0528 and accuracy of 0.98
Iteration 274688: with minibatch training loss = 0.00301 and accuracy of 1
Iteration 274816: with minibatch training loss = 0.00154 and accuracy of 1
Iteration 274944: with minibatch training loss = 0.0215 and accuracy of 0.98
Iteration 275072: with minibatch training loss = 0.0108 and accuracy of 1
Iteration 275200: with minibatch training loss = 0.0688 and accuracy of 0.98
Iteration 275328: with minibatch training loss = 0.011 and accuracy of 0.99
Iteration 275456: with minibatch training loss = 0.043 and accuracy of 0.98
Iteration 275584: with minibatch training loss = 0.0256 and accuracy of 0.99
Iteration 275712: with minibatch training loss = 0.0397 and accuracy of 0.99
Iteration 275840: with minibatch training loss = 0.00229 and accuracy of 1
Iteration 275968: with minibatch training loss = 0.00595 and accuracy of 1
Iteration 276096: with minibatch training loss = 0.0486 and accuracy of 0.99
Iteration 276224: with minibatch training loss = 0.00417 and accuracy of 1
Iteration 276352: with minibatch training loss = 0.0224 and accuracy of 0.99
Iteration 276480: with minibatch training loss = 0.00197 and accuracy of 1
Iteration 276608: with minibatch training loss = 0.00173 and accuracy of 1
Iteration 276736: with minibatch training loss = 0.0185 and accuracy of 0.99
Iteration 276864: with minibatch training loss = 0.0167 and accuracy of 0.99
Iteration 276992: with minibatch training loss = 0.0257 and accuracy of 0.99
Iteration 277120: with minibatch training loss = 0.00314 and accuracy of 1
Iteration 277248: with minibatch training loss = 0.0192 and accuracy of 0.98
Iteration 277376: with minibatch training loss = 0.0213 and accuracy of 0.98
Iteration 277504: with minibatch training loss = 0.0932 and accuracy of 0.98
Iteration 277632: with minibatch training loss = 0.0114 and accuracy of 0.99
Iteration 277760: with minibatch training loss = 0.0192 and accuracy of 0.98
Iteration 277888: with minibatch training loss = 0.0195 and accuracy of 0.98
Iteration 278016: with minibatch training loss = 0.0174 and accuracy of 0.99
Epoch 67, Train loss: 0.0227 and Train accuracy of 0.994, Test loss: 0.162 and Test accuracy of 0.967
Iteration 278144: with minibatch training loss = 0.0394 and accuracy of 0.98
Iteration 278272: with minibatch training loss = 0.00789 and accuracy of 0.99
Iteration 278400: with minibatch training loss = 0.00983 and accuracy of 1
Iteration 278528: with minibatch training loss = 0.00323 and accuracy of 1
Iteration 278656: with minibatch training loss = 0.0718 and accuracy of 0.98
Iteration 278784: with minibatch training loss = 0.00705 and accuracy of 1
Iteration 278912: with minibatch training loss = 0.00967 and accuracy of 1
Iteration 279040: with minibatch training loss = 0.00912 and accuracy of 1
Iteration 279168: with minibatch training loss = 0.00112 and accuracy of 1
Iteration 279296: with minibatch training loss = 0.00288 and accuracy of 1
Iteration 279424: with minibatch training loss = 0.012 and accuracy of 1
Iteration 279552: with minibatch training loss = 0.0324 and accuracy of 0.99
Iteration 279680: with minibatch training loss = 0.00814 and accuracy of 1
Iteration 279808: with minibatch training loss = 0.0194 and accuracy of 0.99
Iteration 279936: with minibatch training loss = 0.00459 and accuracy of 1
Iteration 280064: with minibatch training loss = 0.00885 and accuracy of 1
Iteration 280192: with minibatch training loss = 0.00693 and accuracy of 1
Iteration 280320: with minibatch training loss = 0.039 and accuracy of 0.99
Iteration 280448: with minibatch training loss = 0.044 and accuracy of 0.99
Iteration 280576: with minibatch training loss = 0.00493 and accuracy of 1
Iteration 280704: with minibatch training loss = 0.11 and accuracy of 0.97
Iteration 280832: with minibatch training loss = 0.0793 and accuracy of 0.98
Iteration 280960: with minibatch training loss = 0.0212 and accuracy of 0.99
Iteration 281088: with minibatch training loss = 0.0419 and accuracy of 0.99
Iteration 281216: with minibatch training loss = 0.00118 and accuracy of 1
Iteration 281344: with minibatch training loss = 0.0178 and accuracy of 0.98
Iteration 281472: with minibatch training loss = 0.00542 and accuracy of 1
Iteration 281600: with minibatch training loss = 0.0028 and accuracy of 1
Iteration 281728: with minibatch training loss = 0.0201 and accuracy of 0.99
Iteration 281856: with minibatch training loss = 0.0316 and accuracy of 0.98
Iteration 281984: with minibatch training loss = 0.0532 and accuracy of 0.98
Iteration 282112: with minibatch training loss = 0.00653 and accuracy of 1
Epoch 68, Train loss: 0.0221 and Train accuracy of 0.994, Test loss: 0.163 and Test accuracy of 0.968
Iteration 282240: with minibatch training loss = 0.0869 and accuracy of 0.98
Iteration 282368: with minibatch training loss = 0.0346 and accuracy of 0.99
Iteration 282496: with minibatch training loss = 0.00512 and accuracy of 1
Iteration 282624: with minibatch training loss = 0.0265 and accuracy of 0.98
Iteration 282752: with minibatch training loss = 0.00901 and accuracy of 0.99
Iteration 282880: with minibatch training loss = 0.00303 and accuracy of 1
Iteration 283008: with minibatch training loss = 0.0299 and accuracy of 0.99
Iteration 283136: with minibatch training loss = 0.00353 and accuracy of 1
Iteration 283264: with minibatch training loss = 0.0775 and accuracy of 0.98
Iteration 283392: with minibatch training loss = 0.0193 and accuracy of 0.99
Iteration 283520: with minibatch training loss = 0.00122 and accuracy of 1
Iteration 283648: with minibatch training loss = 0.00465 and accuracy of 1
Iteration 283776: with minibatch training loss = 0.0423 and accuracy of 0.99
Iteration 283904: with minibatch training loss = 0.0012 and accuracy of 1
Iteration 284032: with minibatch training loss = 0.00968 and accuracy of 1
Iteration 284160: with minibatch training loss = 0.0526 and accuracy of 0.98
Iteration 284288: with minibatch training loss = 0.0316 and accuracy of 0.99
Iteration 284416: with minibatch training loss = 0.00514 and accuracy of 1
Iteration 284544: with minibatch training loss = 0.039 and accuracy of 0.99
Iteration 284672: with minibatch training loss = 0.00761 and accuracy of 1
Iteration 284800: with minibatch training loss = 0.00246 and accuracy of 1
Iteration 284928: with minibatch training loss = 0.00489 and accuracy of 1
Iteration 285056: with minibatch training loss = 0.0388 and accuracy of 0.98
Iteration 285184: with minibatch training loss = 0.0391 and accuracy of 0.99
Iteration 285312: with minibatch training loss = 0.00829 and accuracy of 1
Iteration 285440: with minibatch training loss = 0.028 and accuracy of 0.98
Iteration 285568: with minibatch training loss = 0.00812 and accuracy of 0.99
Iteration 285696: with minibatch training loss = 0.00796 and accuracy of 1
Iteration 285824: with minibatch training loss = 0.0321 and accuracy of 0.99
Iteration 285952: with minibatch training loss = 0.00654 and accuracy of 1
Iteration 286080: with minibatch training loss = 0.00844 and accuracy of 1
Iteration 286208: with minibatch training loss = 0.00948 and accuracy of 1
Iteration 286336: with minibatch training loss = 0.0113 and accuracy of 0.99
Epoch 69, Train loss: 0.0226 and Train accuracy of 0.994, Test loss: 0.162 and Test accuracy of 0.967
Iteration 286464: with minibatch training loss = 0.0525 and accuracy of 0.98
Iteration 286592: with minibatch training loss = 0.0469 and accuracy of 0.98
Iteration 286720: with minibatch training loss = 0.072 and accuracy of 0.98
Iteration 286848: with minibatch training loss = 0.00533 and accuracy of 1
Iteration 286976: with minibatch training loss = 0.0468 and accuracy of 0.96
Iteration 287104: with minibatch training loss = 0.0524 and accuracy of 0.99
Iteration 287232: with minibatch training loss = 0.0277 and accuracy of 0.99
Iteration 287360: with minibatch training loss = 0.00191 and accuracy of 1
Iteration 287488: with minibatch training loss = 0.0675 and accuracy of 0.98
Iteration 287616: with minibatch training loss = 0.0106 and accuracy of 0.99
Iteration 287744: with minibatch training loss = 0.0181 and accuracy of 0.99
Iteration 287872: with minibatch training loss = 0.00202 and accuracy of 1
Iteration 288000: with minibatch training loss = 0.0584 and accuracy of 0.98
Iteration 288128: with minibatch training loss = 0.0254 and accuracy of 0.99
Iteration 288256: with minibatch training loss = 0.00447 and accuracy of 1
Iteration 288384: with minibatch training loss = 0.00231 and accuracy of 1
Iteration 288512: with minibatch training loss = 0.00231 and accuracy of 1
Iteration 288640: with minibatch training loss = 0.00334 and accuracy of 1
Iteration 288768: with minibatch training loss = 0.0766 and accuracy of 0.98
Iteration 288896: with minibatch training loss = 0.00728 and accuracy of 1
Iteration 289024: with minibatch training loss = 0.00274 and accuracy of 1
Iteration 289152: with minibatch training loss = 0.00929 and accuracy of 0.99
Iteration 289280: with minibatch training loss = 0.046 and accuracy of 0.98
Iteration 289408: with minibatch training loss = 0.00227 and accuracy of 1
Iteration 289536: with minibatch training loss = 0.0333 and accuracy of 0.98
Iteration 289664: with minibatch training loss = 0.00238 and accuracy of 1
Iteration 289792: with minibatch training loss = 0.0018 and accuracy of 1
Iteration 289920: with minibatch training loss = 0.000744 and accuracy of 1
Iteration 290048: with minibatch training loss = 0.00173 and accuracy of 1
Iteration 290176: with minibatch training loss = 0.00264 and accuracy of 1
Iteration 290304: with minibatch training loss = 0.00787 and accuracy of 0.99
Iteration 290432: with minibatch training loss = 0.0198 and accuracy of 0.99
Epoch 70, Train loss: 0.0219 and Train accuracy of 0.994, Test loss: 0.162 and Test accuracy of 0.968
Iteration 290560: with minibatch training loss = 0.0031 and accuracy of 1
Iteration 290688: with minibatch training loss = 0.0165 and accuracy of 0.98
Iteration 290816: with minibatch training loss = 0.0217 and accuracy of 0.99
Iteration 290944: with minibatch training loss = 0.00684 and accuracy of 1
Iteration 291072: with minibatch training loss = 0.0357 and accuracy of 0.99
Iteration 291200: with minibatch training loss = 0.0169 and accuracy of 0.98
Iteration 291328: with minibatch training loss = 0.00423 and accuracy of 1
Iteration 291456: with minibatch training loss = 0.00276 and accuracy of 1
Iteration 291584: with minibatch training loss = 0.00319 and accuracy of 1
Iteration 291712: with minibatch training loss = 0.0106 and accuracy of 0.99
Iteration 291840: with minibatch training loss = 0.0166 and accuracy of 0.99
Iteration 291968: with minibatch training loss = 0.0683 and accuracy of 0.99
Iteration 292096: with minibatch training loss = 0.0312 and accuracy of 0.98
Iteration 292224: with minibatch training loss = 0.0074 and accuracy of 1
Iteration 292352: with minibatch training loss = 0.0177 and accuracy of 0.99
Iteration 292480: with minibatch training loss = 0.025 and accuracy of 0.98
Iteration 292608: with minibatch training loss = 0.0382 and accuracy of 0.99
Iteration 292736: with minibatch training loss = 0.0143 and accuracy of 0.99
Iteration 292864: with minibatch training loss = 0.00246 and accuracy of 1
Iteration 292992: with minibatch training loss = 0.00854 and accuracy of 1
Iteration 293120: with minibatch training loss = 0.0286 and accuracy of 0.99
Iteration 293248: with minibatch training loss = 0.0117 and accuracy of 1
Iteration 293376: with minibatch training loss = 0.0519 and accuracy of 0.98
Iteration 293504: with minibatch training loss = 0.0231 and accuracy of 0.99
Iteration 293632: with minibatch training loss = 0.00919 and accuracy of 1
Iteration 293760: with minibatch training loss = 0.0765 and accuracy of 0.98
Iteration 293888: with minibatch training loss = 0.00374 and accuracy of 1
Iteration 294016: with minibatch training loss = 0.00714 and accuracy of 1
Iteration 294144: with minibatch training loss = 0.0261 and accuracy of 0.99
Iteration 294272: with minibatch training loss = 0.033 and accuracy of 0.98
Iteration 294400: with minibatch training loss = 0.0238 and accuracy of 0.99
Iteration 294528: with minibatch training loss = 0.0274 and accuracy of 0.99
Epoch 71, Train loss: 0.0217 and Train accuracy of 0.994, Test loss: 0.162 and Test accuracy of 0.968
Iteration 294656: with minibatch training loss = 0.00495 and accuracy of 1
Iteration 294784: with minibatch training loss = 0.0459 and accuracy of 0.98
Iteration 294912: with minibatch training loss = 0.00799 and accuracy of 1
Iteration 295040: with minibatch training loss = 0.0203 and accuracy of 0.99
Iteration 295168: with minibatch training loss = 0.0245 and accuracy of 0.99
Iteration 295296: with minibatch training loss = 0.0351 and accuracy of 0.99
Iteration 295424: with minibatch training loss = 0.00153 and accuracy of 1
Iteration 295552: with minibatch training loss = 0.00719 and accuracy of 1
Iteration 295680: with minibatch training loss = 0.0694 and accuracy of 0.99
Iteration 295808: with minibatch training loss = 0.00588 and accuracy of 1
Iteration 295936: with minibatch training loss = 0.00747 and accuracy of 1
Iteration 296064: with minibatch training loss = 0.0118 and accuracy of 0.99
Iteration 296192: with minibatch training loss = 0.00297 and accuracy of 1
Iteration 296320: with minibatch training loss = 0.019 and accuracy of 0.99
Iteration 296448: with minibatch training loss = 0.0592 and accuracy of 0.99
Iteration 296576: with minibatch training loss = 0.0161 and accuracy of 0.99
Iteration 296704: with minibatch training loss = 0.0547 and accuracy of 0.98
Iteration 296832: with minibatch training loss = 0.0075 and accuracy of 1
Iteration 296960: with minibatch training loss = 0.0188 and accuracy of 0.99
Iteration 297088: with minibatch training loss = 0.0179 and accuracy of 0.99
Iteration 297216: with minibatch training loss = 0.0092 and accuracy of 0.99
Iteration 297344: with minibatch training loss = 0.0747 and accuracy of 0.98
Iteration 297472: with minibatch training loss = 0.029 and accuracy of 0.99
Iteration 297600: with minibatch training loss = 0.0105 and accuracy of 1
Iteration 297728: with minibatch training loss = 0.00365 and accuracy of 1
Iteration 297856: with minibatch training loss = 0.0195 and accuracy of 0.99
Iteration 297984: with minibatch training loss = 0.00263 and accuracy of 1
Iteration 298112: with minibatch training loss = 0.00282 and accuracy of 1
Iteration 298240: with minibatch training loss = 0.095 and accuracy of 0.99
Iteration 298368: with minibatch training loss = 0.0144 and accuracy of 0.99
Iteration 298496: with minibatch training loss = 0.00536 and accuracy of 1
Iteration 298624: with minibatch training loss = 0.0765 and accuracy of 0.98
Iteration 298752: with minibatch training loss = 0.0167 and accuracy of 0.99
Epoch 72, Train loss: 0.0214 and Train accuracy of 0.994, Test loss: 0.17 and Test accuracy of 0.966
Iteration 298880: with minibatch training loss = 0.0343 and accuracy of 0.99
Iteration 299008: with minibatch training loss = 0.0083 and accuracy of 1
Iteration 299136: with minibatch training loss = 0.108 and accuracy of 0.98
Iteration 299264: with minibatch training loss = 0.00133 and accuracy of 1
Iteration 299392: with minibatch training loss = 0.0564 and accuracy of 0.98
Iteration 299520: with minibatch training loss = 0.00192 and accuracy of 1
Iteration 299648: with minibatch training loss = 0.00212 and accuracy of 1
Iteration 299776: with minibatch training loss = 0.00141 and accuracy of 1
Iteration 299904: with minibatch training loss = 0.0134 and accuracy of 0.99
Iteration 300032: with minibatch training loss = 0.00139 and accuracy of 1
Iteration 300160: with minibatch training loss = 0.00325 and accuracy of 1
Iteration 300288: with minibatch training loss = 0.00187 and accuracy of 1
Iteration 300416: with minibatch training loss = 0.0133 and accuracy of 0.99
Iteration 300544: with minibatch training loss = 0.0245 and accuracy of 0.99
Iteration 300672: with minibatch training loss = 0.0492 and accuracy of 0.99
Iteration 300800: with minibatch training loss = 0.028 and accuracy of 0.98
Iteration 300928: with minibatch training loss = 0.00433 and accuracy of 1
Iteration 301056: with minibatch training loss = 0.0129 and accuracy of 0.99
Iteration 301184: with minibatch training loss = 0.013 and accuracy of 1
Iteration 301312: with minibatch training loss = 0.0634 and accuracy of 0.98
Iteration 301440: with minibatch training loss = 0.0241 and accuracy of 0.99
Iteration 301568: with minibatch training loss = 0.046 and accuracy of 0.97
Iteration 301696: with minibatch training loss = 0.00662 and accuracy of 1
Iteration 301824: with minibatch training loss = 0.00423 and accuracy of 1
Iteration 301952: with minibatch training loss = 0.00262 and accuracy of 1
Iteration 302080: with minibatch training loss = 0.00294 and accuracy of 1
Iteration 302208: with minibatch training loss = 0.0766 and accuracy of 0.98
Iteration 302336: with minibatch training loss = 0.016 and accuracy of 0.99
Iteration 302464: with minibatch training loss = 0.0103 and accuracy of 1
Iteration 302592: with minibatch training loss = 0.00179 and accuracy of 1
Iteration 302720: with minibatch training loss = 0.0138 and accuracy of 0.99
Iteration 302848: with minibatch training loss = 0.0351 and accuracy of 0.99
Epoch 73, Train loss: 0.0209 and Train accuracy of 0.994, Test loss: 0.166 and Test accuracy of 0.968
Iteration 302976: with minibatch training loss = 0.00799 and accuracy of 1
Iteration 303104: with minibatch training loss = 0.0121 and accuracy of 0.99
Iteration 303232: with minibatch training loss = 0.00141 and accuracy of 1
Iteration 303360: with minibatch training loss = 0.0448 and accuracy of 0.98
Iteration 303488: with minibatch training loss = 0.0951 and accuracy of 0.99
Iteration 303616: with minibatch training loss = 0.00578 and accuracy of 1
Iteration 303744: with minibatch training loss = 0.00848 and accuracy of 1
Iteration 303872: with minibatch training loss = 0.0492 and accuracy of 0.99
Iteration 304000: with minibatch training loss = 0.000745 and accuracy of 1
Iteration 304128: with minibatch training loss = 0.0336 and accuracy of 0.99
Iteration 304256: with minibatch training loss = 0.0338 and accuracy of 0.98
Iteration 304384: with minibatch training loss = 0.00899 and accuracy of 1
Iteration 304512: with minibatch training loss = 0.018 and accuracy of 0.99
Iteration 304640: with minibatch training loss = 0.00327 and accuracy of 1
Iteration 304768: with minibatch training loss = 0.0218 and accuracy of 0.98
Iteration 304896: with minibatch training loss = 0.00539 and accuracy of 1
Iteration 305024: with minibatch training loss = 0.00162 and accuracy of 1
Iteration 305152: with minibatch training loss = 0.0279 and accuracy of 0.99
Iteration 305280: with minibatch training loss = 0.0302 and accuracy of 0.99
Iteration 305408: with minibatch training loss = 0.00526 and accuracy of 1
Iteration 305536: with minibatch training loss = 0.0124 and accuracy of 1
Iteration 305664: with minibatch training loss = 0.0214 and accuracy of 0.99
Iteration 305792: with minibatch training loss = 0.0633 and accuracy of 0.99
Iteration 305920: with minibatch training loss = 0.0311 and accuracy of 0.99
Iteration 306048: with minibatch training loss = 0.00168 and accuracy of 1
Iteration 306176: with minibatch training loss = 0.00255 and accuracy of 1
Iteration 306304: with minibatch training loss = 0.0467 and accuracy of 0.98
Iteration 306432: with minibatch training loss = 0.0404 and accuracy of 0.99
Iteration 306560: with minibatch training loss = 0.0161 and accuracy of 0.99
Iteration 306688: with minibatch training loss = 0.0014 and accuracy of 1
Iteration 306816: with minibatch training loss = 0.0347 and accuracy of 0.98
Iteration 306944: with minibatch training loss = 0.00526 and accuracy of 1
Iteration 307072: with minibatch training loss = 0.0199 and accuracy of 0.99
Epoch 74, Train loss: 0.021 and Train accuracy of 0.994, Test loss: 0.165 and Test accuracy of 0.968
Iteration 307200: with minibatch training loss = 0.0148 and accuracy of 0.99
Iteration 307328: with minibatch training loss = 0.00858 and accuracy of 0.99
Iteration 307456: with minibatch training loss = 0.0235 and accuracy of 0.99
Iteration 307584: with minibatch training loss = 0.0853 and accuracy of 0.98
Iteration 307712: with minibatch training loss = 0.00661 and accuracy of 1
Iteration 307840: with minibatch training loss = 0.0013 and accuracy of 1
Iteration 307968: with minibatch training loss = 0.0215 and accuracy of 0.99
Iteration 308096: with minibatch training loss = 0.0413 and accuracy of 0.98
Iteration 308224: with minibatch training loss = 0.0208 and accuracy of 0.99
Iteration 308352: with minibatch training loss = 0.00145 and accuracy of 1
Iteration 308480: with minibatch training loss = 0.00212 and accuracy of 1
Iteration 308608: with minibatch training loss = 0.0758 and accuracy of 0.98
Iteration 308736: with minibatch training loss = 0.00417 and accuracy of 1
Iteration 308864: with minibatch training loss = 0.0611 and accuracy of 0.98
Iteration 308992: with minibatch training loss = 0.0167 and accuracy of 1
Iteration 309120: with minibatch training loss = 0.0152 and accuracy of 0.99
Iteration 309248: with minibatch training loss = 0.00898 and accuracy of 0.99
Iteration 309376: with minibatch training loss = 0.0332 and accuracy of 0.99
Iteration 309504: with minibatch training loss = 0.00365 and accuracy of 1
Iteration 309632: with minibatch training loss = 0.023 and accuracy of 0.99
Iteration 309760: with minibatch training loss = 0.00839 and accuracy of 1
Iteration 309888: with minibatch training loss = 0.0304 and accuracy of 0.98
Iteration 310016: with minibatch training loss = 0.0124 and accuracy of 0.99
Iteration 310144: with minibatch training loss = 0.00216 and accuracy of 1
Iteration 310272: with minibatch training loss = 0.0439 and accuracy of 0.98
Iteration 310400: with minibatch training loss = 0.0192 and accuracy of 0.99
Iteration 310528: with minibatch training loss = 0.00449 and accuracy of 1
Iteration 310656: with minibatch training loss = 0.0179 and accuracy of 0.99
Iteration 310784: with minibatch training loss = 0.00454 and accuracy of 1
Iteration 310912: with minibatch training loss = 0.00205 and accuracy of 1
Iteration 311040: with minibatch training loss = 0.0292 and accuracy of 0.98
Iteration 311168: with minibatch training loss = 0.0156 and accuracy of 0.98
Epoch 75, Train loss: 0.0209 and Train accuracy of 0.994, Test loss: 0.165 and Test accuracy of 0.968
Iteration 311296: with minibatch training loss = 0.00292 and accuracy of 1
Iteration 311424: with minibatch training loss = 0.0538 and accuracy of 0.98
Iteration 311552: with minibatch training loss = 0.0181 and accuracy of 0.99
Iteration 311680: with minibatch training loss = 0.0049 and accuracy of 1
Iteration 311808: with minibatch training loss = 0.0534 and accuracy of 0.98
Iteration 311936: with minibatch training loss = 0.00281 and accuracy of 1
Iteration 312064: with minibatch training loss = 0.00543 and accuracy of 1
Iteration 312192: with minibatch training loss = 0.0341 and accuracy of 0.98
Iteration 312320: with minibatch training loss = 0.0325 and accuracy of 0.98
Iteration 312448: with minibatch training loss = 0.00196 and accuracy of 1
Iteration 312576: with minibatch training loss = 0.0142 and accuracy of 0.99
Iteration 312704: with minibatch training loss = 0.00832 and accuracy of 1
Iteration 312832: with minibatch training loss = 0.00287 and accuracy of 1
Iteration 312960: with minibatch training loss = 0.0457 and accuracy of 0.98
Iteration 313088: with minibatch training loss = 0.00193 and accuracy of 1
Iteration 313216: with minibatch training loss = 0.0162 and accuracy of 0.99
Iteration 313344: with minibatch training loss = 0.0279 and accuracy of 0.98
Iteration 313472: with minibatch training loss = 0.016 and accuracy of 0.99
Iteration 313600: with minibatch training loss = 0.0177 and accuracy of 0.99
Iteration 313728: with minibatch training loss = 0.00566 and accuracy of 1
Iteration 313856: with minibatch training loss = 0.00522 and accuracy of 1
Iteration 313984: with minibatch training loss = 0.0901 and accuracy of 0.99
Iteration 314112: with minibatch training loss = 0.0182 and accuracy of 0.98
Iteration 314240: with minibatch training loss = 0.0833 and accuracy of 0.98
Iteration 314368: with minibatch training loss = 0.00132 and accuracy of 1
Iteration 314496: with minibatch training loss = 0.0056 and accuracy of 1
Iteration 314624: with minibatch training loss = 0.0455 and accuracy of 0.98
Iteration 314752: with minibatch training loss = 0.0618 and accuracy of 0.99
Iteration 314880: with minibatch training loss = 0.00435 and accuracy of 1
Iteration 315008: with minibatch training loss = 0.002 and accuracy of 1
Iteration 315136: with minibatch training loss = 0.0204 and accuracy of 0.98
Iteration 315264: with minibatch training loss = 0.0218 and accuracy of 0.99
Iteration 315392: with minibatch training loss = 0.00505 and accuracy of 1
Epoch 76, Train loss: 0.0207 and Train accuracy of 0.994, Test loss: 0.163 and Test accuracy of 0.969
Iteration 315520: with minibatch training loss = 0.00202 and accuracy of 1
Iteration 315648: with minibatch training loss = 0.0346 and accuracy of 0.99
Iteration 315776: with minibatch training loss = 0.0304 and accuracy of 0.99
Iteration 315904: with minibatch training loss = 0.0361 and accuracy of 0.99
Iteration 316032: with minibatch training loss = 0.000832 and accuracy of 1
Iteration 316160: with minibatch training loss = 0.0496 and accuracy of 0.98
Iteration 316288: with minibatch training loss = 0.00416 and accuracy of 1
Iteration 316416: with minibatch training loss = 0.0113 and accuracy of 1
Iteration 316544: with minibatch training loss = 0.0116 and accuracy of 1
Iteration 316672: with minibatch training loss = 0.057 and accuracy of 0.98
Iteration 316800: with minibatch training loss = 0.00767 and accuracy of 0.99
Iteration 316928: with minibatch training loss = 0.00389 and accuracy of 1
Iteration 317056: with minibatch training loss = 0.00265 and accuracy of 1
Iteration 317184: with minibatch training loss = 0.0115 and accuracy of 1
Iteration 317312: with minibatch training loss = 0.00165 and accuracy of 1
Iteration 317440: with minibatch training loss = 0.0132 and accuracy of 0.99
Iteration 317568: with minibatch training loss = 0.00387 and accuracy of 1
Iteration 317696: with minibatch training loss = 0.0372 and accuracy of 0.98
Iteration 317824: with minibatch training loss = 0.00352 and accuracy of 1
Iteration 317952: with minibatch training loss = 0.00487 and accuracy of 1
Iteration 318080: with minibatch training loss = 0.0031 and accuracy of 1
Iteration 318208: with minibatch training loss = 0.024 and accuracy of 0.98
Iteration 318336: with minibatch training loss = 0.0018 and accuracy of 1
Iteration 318464: with minibatch training loss = 0.00746 and accuracy of 1
Iteration 318592: with minibatch training loss = 0.0202 and accuracy of 0.99
Iteration 318720: with minibatch training loss = 0.0013 and accuracy of 1
Iteration 318848: with minibatch training loss = 0.0405 and accuracy of 0.99
Iteration 318976: with minibatch training loss = 0.00131 and accuracy of 1
Iteration 319104: with minibatch training loss = 0.0186 and accuracy of 0.99
Iteration 319232: with minibatch training loss = 0.105 and accuracy of 0.99
Iteration 319360: with minibatch training loss = 0.00333 and accuracy of 1
Iteration 319488: with minibatch training loss = 0.0591 and accuracy of 0.98
Epoch 77, Train loss: 0.0202 and Train accuracy of 0.994, Test loss: 0.169 and Test accuracy of 0.967
Iteration 319616: with minibatch training loss = 0.00943 and accuracy of 1
Iteration 319744: with minibatch training loss = 0.00839 and accuracy of 0.99
Iteration 319872: with minibatch training loss = 0.0124 and accuracy of 0.99
Iteration 320000: with minibatch training loss = 0.026 and accuracy of 0.99
Iteration 320128: with minibatch training loss = 0.00162 and accuracy of 1
Iteration 320256: with minibatch training loss = 0.0305 and accuracy of 0.99
Iteration 320384: with minibatch training loss = 0.015 and accuracy of 0.99
Iteration 320512: with minibatch training loss = 0.00783 and accuracy of 1
Iteration 320640: with minibatch training loss = 0.0266 and accuracy of 0.99
Iteration 320768: with minibatch training loss = 0.0014 and accuracy of 1
Iteration 320896: with minibatch training loss = 0.000848 and accuracy of 1
Iteration 321024: with minibatch training loss = 0.00756 and accuracy of 0.99
Iteration 321152: with minibatch training loss = 0.0187 and accuracy of 0.99
Iteration 321280: with minibatch training loss = 0.00192 and accuracy of 1
Iteration 321408: with minibatch training loss = 0.0292 and accuracy of 0.98
Iteration 321536: with minibatch training loss = 0.00178 and accuracy of 1
Iteration 321664: with minibatch training loss = 0.00922 and accuracy of 0.99
Iteration 321792: with minibatch training loss = 0.0297 and accuracy of 0.99
Iteration 321920: with minibatch training loss = 0.0321 and accuracy of 0.99
Iteration 322048: with minibatch training loss = 0.049 and accuracy of 0.98
Iteration 322176: with minibatch training loss = 0.00653 and accuracy of 1
Iteration 322304: with minibatch training loss = 0.0137 and accuracy of 0.99
Iteration 322432: with minibatch training loss = 0.00284 and accuracy of 1
Iteration 322560: with minibatch training loss = 0.0296 and accuracy of 0.98
Iteration 322688: with minibatch training loss = 0.00258 and accuracy of 1
Iteration 322816: with minibatch training loss = 0.0776 and accuracy of 0.99
Iteration 322944: with minibatch training loss = 0.0294 and accuracy of 0.98
Iteration 323072: with minibatch training loss = 0.0039 and accuracy of 1
Iteration 323200: with minibatch training loss = 0.0839 and accuracy of 0.99
Iteration 323328: with minibatch training loss = 0.0227 and accuracy of 0.99
Iteration 323456: with minibatch training loss = 0.00131 and accuracy of 1
Iteration 323584: with minibatch training loss = 0.0353 and accuracy of 0.99
Epoch 78, Train loss: 0.0201 and Train accuracy of 0.995, Test loss: 0.176 and Test accuracy of 0.966
Iteration 323712: with minibatch training loss = 0.00141 and accuracy of 1
Iteration 323840: with minibatch training loss = 0.000874 and accuracy of 1
Iteration 323968: with minibatch training loss = 0.00936 and accuracy of 1
Iteration 324096: with minibatch training loss = 0.106 and accuracy of 0.98
Iteration 324224: with minibatch training loss = 0.00644 and accuracy of 1
Iteration 324352: with minibatch training loss = 0.0162 and accuracy of 0.98
Iteration 324480: with minibatch training loss = 0.103 and accuracy of 0.98
Iteration 324608: with minibatch training loss = 0.0351 and accuracy of 0.98
Iteration 324736: with minibatch training loss = 0.0302 and accuracy of 0.98
Iteration 324864: with minibatch training loss = 0.00791 and accuracy of 1
Iteration 324992: with minibatch training loss = 0.0071 and accuracy of 1
Iteration 325120: with minibatch training loss = 0.0235 and accuracy of 0.99
Iteration 325248: with minibatch training loss = 0.0111 and accuracy of 0.99
Iteration 325376: with minibatch training loss = 0.00311 and accuracy of 1
Iteration 325504: with minibatch training loss = 0.00116 and accuracy of 1
Iteration 325632: with minibatch training loss = 0.00115 and accuracy of 1
Iteration 325760: with minibatch training loss = 0.00134 and accuracy of 1
Iteration 325888: with minibatch training loss = 0.0243 and accuracy of 0.99
Iteration 326016: with minibatch training loss = 0.0248 and accuracy of 0.99
Iteration 326144: with minibatch training loss = 0.00253 and accuracy of 1
Iteration 326272: with minibatch training loss = 0.00201 and accuracy of 1
Iteration 326400: with minibatch training loss = 0.00479 and accuracy of 1
Iteration 326528: with minibatch training loss = 0.0347 and accuracy of 0.99
Iteration 326656: with minibatch training loss = 0.00636 and accuracy of 1
Iteration 326784: with minibatch training loss = 0.0139 and accuracy of 0.99
Iteration 326912: with minibatch training loss = 0.0924 and accuracy of 0.97
Iteration 327040: with minibatch training loss = 0.0546 and accuracy of 0.99
Iteration 327168: with minibatch training loss = 0.0115 and accuracy of 1
Iteration 327296: with minibatch training loss = 0.0129 and accuracy of 1
Iteration 327424: with minibatch training loss = 0.0265 and accuracy of 0.99
Iteration 327552: with minibatch training loss = 0.00323 and accuracy of 1
Iteration 327680: with minibatch training loss = 0.0809 and accuracy of 0.98
Iteration 327808: with minibatch training loss = 0.0017 and accuracy of 1
Epoch 79, Train loss: 0.02 and Train accuracy of 0.995, Test loss: 0.167 and Test accuracy of 0.968
Iteration 327936: with minibatch training loss = 0.00135 and accuracy of 1
Iteration 328064: with minibatch training loss = 0.00822 and accuracy of 1
Iteration 328192: with minibatch training loss = 0.0425 and accuracy of 0.99
Iteration 328320: with minibatch training loss = 0.0133 and accuracy of 0.99
Iteration 328448: with minibatch training loss = 0.073 and accuracy of 0.98
Iteration 328576: with minibatch training loss = 0.0111 and accuracy of 0.99
Iteration 328704: with minibatch training loss = 0.0234 and accuracy of 0.99
Iteration 328832: with minibatch training loss = 0.0015 and accuracy of 1
Iteration 328960: with minibatch training loss = 0.0759 and accuracy of 0.98
Iteration 329088: with minibatch training loss = 0.0076 and accuracy of 1
Iteration 329216: with minibatch training loss = 0.069 and accuracy of 0.98
Iteration 329344: with minibatch training loss = 0.0116 and accuracy of 1
Iteration 329472: with minibatch training loss = 0.00922 and accuracy of 1
Iteration 329600: with minibatch training loss = 0.00839 and accuracy of 1
Iteration 329728: with minibatch training loss = 0.0262 and accuracy of 0.99
Iteration 329856: with minibatch training loss = 0.00877 and accuracy of 0.99
Iteration 329984: with minibatch training loss = 0.0246 and accuracy of 0.99
Iteration 330112: with minibatch training loss = 0.021 and accuracy of 0.99
Iteration 330240: with minibatch training loss = 0.0125 and accuracy of 1
Iteration 330368: with minibatch training loss = 0.00346 and accuracy of 1
Iteration 330496: with minibatch training loss = 0.0878 and accuracy of 0.98
Iteration 330624: with minibatch training loss = 0.00539 and accuracy of 1
Iteration 330752: with minibatch training loss = 0.0203 and accuracy of 0.98
Iteration 330880: with minibatch training loss = 0.0191 and accuracy of 0.99
Iteration 331008: with minibatch training loss = 0.014 and accuracy of 1
Iteration 331136: with minibatch training loss = 0.0102 and accuracy of 0.99
Iteration 331264: with minibatch training loss = 0.00383 and accuracy of 1
Iteration 331392: with minibatch training loss = 0.014 and accuracy of 0.99
Iteration 331520: with minibatch training loss = 0.00758 and accuracy of 1
Iteration 331648: with minibatch training loss = 0.0108 and accuracy of 0.99
Iteration 331776: with minibatch training loss = 0.0426 and accuracy of 0.99
Iteration 331904: with minibatch training loss = 0.0391 and accuracy of 0.98
Epoch 80, Train loss: 0.0198 and Train accuracy of 0.995, Test loss: 0.167 and Test accuracy of 0.967
Iteration 332032: with minibatch training loss = 0.00895 and accuracy of 1
Iteration 332160: with minibatch training loss = 0.00401 and accuracy of 1
Iteration 332288: with minibatch training loss = 0.00332 and accuracy of 1
Iteration 332416: with minibatch training loss = 0.175 and accuracy of 0.98
Iteration 332544: with minibatch training loss = 0.0277 and accuracy of 0.99
Iteration 332672: with minibatch training loss = 0.00335 and accuracy of 1
Iteration 332800: with minibatch training loss = 0.111 and accuracy of 0.98
Iteration 332928: with minibatch training loss = 0.0128 and accuracy of 0.99
Iteration 333056: with minibatch training loss = 0.00827 and accuracy of 0.99
Iteration 333184: with minibatch training loss = 0.00477 and accuracy of 1
Iteration 333312: with minibatch training loss = 0.00276 and accuracy of 1
Iteration 333440: with minibatch training loss = 0.0895 and accuracy of 0.99
Iteration 333568: with minibatch training loss = 0.00304 and accuracy of 1
Iteration 333696: with minibatch training loss = 0.0507 and accuracy of 0.98
Iteration 333824: with minibatch training loss = 0.0116 and accuracy of 0.99
Iteration 333952: with minibatch training loss = 0.000813 and accuracy of 1
Iteration 334080: with minibatch training loss = 0.00142 and accuracy of 1
Iteration 334208: with minibatch training loss = 0.0361 and accuracy of 0.99
Iteration 334336: with minibatch training loss = 0.0057 and accuracy of 1
Iteration 334464: with minibatch training loss = 0.00475 and accuracy of 1
Iteration 334592: with minibatch training loss = 0.0968 and accuracy of 0.99
Iteration 334720: with minibatch training loss = 0.00101 and accuracy of 1
Iteration 334848: with minibatch training loss = 0.00291 and accuracy of 1
Iteration 334976: with minibatch training loss = 0.0089 and accuracy of 0.99
Iteration 335104: with minibatch training loss = 0.0163 and accuracy of 0.99
Iteration 335232: with minibatch training loss = 0.00265 and accuracy of 1
Iteration 335360: with minibatch training loss = 0.0696 and accuracy of 0.98
Iteration 335488: with minibatch training loss = 0.00199 and accuracy of 1
Iteration 335616: with minibatch training loss = 0.039 and accuracy of 0.99
Iteration 335744: with minibatch training loss = 0.00628 and accuracy of 1
Iteration 335872: with minibatch training loss = 0.00344 and accuracy of 1
Iteration 336000: with minibatch training loss = 0.0694 and accuracy of 0.99
Iteration 336128: with minibatch training loss = 0.00513 and accuracy of 1
Epoch 81, Train loss: 0.0198 and Train accuracy of 0.995, Test loss: 0.174 and Test accuracy of 0.967
Iteration 336256: with minibatch training loss = 0.00145 and accuracy of 1
Iteration 336384: with minibatch training loss = 0.014 and accuracy of 0.99
Iteration 336512: with minibatch training loss = 0.00195 and accuracy of 1
Iteration 336640: with minibatch training loss = 0.00569 and accuracy of 1
Iteration 336768: with minibatch training loss = 0.029 and accuracy of 0.98
Iteration 336896: with minibatch training loss = 0.00324 and accuracy of 1
Iteration 337024: with minibatch training loss = 0.0056 and accuracy of 1
Iteration 337152: with minibatch training loss = 0.0584 and accuracy of 0.98
Iteration 337280: with minibatch training loss = 0.0657 and accuracy of 0.98
Iteration 337408: with minibatch training loss = 0.00856 and accuracy of 0.99
Iteration 337536: with minibatch training loss = 0.0357 and accuracy of 0.99
Iteration 337664: with minibatch training loss = 0.00597 and accuracy of 1
Iteration 337792: with minibatch training loss = 0.0564 and accuracy of 0.98
Iteration 337920: with minibatch training loss = 0.00648 and accuracy of 1
Iteration 338048: with minibatch training loss = 0.00893 and accuracy of 1
Iteration 338176: with minibatch training loss = 0.00354 and accuracy of 1
Iteration 338304: with minibatch training loss = 0.00464 and accuracy of 1
Iteration 338432: with minibatch training loss = 0.00499 and accuracy of 1
Iteration 338560: with minibatch training loss = 0.0413 and accuracy of 0.99
Iteration 338688: with minibatch training loss = 0.0221 and accuracy of 0.99
Iteration 338816: with minibatch training loss = 0.0142 and accuracy of 0.99
Iteration 338944: with minibatch training loss = 0.0353 and accuracy of 0.98
Iteration 339072: with minibatch training loss = 0.00414 and accuracy of 1
Iteration 339200: with minibatch training loss = 0.0486 and accuracy of 0.99
Iteration 339328: with minibatch training loss = 0.00214 and accuracy of 1
Iteration 339456: with minibatch training loss = 0.0096 and accuracy of 1
Iteration 339584: with minibatch training loss = 0.0574 and accuracy of 0.99
Iteration 339712: with minibatch training loss = 0.00819 and accuracy of 1
Iteration 339840: with minibatch training loss = 0.0411 and accuracy of 0.99
Iteration 339968: with minibatch training loss = 0.00345 and accuracy of 1
Iteration 340096: with minibatch training loss = 0.00705 and accuracy of 1
Iteration 340224: with minibatch training loss = 0.0273 and accuracy of 0.99
Epoch 82, Train loss: 0.0196 and Train accuracy of 0.995, Test loss: 0.168 and Test accuracy of 0.968
Iteration 340352: with minibatch training loss = 0.00214 and accuracy of 1
Iteration 340480: with minibatch training loss = 0.0273 and accuracy of 0.98
Iteration 340608: with minibatch training loss = 0.0253 and accuracy of 0.98
Iteration 340736: with minibatch training loss = 0.0102 and accuracy of 1
Iteration 340864: with minibatch training loss = 0.0911 and accuracy of 0.98
Iteration 340992: with minibatch training loss = 0.00249 and accuracy of 1
Iteration 341120: with minibatch training loss = 0.00374 and accuracy of 1
Iteration 341248: with minibatch training loss = 0.017 and accuracy of 0.99
Iteration 341376: with minibatch training loss = 0.00751 and accuracy of 1
Iteration 341504: with minibatch training loss = 0.0234 and accuracy of 0.99
Iteration 341632: with minibatch training loss = 0.00988 and accuracy of 0.99
Iteration 341760: with minibatch training loss = 0.00224 and accuracy of 1
Iteration 341888: with minibatch training loss = 0.00328 and accuracy of 1
Iteration 342016: with minibatch training loss = 0.0048 and accuracy of 1
Iteration 342144: with minibatch training loss = 0.00318 and accuracy of 1
Iteration 342272: with minibatch training loss = 0.0975 and accuracy of 0.98
Iteration 342400: with minibatch training loss = 0.0219 and accuracy of 0.99
Iteration 342528: with minibatch training loss = 0.00229 and accuracy of 1
Iteration 342656: with minibatch training loss = 0.00698 and accuracy of 1
Iteration 342784: with minibatch training loss = 0.0168 and accuracy of 0.99
Iteration 342912: with minibatch training loss = 0.00462 and accuracy of 1
Iteration 343040: with minibatch training loss = 0.0045 and accuracy of 1
Iteration 343168: with minibatch training loss = 0.00395 and accuracy of 1
Iteration 343296: with minibatch training loss = 0.0108 and accuracy of 1
Iteration 343424: with minibatch training loss = 0.0774 and accuracy of 0.98
Iteration 343552: with minibatch training loss = 0.00427 and accuracy of 1
Iteration 343680: with minibatch training loss = 0.00771 and accuracy of 1
Iteration 343808: with minibatch training loss = 0.0171 and accuracy of 0.99
Iteration 343936: with minibatch training loss = 0.0473 and accuracy of 0.98
Iteration 344064: with minibatch training loss = 0.00229 and accuracy of 1
Iteration 344192: with minibatch training loss = 0.0542 and accuracy of 0.99
Iteration 344320: with minibatch training loss = 0.00383 and accuracy of 1
Iteration 344448: with minibatch training loss = 0.0228 and accuracy of 0.99
Epoch 83, Train loss: 0.0195 and Train accuracy of 0.995, Test loss: 0.166 and Test accuracy of 0.968
Iteration 344576: with minibatch training loss = 0.123 and accuracy of 0.98
Iteration 344704: with minibatch training loss = 0.0114 and accuracy of 0.99
Iteration 344832: with minibatch training loss = 0.0389 and accuracy of 0.98
Iteration 344960: with minibatch training loss = 0.0507 and accuracy of 0.99
Iteration 345088: with minibatch training loss = 0.00139 and accuracy of 1
Iteration 345216: with minibatch training loss = 0.102 and accuracy of 0.99
Iteration 345344: with minibatch training loss = 0.00367 and accuracy of 1
Iteration 345472: with minibatch training loss = 0.0051 and accuracy of 1
Iteration 345600: with minibatch training loss = 0.0255 and accuracy of 0.99
Iteration 345728: with minibatch training loss = 0.00417 and accuracy of 1
Iteration 345856: with minibatch training loss = 0.00799 and accuracy of 1
Iteration 345984: with minibatch training loss = 0.0106 and accuracy of 1
Iteration 346112: with minibatch training loss = 0.0008 and accuracy of 1
Iteration 346240: with minibatch training loss = 0.00186 and accuracy of 1
Iteration 346368: with minibatch training loss = 0.00158 and accuracy of 1
Iteration 346496: with minibatch training loss = 0.00317 and accuracy of 1
Iteration 346624: with minibatch training loss = 0.0642 and accuracy of 0.98
Iteration 346752: with minibatch training loss = 0.00957 and accuracy of 1
Iteration 346880: with minibatch training loss = 0.0507 and accuracy of 0.98
Iteration 347008: with minibatch training loss = 0.0109 and accuracy of 0.99
Iteration 347136: with minibatch training loss = 0.0271 and accuracy of 0.99
Iteration 347264: with minibatch training loss = 0.00123 and accuracy of 1
Iteration 347392: with minibatch training loss = 0.0247 and accuracy of 0.99
Iteration 347520: with minibatch training loss = 0.0417 and accuracy of 0.98
Iteration 347648: with minibatch training loss = 0.00123 and accuracy of 1
Iteration 347776: with minibatch training loss = 0.0885 and accuracy of 0.99
Iteration 347904: with minibatch training loss = 0.00671 and accuracy of 1
Iteration 348032: with minibatch training loss = 0.00282 and accuracy of 1
Iteration 348160: with minibatch training loss = 0.00408 and accuracy of 1
Iteration 348288: with minibatch training loss = 0.00189 and accuracy of 1
Iteration 348416: with minibatch training loss = 0.0415 and accuracy of 0.98
Iteration 348544: with minibatch training loss = 0.0106 and accuracy of 1
Epoch 84, Train loss: 0.0191 and Train accuracy of 0.995, Test loss: 0.167 and Test accuracy of 0.967
Iteration 348672: with minibatch training loss = 0.00141 and accuracy of 1
Iteration 348800: with minibatch training loss = 0.0203 and accuracy of 0.99
Iteration 348928: with minibatch training loss = 0.0933 and accuracy of 0.98
Iteration 349056: with minibatch training loss = 0.00265 and accuracy of 1
Iteration 349184: with minibatch training loss = 0.0147 and accuracy of 0.99
Iteration 349312: with minibatch training loss = 0.00176 and accuracy of 1
Iteration 349440: with minibatch training loss = 0.0339 and accuracy of 0.99
Iteration 349568: with minibatch training loss = 0.0122 and accuracy of 0.99
Iteration 349696: with minibatch training loss = 0.0085 and accuracy of 0.99
Iteration 349824: with minibatch training loss = 0.00121 and accuracy of 1
Iteration 349952: with minibatch training loss = 0.00245 and accuracy of 1
Iteration 350080: with minibatch training loss = 0.00113 and accuracy of 1
Iteration 350208: with minibatch training loss = 0.00387 and accuracy of 1
Iteration 350336: with minibatch training loss = 0.00457 and accuracy of 1
Iteration 350464: with minibatch training loss = 0.00196 and accuracy of 1
Iteration 350592: with minibatch training loss = 0.0422 and accuracy of 0.99
Iteration 350720: with minibatch training loss = 0.03 and accuracy of 0.98
Iteration 350848: with minibatch training loss = 0.00355 and accuracy of 1
Iteration 350976: with minibatch training loss = 0.0597 and accuracy of 0.99
Iteration 351104: with minibatch training loss = 0.00737 and accuracy of 1
Iteration 351232: with minibatch training loss = 0.00146 and accuracy of 1
Iteration 351360: with minibatch training loss = 0.00818 and accuracy of 1
Iteration 351488: with minibatch training loss = 0.00468 and accuracy of 1
Iteration 351616: with minibatch training loss = 0.0183 and accuracy of 0.99
Iteration 351744: with minibatch training loss = 0.0196 and accuracy of 0.99
Iteration 351872: with minibatch training loss = 0.00879 and accuracy of 0.99
Iteration 352000: with minibatch training loss = 0.000923 and accuracy of 1
Iteration 352128: with minibatch training loss = 0.0137 and accuracy of 0.99
Iteration 352256: with minibatch training loss = 0.00475 and accuracy of 1
Iteration 352384: with minibatch training loss = 0.0102 and accuracy of 1
Iteration 352512: with minibatch training loss = 0.0135 and accuracy of 0.99
Iteration 352640: with minibatch training loss = 0.00393 and accuracy of 1
Epoch 85, Train loss: 0.0187 and Train accuracy of 0.995, Test loss: 0.172 and Test accuracy of 0.966
Iteration 352768: with minibatch training loss = 0.05 and accuracy of 0.98
Iteration 352896: with minibatch training loss = 0.0138 and accuracy of 0.99
Iteration 353024: with minibatch training loss = 0.00431 and accuracy of 1
Iteration 353152: with minibatch training loss = 0.0149 and accuracy of 0.99
Iteration 353280: with minibatch training loss = 0.0034 and accuracy of 1
Iteration 353408: with minibatch training loss = 0.00219 and accuracy of 1
Iteration 353536: with minibatch training loss = 0.038 and accuracy of 0.99
Iteration 353664: with minibatch training loss = 0.00204 and accuracy of 1
Iteration 353792: with minibatch training loss = 0.00718 and accuracy of 1
Iteration 353920: with minibatch training loss = 0.00369 and accuracy of 1
Iteration 354048: with minibatch training loss = 0.00595 and accuracy of 1
Iteration 354176: with minibatch training loss = 0.011 and accuracy of 0.99
Iteration 354304: with minibatch training loss = 0.00808 and accuracy of 1
Iteration 354432: with minibatch training loss = 0.00832 and accuracy of 1
Iteration 354560: with minibatch training loss = 0.121 and accuracy of 0.98
Iteration 354688: with minibatch training loss = 0.0245 and accuracy of 0.98
Iteration 354816: with minibatch training loss = 0.0687 and accuracy of 0.98
Iteration 354944: with minibatch training loss = 0.0284 and accuracy of 0.98
Iteration 355072: with minibatch training loss = 0.00187 and accuracy of 1
Iteration 355200: with minibatch training loss = 0.0018 and accuracy of 1
Iteration 355328: with minibatch training loss = 0.0181 and accuracy of 0.99
Iteration 355456: with minibatch training loss = 0.0388 and accuracy of 0.99
Iteration 355584: with minibatch training loss = 0.0193 and accuracy of 0.99
Iteration 355712: with minibatch training loss = 0.059 and accuracy of 0.98
Iteration 355840: with minibatch training loss = 0.0222 and accuracy of 0.99
Iteration 355968: with minibatch training loss = 0.0804 and accuracy of 0.98
Iteration 356096: with minibatch training loss = 0.00403 and accuracy of 1
Iteration 356224: with minibatch training loss = 0.00619 and accuracy of 1
Iteration 356352: with minibatch training loss = 0.02 and accuracy of 0.99
Iteration 356480: with minibatch training loss = 0.0268 and accuracy of 0.99
Iteration 356608: with minibatch training loss = 0.00449 and accuracy of 1
Iteration 356736: with minibatch training loss = 0.00129 and accuracy of 1
Iteration 356864: with minibatch training loss = 0.0286 and accuracy of 0.98
Epoch 86, Train loss: 0.0188 and Train accuracy of 0.995, Test loss: 0.169 and Test accuracy of 0.968
Iteration 356992: with minibatch training loss = 0.00336 and accuracy of 1
Iteration 357120: with minibatch training loss = 0.0147 and accuracy of 0.99
Iteration 357248: with minibatch training loss = 0.011 and accuracy of 1
Iteration 357376: with minibatch training loss = 0.022 and accuracy of 0.99
Iteration 357504: with minibatch training loss = 0.0318 and accuracy of 0.99
Iteration 357632: with minibatch training loss = 0.00329 and accuracy of 1
Iteration 357760: with minibatch training loss = 0.0183 and accuracy of 0.99
Iteration 357888: with minibatch training loss = 0.0132 and accuracy of 0.99
Iteration 358016: with minibatch training loss = 0.00237 and accuracy of 1
Iteration 358144: with minibatch training loss = 0.00656 and accuracy of 1
Iteration 358272: with minibatch training loss = 0.0022 and accuracy of 1
Iteration 358400: with minibatch training loss = 0.021 and accuracy of 0.98
Iteration 358528: with minibatch training loss = 0.0428 and accuracy of 0.99
Iteration 358656: with minibatch training loss = 0.002 and accuracy of 1
Iteration 358784: with minibatch training loss = 0.0102 and accuracy of 1
Iteration 358912: with minibatch training loss = 0.0135 and accuracy of 0.99
Iteration 359040: with minibatch training loss = 0.0079 and accuracy of 0.99
Iteration 359168: with minibatch training loss = 0.00998 and accuracy of 1
Iteration 359296: with minibatch training loss = 0.0182 and accuracy of 0.99
Iteration 359424: with minibatch training loss = 0.00669 and accuracy of 1
Iteration 359552: with minibatch training loss = 0.01 and accuracy of 1
Iteration 359680: with minibatch training loss = 0.0168 and accuracy of 0.99
Iteration 359808: with minibatch training loss = 0.00147 and accuracy of 1
Iteration 359936: with minibatch training loss = 0.00644 and accuracy of 1
Iteration 360064: with minibatch training loss = 0.00458 and accuracy of 1
Iteration 360192: with minibatch training loss = 0.011 and accuracy of 1
Iteration 360320: with minibatch training loss = 0.0161 and accuracy of 0.99
Iteration 360448: with minibatch training loss = 0.0309 and accuracy of 0.99
Iteration 360576: with minibatch training loss = 0.00924 and accuracy of 0.99
Iteration 360704: with minibatch training loss = 0.00761 and accuracy of 1
Iteration 360832: with minibatch training loss = 0.00225 and accuracy of 1
Iteration 360960: with minibatch training loss = 0.0494 and accuracy of 0.99
Epoch 87, Train loss: 0.0186 and Train accuracy of 0.995, Test loss: 0.17 and Test accuracy of 0.968
Iteration 361088: with minibatch training loss = 0.0492 and accuracy of 0.98
Iteration 361216: with minibatch training loss = 0.0106 and accuracy of 0.99
Iteration 361344: with minibatch training loss = 0.00668 and accuracy of 1
Iteration 361472: with minibatch training loss = 0.007 and accuracy of 1
Iteration 361600: with minibatch training loss = 0.0379 and accuracy of 0.99
Iteration 361728: with minibatch training loss = 0.0284 and accuracy of 0.98
Iteration 361856: with minibatch training loss = 0.0261 and accuracy of 0.98
Iteration 361984: with minibatch training loss = 0.0224 and accuracy of 0.99
Iteration 362112: with minibatch training loss = 0.0178 and accuracy of 0.99
Iteration 362240: with minibatch training loss = 0.0403 and accuracy of 0.99
Iteration 362368: with minibatch training loss = 0.0306 and accuracy of 0.98
Iteration 362496: with minibatch training loss = 0.0209 and accuracy of 0.99
Iteration 362624: with minibatch training loss = 0.00329 and accuracy of 1
Iteration 362752: with minibatch training loss = 0.0015 and accuracy of 1
Iteration 362880: with minibatch training loss = 0.0148 and accuracy of 0.99
Iteration 363008: with minibatch training loss = 0.00331 and accuracy of 1
Iteration 363136: with minibatch training loss = 0.00177 and accuracy of 1
Iteration 363264: with minibatch training loss = 0.0201 and accuracy of 0.99
Iteration 363392: with minibatch training loss = 0.00501 and accuracy of 1
Iteration 363520: with minibatch training loss = 0.0515 and accuracy of 0.98
Iteration 363648: with minibatch training loss = 0.00284 and accuracy of 1
Iteration 363776: with minibatch training loss = 0.0172 and accuracy of 0.99
Iteration 363904: with minibatch training loss = 0.0171 and accuracy of 0.99
Iteration 364032: with minibatch training loss = 0.0136 and accuracy of 1
Iteration 364160: with minibatch training loss = 0.0038 and accuracy of 1
Iteration 364288: with minibatch training loss = 0.00406 and accuracy of 1
Iteration 364416: with minibatch training loss = 0.0805 and accuracy of 0.99
Iteration 364544: with minibatch training loss = 0.0127 and accuracy of 0.99
Iteration 364672: with minibatch training loss = 0.0611 and accuracy of 0.98
Iteration 364800: with minibatch training loss = 0.0121 and accuracy of 1
Iteration 364928: with minibatch training loss = 0.0203 and accuracy of 0.99
Iteration 365056: with minibatch training loss = 0.00286 and accuracy of 1
Iteration 365184: with minibatch training loss = 0.00473 and accuracy of 1
Epoch 88, Train loss: 0.0186 and Train accuracy of 0.995, Test loss: 0.168 and Test accuracy of 0.968
Iteration 365312: with minibatch training loss = 0.00385 and accuracy of 1
Iteration 365440: with minibatch training loss = 0.0139 and accuracy of 0.99
Iteration 365568: with minibatch training loss = 0.0365 and accuracy of 0.98
Iteration 365696: with minibatch training loss = 0.00137 and accuracy of 1
Iteration 365824: with minibatch training loss = 0.0022 and accuracy of 1
Iteration 365952: with minibatch training loss = 0.0045 and accuracy of 1
Iteration 366080: with minibatch training loss = 0.0176 and accuracy of 0.99
Iteration 366208: with minibatch training loss = 0.00732 and accuracy of 1
Iteration 366336: with minibatch training loss = 0.00678 and accuracy of 1
Iteration 366464: with minibatch training loss = 0.00152 and accuracy of 1
Iteration 366592: with minibatch training loss = 0.0043 and accuracy of 1
Iteration 366720: with minibatch training loss = 0.00638 and accuracy of 1
Iteration 366848: with minibatch training loss = 0.00633 and accuracy of 1
Iteration 366976: with minibatch training loss = 0.0212 and accuracy of 0.99
Iteration 367104: with minibatch training loss = 0.00114 and accuracy of 1
Iteration 367232: with minibatch training loss = 0.0181 and accuracy of 0.99
Iteration 367360: with minibatch training loss = 0.025 and accuracy of 0.98
Iteration 367488: with minibatch training loss = 0.0422 and accuracy of 0.98
Iteration 367616: with minibatch training loss = 0.0404 and accuracy of 0.99
Iteration 367744: with minibatch training loss = 0.0027 and accuracy of 1
Iteration 367872: with minibatch training loss = 0.00251 and accuracy of 1
Iteration 368000: with minibatch training loss = 0.00383 and accuracy of 1
Iteration 368128: with minibatch training loss = 0.0772 and accuracy of 0.99
Iteration 368256: with minibatch training loss = 0.00249 and accuracy of 1
Iteration 368384: with minibatch training loss = 0.0637 and accuracy of 0.98
Iteration 368512: with minibatch training loss = 0.00153 and accuracy of 1
Iteration 368640: with minibatch training loss = 0.0818 and accuracy of 0.98
Iteration 368768: with minibatch training loss = 0.0274 and accuracy of 0.98
Iteration 368896: with minibatch training loss = 0.0284 and accuracy of 0.98
Iteration 369024: with minibatch training loss = 0.00455 and accuracy of 1
Iteration 369152: with minibatch training loss = 0.0443 and accuracy of 0.99
Iteration 369280: with minibatch training loss = 0.00404 and accuracy of 1
Epoch 89, Train loss: 0.0182 and Train accuracy of 0.995, Test loss: 0.17 and Test accuracy of 0.969
Iteration 369408: with minibatch training loss = 0.00363 and accuracy of 1
Iteration 369536: with minibatch training loss = 0.00281 and accuracy of 1
Iteration 369664: with minibatch training loss = 0.0179 and accuracy of 0.99
Iteration 369792: with minibatch training loss = 0.0192 and accuracy of 0.99
Iteration 369920: with minibatch training loss = 0.000712 and accuracy of 1
Iteration 370048: with minibatch training loss = 0.0752 and accuracy of 0.99
Iteration 370176: with minibatch training loss = 0.00836 and accuracy of 1
Iteration 370304: with minibatch training loss = 0.00436 and accuracy of 1
Iteration 370432: with minibatch training loss = 0.0106 and accuracy of 0.99
Iteration 370560: with minibatch training loss = 0.00869 and accuracy of 0.99
Iteration 370688: with minibatch training loss = 0.113 and accuracy of 0.98
Iteration 370816: with minibatch training loss = 0.0321 and accuracy of 0.98
Iteration 370944: with minibatch training loss = 0.00526 and accuracy of 1
Iteration 371072: with minibatch training loss = 0.00663 and accuracy of 1
Iteration 371200: with minibatch training loss = 0.0422 and accuracy of 0.98
Iteration 371328: with minibatch training loss = 0.00127 and accuracy of 1
Iteration 371456: with minibatch training loss = 0.000889 and accuracy of 1
Iteration 371584: with minibatch training loss = 0.0204 and accuracy of 0.99
Iteration 371712: with minibatch training loss = 0.00548 and accuracy of 1
Iteration 371840: with minibatch training loss = 0.00307 and accuracy of 1
Iteration 371968: with minibatch training loss = 0.0104 and accuracy of 0.99
Iteration 372096: with minibatch training loss = 0.00278 and accuracy of 1
Iteration 372224: with minibatch training loss = 0.0239 and accuracy of 0.98
Iteration 372352: with minibatch training loss = 0.00967 and accuracy of 1
Iteration 372480: with minibatch training loss = 0.00431 and accuracy of 1
Iteration 372608: with minibatch training loss = 0.0308 and accuracy of 0.99
Iteration 372736: with minibatch training loss = 0.0435 and accuracy of 0.99
Iteration 372864: with minibatch training loss = 0.00753 and accuracy of 1
Iteration 372992: with minibatch training loss = 0.00108 and accuracy of 1
Iteration 373120: with minibatch training loss = 0.004 and accuracy of 1
Iteration 373248: with minibatch training loss = 0.00918 and accuracy of 0.99
Iteration 373376: with minibatch training loss = 0.0115 and accuracy of 0.99
Epoch 90, Train loss: 0.0178 and Train accuracy of 0.995, Test loss: 0.175 and Test accuracy of 0.968
Iteration 373504: with minibatch training loss = 0.00504 and accuracy of 1
Iteration 373632: with minibatch training loss = 0.00745 and accuracy of 0.99
Iteration 373760: with minibatch training loss = 0.0146 and accuracy of 0.99
Iteration 373888: with minibatch training loss = 0.0206 and accuracy of 0.99
Iteration 374016: with minibatch training loss = 0.0106 and accuracy of 0.99
Iteration 374144: with minibatch training loss = 0.0591 and accuracy of 0.98
Iteration 374272: with minibatch training loss = 0.0563 and accuracy of 0.98
Iteration 374400: with minibatch training loss = 0.00188 and accuracy of 1
Iteration 374528: with minibatch training loss = 0.00377 and accuracy of 1
Iteration 374656: with minibatch training loss = 0.00221 and accuracy of 1
Iteration 374784: with minibatch training loss = 0.00392 and accuracy of 1
Iteration 374912: with minibatch training loss = 0.0126 and accuracy of 0.99
Iteration 375040: with minibatch training loss = 0.00372 and accuracy of 1
Iteration 375168: with minibatch training loss = 0.0128 and accuracy of 1
Iteration 375296: with minibatch training loss = 0.0189 and accuracy of 0.99
Iteration 375424: with minibatch training loss = 0.00491 and accuracy of 1
Iteration 375552: with minibatch training loss = 0.00155 and accuracy of 1
Iteration 375680: with minibatch training loss = 0.0021 and accuracy of 1
Iteration 375808: with minibatch training loss = 0.00251 and accuracy of 1
Iteration 375936: with minibatch training loss = 0.00667 and accuracy of 1
Iteration 376064: with minibatch training loss = 0.0529 and accuracy of 0.99
Iteration 376192: with minibatch training loss = 0.0175 and accuracy of 0.99
Iteration 376320: with minibatch training loss = 0.0244 and accuracy of 0.99
Iteration 376448: with minibatch training loss = 0.00263 and accuracy of 1
Iteration 376576: with minibatch training loss = 0.0388 and accuracy of 0.99
Iteration 376704: with minibatch training loss = 0.00456 and accuracy of 1
Iteration 376832: with minibatch training loss = 0.0119 and accuracy of 0.99
Iteration 376960: with minibatch training loss = 0.00236 and accuracy of 1
Iteration 377088: with minibatch training loss = 0.0678 and accuracy of 0.98
Iteration 377216: with minibatch training loss = 0.0413 and accuracy of 0.98
Iteration 377344: with minibatch training loss = 0.0181 and accuracy of 0.99
Iteration 377472: with minibatch training loss = 0.0428 and accuracy of 0.99
Iteration 377600: with minibatch training loss = 0.0325 and accuracy of 0.99
Epoch 91, Train loss: 0.0177 and Train accuracy of 0.995, Test loss: 0.177 and Test accuracy of 0.967
Iteration 377728: with minibatch training loss = 0.00171 and accuracy of 1
Iteration 377856: with minibatch training loss = 0.00685 and accuracy of 1
Iteration 377984: with minibatch training loss = 0.00832 and accuracy of 0.99
Iteration 378112: with minibatch training loss = 0.00998 and accuracy of 1
Iteration 378240: with minibatch training loss = 0.00659 and accuracy of 1
Iteration 378368: with minibatch training loss = 0.00852 and accuracy of 1
Iteration 378496: with minibatch training loss = 0.0135 and accuracy of 0.99
Iteration 378624: with minibatch training loss = 0.0864 and accuracy of 0.98
Iteration 378752: with minibatch training loss = 0.0245 and accuracy of 0.99
Iteration 378880: with minibatch training loss = 0.00858 and accuracy of 0.99
Iteration 379008: with minibatch training loss = 0.0877 and accuracy of 0.98
Iteration 379136: with minibatch training loss = 0.0454 and accuracy of 0.98
Iteration 379264: with minibatch training loss = 0.00565 and accuracy of 1
Iteration 379392: with minibatch training loss = 0.00496 and accuracy of 1
Iteration 379520: with minibatch training loss = 0.00173 and accuracy of 1
Iteration 379648: with minibatch training loss = 0.0114 and accuracy of 0.99
Iteration 379776: with minibatch training loss = 0.0232 and accuracy of 0.99
Iteration 379904: with minibatch training loss = 0.0149 and accuracy of 0.99
Iteration 380032: with minibatch training loss = 0.00429 and accuracy of 1
Iteration 380160: with minibatch training loss = 0.0452 and accuracy of 0.99
Iteration 380288: with minibatch training loss = 0.00362 and accuracy of 1
Iteration 380416: with minibatch training loss = 0.00463 and accuracy of 1
Iteration 380544: with minibatch training loss = 0.0323 and accuracy of 0.99
Iteration 380672: with minibatch training loss = 0.00923 and accuracy of 0.99
Iteration 380800: with minibatch training loss = 0.00842 and accuracy of 0.99
Iteration 380928: with minibatch training loss = 0.0274 and accuracy of 0.99
Iteration 381056: with minibatch training loss = 0.00258 and accuracy of 1
Iteration 381184: with minibatch training loss = 0.015 and accuracy of 0.99
Iteration 381312: with minibatch training loss = 0.0197 and accuracy of 0.98
Iteration 381440: with minibatch training loss = 0.00398 and accuracy of 1
Iteration 381568: with minibatch training loss = 0.00397 and accuracy of 1
Iteration 381696: with minibatch training loss = 0.0679 and accuracy of 0.99
Epoch 92, Train loss: 0.018 and Train accuracy of 0.995, Test loss: 0.171 and Test accuracy of 0.968
Iteration 381824: with minibatch training loss = 0.0242 and accuracy of 0.98
Iteration 381952: with minibatch training loss = 0.00169 and accuracy of 1
Iteration 382080: with minibatch training loss = 0.00416 and accuracy of 1
Iteration 382208: with minibatch training loss = 0.012 and accuracy of 1
Iteration 382336: with minibatch training loss = 0.0026 and accuracy of 1
Iteration 382464: with minibatch training loss = 0.00356 and accuracy of 1
Iteration 382592: with minibatch training loss = 0.00831 and accuracy of 0.99
Iteration 382720: with minibatch training loss = 0.00212 and accuracy of 1
Iteration 382848: with minibatch training loss = 0.0015 and accuracy of 1
Iteration 382976: with minibatch training loss = 0.00236 and accuracy of 1
Iteration 383104: with minibatch training loss = 0.0313 and accuracy of 0.99
Iteration 383232: with minibatch training loss = 0.00217 and accuracy of 1
Iteration 383360: with minibatch training loss = 0.0322 and accuracy of 0.99
Iteration 383488: with minibatch training loss = 0.000976 and accuracy of 1
Iteration 383616: with minibatch training loss = 0.0217 and accuracy of 0.99
Iteration 383744: with minibatch training loss = 0.0511 and accuracy of 0.99
Iteration 383872: with minibatch training loss = 0.00158 and accuracy of 1
Iteration 384000: with minibatch training loss = 0.0477 and accuracy of 0.98
Iteration 384128: with minibatch training loss = 0.0063 and accuracy of 1
Iteration 384256: with minibatch training loss = 0.0663 and accuracy of 0.99
Iteration 384384: with minibatch training loss = 0.013 and accuracy of 0.99
Iteration 384512: with minibatch training loss = 0.0485 and accuracy of 0.98
Iteration 384640: with minibatch training loss = 0.0507 and accuracy of 0.99
Iteration 384768: with minibatch training loss = 0.0012 and accuracy of 1
Iteration 384896: with minibatch training loss = 0.00295 and accuracy of 1
Iteration 385024: with minibatch training loss = 0.0219 and accuracy of 0.99
Iteration 385152: with minibatch training loss = 0.00533 and accuracy of 1
Iteration 385280: with minibatch training loss = 0.0247 and accuracy of 0.99
Iteration 385408: with minibatch training loss = 0.0933 and accuracy of 0.97
Iteration 385536: with minibatch training loss = 0.0133 and accuracy of 0.99
Iteration 385664: with minibatch training loss = 0.0788 and accuracy of 0.99
Iteration 385792: with minibatch training loss = 0.000896 and accuracy of 1
Iteration 385920: with minibatch training loss = 0.0105 and accuracy of 0.99
Epoch 93, Train loss: 0.0175 and Train accuracy of 0.995, Test loss: 0.17 and Test accuracy of 0.968
Iteration 386048: with minibatch training loss = 0.00221 and accuracy of 1
Iteration 386176: with minibatch training loss = 0.00106 and accuracy of 1
Iteration 386304: with minibatch training loss = 0.0189 and accuracy of 0.99
Iteration 386432: with minibatch training loss = 0.0575 and accuracy of 0.97
Iteration 386560: with minibatch training loss = 0.0246 and accuracy of 0.99
Iteration 386688: with minibatch training loss = 0.0226 and accuracy of 0.99
Iteration 386816: with minibatch training loss = 0.0241 and accuracy of 0.98
Iteration 386944: with minibatch training loss = 0.000843 and accuracy of 1
Iteration 387072: with minibatch training loss = 0.000978 and accuracy of 1
Iteration 387200: with minibatch training loss = 0.00135 and accuracy of 1
Iteration 387328: with minibatch training loss = 0.0123 and accuracy of 1
Iteration 387456: with minibatch training loss = 0.0292 and accuracy of 0.99
Iteration 387584: with minibatch training loss = 0.00658 and accuracy of 1
Iteration 387712: with minibatch training loss = 0.00563 and accuracy of 1
Iteration 387840: with minibatch training loss = 0.00222 and accuracy of 1
Iteration 387968: with minibatch training loss = 0.000485 and accuracy of 1
Iteration 388096: with minibatch training loss = 0.0179 and accuracy of 0.99
Iteration 388224: with minibatch training loss = 0.0246 and accuracy of 0.99
Iteration 388352: with minibatch training loss = 0.00152 and accuracy of 1
Iteration 388480: with minibatch training loss = 0.0321 and accuracy of 0.99
Iteration 388608: with minibatch training loss = 0.0483 and accuracy of 0.98
Iteration 388736: with minibatch training loss = 0.00113 and accuracy of 1
Iteration 388864: with minibatch training loss = 0.0026 and accuracy of 1
Iteration 388992: with minibatch training loss = 0.0399 and accuracy of 0.99
Iteration 389120: with minibatch training loss = 0.00165 and accuracy of 1
Iteration 389248: with minibatch training loss = 0.035 and accuracy of 0.98
Iteration 389376: with minibatch training loss = 0.00317 and accuracy of 1
Iteration 389504: with minibatch training loss = 0.0303 and accuracy of 0.98
Iteration 389632: with minibatch training loss = 0.000978 and accuracy of 1
Iteration 389760: with minibatch training loss = 0.027 and accuracy of 0.99
Iteration 389888: with minibatch training loss = 0.00138 and accuracy of 1
Iteration 390016: with minibatch training loss = 0.00307 and accuracy of 1
Epoch 94, Train loss: 0.0176 and Train accuracy of 0.995, Test loss: 0.171 and Test accuracy of 0.967
Iteration 390144: with minibatch training loss = 0.0462 and accuracy of 0.98
Iteration 390272: with minibatch training loss = 0.00203 and accuracy of 1
Iteration 390400: with minibatch training loss = 0.00243 and accuracy of 1
Iteration 390528: with minibatch training loss = 0.00204 and accuracy of 1
Iteration 390656: with minibatch training loss = 0.00119 and accuracy of 1
Iteration 390784: with minibatch training loss = 0.0138 and accuracy of 0.99
Iteration 390912: with minibatch training loss = 0.00153 and accuracy of 1
Iteration 391040: with minibatch training loss = 0.00612 and accuracy of 1
Iteration 391168: with minibatch training loss = 0.0193 and accuracy of 0.99
Iteration 391296: with minibatch training loss = 0.00286 and accuracy of 1
Iteration 391424: with minibatch training loss = 0.00161 and accuracy of 1
Iteration 391552: with minibatch training loss = 0.00796 and accuracy of 0.99
Iteration 391680: with minibatch training loss = 0.106 and accuracy of 0.97
Iteration 391808: with minibatch training loss = 0.0974 and accuracy of 0.98
Iteration 391936: with minibatch training loss = 0.00255 and accuracy of 1
Iteration 392064: with minibatch training loss = 0.000635 and accuracy of 1
Iteration 392192: with minibatch training loss = 0.0154 and accuracy of 1
Iteration 392320: with minibatch training loss = 0.0176 and accuracy of 0.99
Iteration 392448: with minibatch training loss = 0.0263 and accuracy of 0.99
Iteration 392576: with minibatch training loss = 0.00188 and accuracy of 1
Iteration 392704: with minibatch training loss = 0.00817 and accuracy of 1
Iteration 392832: with minibatch training loss = 0.0111 and accuracy of 1
Iteration 392960: with minibatch training loss = 0.0221 and accuracy of 0.99
Iteration 393088: with minibatch training loss = 0.0355 and accuracy of 0.98
Iteration 393216: with minibatch training loss = 0.0011 and accuracy of 1
Iteration 393344: with minibatch training loss = 0.0312 and accuracy of 0.98
Iteration 393472: with minibatch training loss = 0.00324 and accuracy of 1
Iteration 393600: with minibatch training loss = 0.00678 and accuracy of 1
Iteration 393728: with minibatch training loss = 0.00289 and accuracy of 1
Iteration 393856: with minibatch training loss = 0.00285 and accuracy of 1
Iteration 393984: with minibatch training loss = 0.00838 and accuracy of 1
Iteration 394112: with minibatch training loss = 0.0015 and accuracy of 1
Iteration 394240: with minibatch training loss = 0.000812 and accuracy of 1
Epoch 95, Train loss: 0.0175 and Train accuracy of 0.995, Test loss: 0.174 and Test accuracy of 0.967
Iteration 394368: with minibatch training loss = 0.00212 and accuracy of 1
Iteration 394496: with minibatch training loss = 0.0359 and accuracy of 0.98
Iteration 394624: with minibatch training loss = 0.00233 and accuracy of 1
Iteration 394752: with minibatch training loss = 0.00348 and accuracy of 1
Iteration 394880: with minibatch training loss = 0.00201 and accuracy of 1
Iteration 395008: with minibatch training loss = 0.0299 and accuracy of 0.98
Iteration 395136: with minibatch training loss = 0.000945 and accuracy of 1
Iteration 395264: with minibatch training loss = 0.00315 and accuracy of 1
Iteration 395392: with minibatch training loss = 0.03 and accuracy of 0.99
Iteration 395520: with minibatch training loss = 0.00193 and accuracy of 1
Iteration 395648: with minibatch training loss = 0.0023 and accuracy of 1
Iteration 395776: with minibatch training loss = 0.0273 and accuracy of 0.99
Iteration 395904: with minibatch training loss = 0.0108 and accuracy of 0.99
Iteration 396032: with minibatch training loss = 0.0092 and accuracy of 0.99
Iteration 396160: with minibatch training loss = 0.0052 and accuracy of 1
Iteration 396288: with minibatch training loss = 0.0587 and accuracy of 0.98
Iteration 396416: with minibatch training loss = 0.00771 and accuracy of 1
Iteration 396544: with minibatch training loss = 0.00214 and accuracy of 1
Iteration 396672: with minibatch training loss = 0.00418 and accuracy of 1
Iteration 396800: with minibatch training loss = 0.0158 and accuracy of 0.99
Iteration 396928: with minibatch training loss = 0.0213 and accuracy of 0.99
Iteration 397056: with minibatch training loss = 0.0179 and accuracy of 0.99
Iteration 397184: with minibatch training loss = 0.000868 and accuracy of 1
Iteration 397312: with minibatch training loss = 0.0362 and accuracy of 0.99
Iteration 397440: with minibatch training loss = 0.0219 and accuracy of 0.99
Iteration 397568: with minibatch training loss = 0.00264 and accuracy of 1
Iteration 397696: with minibatch training loss = 0.00606 and accuracy of 1
Iteration 397824: with minibatch training loss = 0.0191 and accuracy of 0.99
Iteration 397952: with minibatch training loss = 0.0893 and accuracy of 0.98
Iteration 398080: with minibatch training loss = 0.0185 and accuracy of 0.99
Iteration 398208: with minibatch training loss = 0.00118 and accuracy of 1
Iteration 398336: with minibatch training loss = 0.03 and accuracy of 0.99
Epoch 96, Train loss: 0.0172 and Train accuracy of 0.995, Test loss: 0.172 and Test accuracy of 0.968
Iteration 398464: with minibatch training loss = 0.00332 and accuracy of 1
Iteration 398592: with minibatch training loss = 0.00519 and accuracy of 1
Iteration 398720: with minibatch training loss = 0.0281 and accuracy of 0.98
Iteration 398848: with minibatch training loss = 0.000557 and accuracy of 1
Iteration 398976: with minibatch training loss = 0.0103 and accuracy of 1
Iteration 399104: with minibatch training loss = 0.00932 and accuracy of 0.99
Iteration 399232: with minibatch training loss = 0.00138 and accuracy of 1
Iteration 399360: with minibatch training loss = 0.00234 and accuracy of 1
Iteration 399488: with minibatch training loss = 0.0205 and accuracy of 0.99
Iteration 399616: with minibatch training loss = 0.0025 and accuracy of 1
Iteration 399744: with minibatch training loss = 0.00366 and accuracy of 1
Iteration 399872: with minibatch training loss = 0.0073 and accuracy of 1
Iteration 400000: with minibatch training loss = 0.00225 and accuracy of 1
Iteration 400128: with minibatch training loss = 0.0019 and accuracy of 1
Iteration 400256: with minibatch training loss = 0.0436 and accuracy of 0.98
Iteration 400384: with minibatch training loss = 0.0552 and accuracy of 0.99
Iteration 400512: with minibatch training loss = 0.0193 and accuracy of 0.98
Iteration 400640: with minibatch training loss = 0.001 and accuracy of 1
Iteration 400768: with minibatch training loss = 0.00115 and accuracy of 1
Iteration 400896: with minibatch training loss = 0.000842 and accuracy of 1
Iteration 401024: with minibatch training loss = 0.0459 and accuracy of 0.98
Iteration 401152: with minibatch training loss = 0.00843 and accuracy of 0.99
Iteration 401280: with minibatch training loss = 0.00244 and accuracy of 1
Iteration 401408: with minibatch training loss = 0.00244 and accuracy of 1
Iteration 401536: with minibatch training loss = 0.0131 and accuracy of 0.99
Iteration 401664: with minibatch training loss = 0.0283 and accuracy of 0.98
Iteration 401792: with minibatch training loss = 0.0697 and accuracy of 0.98
Iteration 401920: with minibatch training loss = 0.00462 and accuracy of 1
Iteration 402048: with minibatch training loss = 0.000854 and accuracy of 1
Iteration 402176: with minibatch training loss = 0.00571 and accuracy of 1
Iteration 402304: with minibatch training loss = 0.0459 and accuracy of 0.98
Iteration 402432: with minibatch training loss = 0.0208 and accuracy of 0.99
Epoch 97, Train loss: 0.0169 and Train accuracy of 0.995, Test loss: 0.172 and Test accuracy of 0.969
Iteration 402560: with minibatch training loss = 0.0273 and accuracy of 0.99
Iteration 402688: with minibatch training loss = 0.035 and accuracy of 0.99
Iteration 402816: with minibatch training loss = 0.0045 and accuracy of 1
Iteration 402944: with minibatch training loss = 0.0332 and accuracy of 0.98
Iteration 403072: with minibatch training loss = 0.00135 and accuracy of 1
Iteration 403200: with minibatch training loss = 0.00151 and accuracy of 1
Iteration 403328: with minibatch training loss = 0.0159 and accuracy of 0.99
Iteration 403456: with minibatch training loss = 0.00916 and accuracy of 1
Iteration 403584: with minibatch training loss = 0.0135 and accuracy of 0.99
Iteration 403712: with minibatch training loss = 0.00567 and accuracy of 1
Iteration 403840: with minibatch training loss = 0.0156 and accuracy of 0.99
Iteration 403968: with minibatch training loss = 0.0104 and accuracy of 0.99
Iteration 404096: with minibatch training loss = 0.000853 and accuracy of 1
Iteration 404224: with minibatch training loss = 0.00161 and accuracy of 1
Iteration 404352: with minibatch training loss = 0.0123 and accuracy of 1
Iteration 404480: with minibatch training loss = 0.0164 and accuracy of 0.99
Iteration 404608: with minibatch training loss = 0.00309 and accuracy of 1
Iteration 404736: with minibatch training loss = 0.0597 and accuracy of 0.98
Iteration 404864: with minibatch training loss = 0.0229 and accuracy of 0.99
Iteration 404992: with minibatch training loss = 0.0542 and accuracy of 0.99
Iteration 405120: with minibatch training loss = 0.00308 and accuracy of 1
Iteration 405248: with minibatch training loss = 0.00295 and accuracy of 1
Iteration 405376: with minibatch training loss = 0.00438 and accuracy of 1
Iteration 405504: with minibatch training loss = 0.0398 and accuracy of 0.99
Iteration 405632: with minibatch training loss = 0.0158 and accuracy of 0.99
Iteration 405760: with minibatch training loss = 0.00156 and accuracy of 1
Iteration 405888: with minibatch training loss = 0.0107 and accuracy of 0.99
Iteration 406016: with minibatch training loss = 0.00344 and accuracy of 1
Iteration 406144: with minibatch training loss = 0.0174 and accuracy of 0.99
Iteration 406272: with minibatch training loss = 0.00244 and accuracy of 1
Iteration 406400: with minibatch training loss = 0.00183 and accuracy of 1
Iteration 406528: with minibatch training loss = 0.0038 and accuracy of 1
Iteration 406656: with minibatch training loss = 0.0137 and accuracy of 1
Epoch 98, Train loss: 0.0168 and Train accuracy of 0.995, Test loss: 0.173 and Test accuracy of 0.967
Iteration 406784: with minibatch training loss = 0.0369 and accuracy of 0.99
Iteration 406912: with minibatch training loss = 0.0194 and accuracy of 0.99
Iteration 407040: with minibatch training loss = 0.00132 and accuracy of 1
Iteration 407168: with minibatch training loss = 0.00783 and accuracy of 1
Iteration 407296: with minibatch training loss = 0.0131 and accuracy of 0.99
Iteration 407424: with minibatch training loss = 0.0364 and accuracy of 0.99
Iteration 407552: with minibatch training loss = 0.0109 and accuracy of 0.99
Iteration 407680: with minibatch training loss = 0.00137 and accuracy of 1
Iteration 407808: with minibatch training loss = 0.000973 and accuracy of 1
Iteration 407936: with minibatch training loss = 0.0743 and accuracy of 0.98
Iteration 408064: with minibatch training loss = 0.00267 and accuracy of 1
Iteration 408192: with minibatch training loss = 0.00222 and accuracy of 1
Iteration 408320: with minibatch training loss = 0.0121 and accuracy of 0.99
Iteration 408448: with minibatch training loss = 0.00502 and accuracy of 1
Iteration 408576: with minibatch training loss = 0.00318 and accuracy of 1
Iteration 408704: with minibatch training loss = 0.0128 and accuracy of 0.99
Iteration 408832: with minibatch training loss = 0.00738 and accuracy of 0.99
Iteration 408960: with minibatch training loss = 0.0074 and accuracy of 1
Iteration 409088: with minibatch training loss = 0.00196 and accuracy of 1
Iteration 409216: with minibatch training loss = 0.0171 and accuracy of 0.99
Iteration 409344: with minibatch training loss = 0.0112 and accuracy of 0.99
Iteration 409472: with minibatch training loss = 0.00307 and accuracy of 1
Iteration 409600: with minibatch training loss = 0.00369 and accuracy of 1
Iteration 409728: with minibatch training loss = 0.0423 and accuracy of 0.98
Iteration 409856: with minibatch training loss = 0.0441 and accuracy of 0.98
Iteration 409984: with minibatch training loss = 0.031 and accuracy of 0.99
Iteration 410112: with minibatch training loss = 0.0214 and accuracy of 0.99
Iteration 410240: with minibatch training loss = 0.0578 and accuracy of 0.98
Iteration 410368: with minibatch training loss = 0.0218 and accuracy of 0.99
Iteration 410496: with minibatch training loss = 0.0514 and accuracy of 0.99
Iteration 410624: with minibatch training loss = 0.00542 and accuracy of 1
Iteration 410752: with minibatch training loss = 0.00148 and accuracy of 1
Epoch 99, Train loss: 0.0168 and Train accuracy of 0.995, Test loss: 0.172 and Test accuracy of 0.968
Iteration 410880: with minibatch training loss = 0.0125 and accuracy of 0.99
Iteration 411008: with minibatch training loss = 0.00623 and accuracy of 1
Iteration 411136: with minibatch training loss = 0.0367 and accuracy of 0.99
Iteration 411264: with minibatch training loss = 0.00337 and accuracy of 1
Iteration 411392: with minibatch training loss = 0.0765 and accuracy of 0.98
Iteration 411520: with minibatch training loss = 0.0271 and accuracy of 0.99
Iteration 411648: with minibatch training loss = 0.00528 and accuracy of 1
Iteration 411776: with minibatch training loss = 0.0182 and accuracy of 0.98
Iteration 411904: with minibatch training loss = 0.00377 and accuracy of 1
Iteration 412032: with minibatch training loss = 0.0146 and accuracy of 0.99
Iteration 412160: with minibatch training loss = 0.00252 and accuracy of 1
Iteration 412288: with minibatch training loss = 0.00181 and accuracy of 1
Iteration 412416: with minibatch training loss = 0.0359 and accuracy of 0.99
Iteration 412544: with minibatch training loss = 0.000771 and accuracy of 1
Iteration 412672: with minibatch training loss = 0.0407 and accuracy of 0.98
Iteration 412800: with minibatch training loss = 0.0169 and accuracy of 0.98
Iteration 412928: with minibatch training loss = 0.00169 and accuracy of 1
Iteration 413056: with minibatch training loss = 0.00153 and accuracy of 1
Iteration 413184: with minibatch training loss = 0.00161 and accuracy of 1
Iteration 413312: with minibatch training loss = 0.00945 and accuracy of 0.99
Iteration 413440: with minibatch training loss = 0.00105 and accuracy of 1
Iteration 413568: with minibatch training loss = 0.0154 and accuracy of 1
Iteration 413696: with minibatch training loss = 0.0024 and accuracy of 1
Iteration 413824: with minibatch training loss = 0.00577 and accuracy of 1
Iteration 413952: with minibatch training loss = 0.0105 and accuracy of 0.99
Iteration 414080: with minibatch training loss = 0.00193 and accuracy of 1
Iteration 414208: with minibatch training loss = 0.0042 and accuracy of 1
Iteration 414336: with minibatch training loss = 0.00145 and accuracy of 1
Iteration 414464: with minibatch training loss = 0.0119 and accuracy of 0.99
Iteration 414592: with minibatch training loss = 0.000736 and accuracy of 1
Iteration 414720: with minibatch training loss = 0.0306 and accuracy of 0.98
Iteration 414848: with minibatch training loss = 0.0339 and accuracy of 0.99
Iteration 414976: with minibatch training loss = 0.00204 and accuracy of 1
Epoch 100, Train loss: 0.0166 and Train accuracy of 0.995, Test loss: 0.173 and Test accuracy of 0.968
4:34:29.507621