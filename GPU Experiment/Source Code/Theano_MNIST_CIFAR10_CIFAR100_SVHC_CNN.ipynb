{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import pickle\n",
    "import lasagne\n",
    "import gzip\n",
    "import LoggerYN as YN\n",
    "import scipy.io as sio\n",
    "import utilsYN as uYN\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Reloaded modules: <module_name>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    global Dataset    \n",
    "    global pbatchSize\n",
    "    global pnumClasses\n",
    "    global pEpochs\n",
    "    global pLearningRate\n",
    "    global pMomentum\n",
    "    global pWeightDecay\n",
    "    Dataset = dataset\n",
    "    pbatchSize = batchSize\n",
    "    pnumClasses = numClasses\n",
    "    pEpochs = epochs\n",
    "    pLearningRate = learningRate\n",
    "    pMomentum = momentum\n",
    "    pWeightDecay = weightDecay\n",
    "\n",
    "def NormalizeData(x_train,x_test):\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    return x_train, x_test\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]    \n",
    "    \n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "    \n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def loadDataCIFAR10():\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for j in range(5):\n",
    "      d = unpickle('cifar-10-batches-py/data_batch_'+ str(j+1))\n",
    "      x = d['data']\n",
    "      y = d['labels']\n",
    "      xs.append(x)\n",
    "      ys.append(y)\n",
    "\n",
    "    d = unpickle('cifar-10-batches-py/test_batch')\n",
    "    xs.append(d['data'])\n",
    "    ys.append(d['labels'])\n",
    "\n",
    "    x = np.concatenate(xs)/np.float32(255)\n",
    "    y = np.concatenate(ys)\n",
    "    x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3)).transpose(0,3,1,2)\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "    \n",
    "    X_test = x[50000:,:,:,:]\n",
    "    Y_test = y[50000:]\n",
    "\n",
    "    return dict(\n",
    "        X_train=X_train,\n",
    "        Y_train=Y_train.astype('int32'),\n",
    "        X_test = X_test,\n",
    "        Y_test = Y_test.astype('int32'),)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def loadDataCIFAR100():\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for j in range(1):\n",
    "      d = unpickle('cifar-100-batches-py/train')\n",
    "      x = d['data']\n",
    "      y = d['fine_labels']\n",
    "      xs.append(x)\n",
    "      ys.append(y)\n",
    "\n",
    "    d = unpickle('cifar-100-batches-py/test')\n",
    "    xs.append(d['data'])\n",
    "    ys.append(d['fine_labels'])\n",
    "\n",
    "    x = np.concatenate(xs)/np.float32(255)\n",
    "    y = np.concatenate(ys)\n",
    "    x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3)).transpose(0,3,1,2)\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "\n",
    "    X_test = x[50000:,:,:,:]\n",
    "    Y_test = y[50000:]\n",
    "\n",
    "    return dict(X_train=X_train,Y_train=Y_train.astype('int32'),X_test = X_test,Y_test = Y_test.astype('int32'),)   \n",
    "    \n",
    "def loadDataMNIST():\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        return data / np.float32(255)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "#    print(X_train.shape)\n",
    "    global imgRows\n",
    "    global imgCols\n",
    "    global imgRGB_Dimensions\n",
    "    global inputShape\n",
    "    \n",
    "    imgRGB_Dimensions = X_train.shape[1]\n",
    "    imgRows = X_train.shape[2]\n",
    "    imgCols = X_train.shape[3]\n",
    "    inputShape = (imgRows, imgCols, imgRGB_Dimensions)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def loadDataSVHN(fname,extra=False):\n",
    "    \"\"\"Load the SVHN dataset (optionally with extra images)\n",
    "    Args:\n",
    "        extra (bool, optional): load extra training data\n",
    "    Returns:\n",
    "        Dataset: SVHN data\n",
    "    \"\"\"\n",
    "    def load_mat(fname):\n",
    "        data = sio.loadmat(fname)\n",
    "        X = data['X'].transpose(3, 0, 1, 2)\n",
    "        y = data['y'] % 10  # map label \"10\" --> \"0\"\n",
    "        return X, y\n",
    "\n",
    "    data = uYN.Dataset()\n",
    "    data.classes = np.arange(10)\n",
    "\n",
    "\n",
    "    X, y = load_mat(fname % 'train')\n",
    "    data.train_images = X\n",
    "    data.train_labels = y.reshape(-1)\n",
    "\n",
    "    X, y = load_mat(fname % 'test')\n",
    "    data.test_images = X\n",
    "    data.test_labels = y.reshape(-1)\n",
    "\n",
    "    new_x = data.train_images\n",
    "    new_y = data.train_labels\n",
    "    \n",
    "    if extra:\n",
    "        X, y = load_mat(fname % 'extra')\n",
    "        data.extra_images = X\n",
    "        data.extra_labels = y.reshape(-1)\n",
    "    \n",
    "        # Use extra dataset\n",
    "        new_x = data.extra_images#np.concatenate((data.extra_images, data.train_images), axis=0)\n",
    "        new_y = data.extra_labels#np.concatenate((data.extra_labels, data.train_labels), axis=0)\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test)  = (new_x,new_y),(data.test_images,data.test_labels)\n",
    "    \n",
    "    global imgRows\n",
    "    global imgCols\n",
    "    global imgRGB_Dimensions\n",
    "    global inputShape\n",
    "    \n",
    "    imgRows = x_train.shape[1]\n",
    "    imgCols = x_train.shape[2]\n",
    "\n",
    "    try:\n",
    "        imgRGB_Dimensions = x_train.shape[3]\n",
    "    except Exception:\n",
    "        imgRGB_Dimensions = 1 #For Gray Scale Images\n",
    "\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_test = x_test.reshape(x_test.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train, x_test = NormalizeData(x_train, x_test)\n",
    "    inputShape = (imgRows, imgCols, imgRGB_Dimensions)\n",
    "    \n",
    "    \n",
    "    x_train = x_train.reshape(len(y_train), imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_train= theano.shared(x_train).get_value()\n",
    "    label= theano.shared(y_train).get_value()\n",
    "    \n",
    "\n",
    "    x_test = x_test.reshape(len(y_test), imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_test= theano.shared(x_test).get_value()\n",
    "    labelTest= theano.shared(y_test).get_value()\n",
    "\n",
    "    \n",
    "    return x_train,label, x_test,labelTest\n",
    "        \n",
    "        \n",
    "        \n",
    "def modelMNIST(input_var=None):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, imgRGB_Dimensions, imgRows, imgCols),input_var=input_var)\n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=32, filter_size=(3, 3),nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=64, filter_size=(3, 3),nonlinearity=lasagne.nonlinearities.rectify,,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, p=.25),num_units=128,nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.FlattenLayer(network)\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, p=.5),num_units=10,nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validateModel(X_test,y_test,batchSize,epoch,train_batches,train_err,train_acc,start_time,val_fn):\n",
    "    #pass over the validation data:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, batchSize, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    \n",
    "    print(\"Final results:\")\n",
    "    print(\" training loss:{:.6f}\".format(train_err / train_batches) \n",
    "          + \",Train accuracy:{:.2f} %\".format(train_acc / train_batches * 100)\n",
    "          + \",test loss:{:.6f}\".format(test_err / test_batches) \n",
    "          + \",test accuracy:{:.2f} %\".format(test_acc / test_batches * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = loadDataMNIST()\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    # prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # build neural network model\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    \n",
    "    net = modelMNIST(input_var)\n",
    "    \n",
    "    # cross-entropy loss for training loss:\n",
    "    prediction = lasagne.layers.get_output(net)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    weightsl2 = lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "    loss += weightDecay * weightsl2\n",
    "    \n",
    "    # SGD\n",
    "    params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "    updates = lasagne.updates.momentum(loss, params, learningRate, momentum)\n",
    "    \n",
    "    test_prediction = lasagne.layers.get_output(net, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "     # training loss\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    # test accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),dtype=theano.config.floatX)\n",
    "\n",
    "    #validation loss and accuracy\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    # loop over all epochs:\n",
    "    print(\"MNIST Training Started.....\")\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Theano\",\"MNIST\")\n",
    "    start = time.time() \n",
    "    for epoch in range(epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        train_acc = 0\n",
    "        for batch in iterate_minibatches(X_train, y_train, batchSize, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            _, acc = val_fn(inputs, targets)\n",
    "            train_acc += acc\n",
    "            if train_batches % batchSize == 0:\n",
    "                print('Epoch(',epoch,') Batch(',train_batches+1,\")\",train_err,)\n",
    "            train_batches += 1\n",
    "        validateModel(X_test,y_test,batchSize,epoch,train_batches,train_err,train_acc,start,val_fn)\n",
    "    end = time.time()\n",
    "    print(\"MNIST Training Finished.....\")\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "\n",
    "\n",
    "\n",
    "def modelSVHN(input_var=None):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, imgRGB_Dimensions, imgRows, imgCols),input_var=input_var)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=48, filter_size=(5, 5),pad=2,stride=1,nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.BatchNormLayer(network)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2),stride=2,pad=1)\n",
    "    network = lasagne.layers.DropoutLayer(network,p=0.2)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=64, filter_size=(5, 5),pad=2,stride=1,nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.BatchNormLayer(network)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2),stride=2,pad=1)\n",
    "    network = lasagne.layers.DropoutLayer(network,p=0.2)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=128, filter_size=(5, 5),pad=2,stride=1,nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.BatchNormLayer(network)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2),stride=2,pad=1)\n",
    "    network = lasagne.layers.DropoutLayer(network,p=0.2)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=160, filter_size=(5, 5),pad=2,stride=1,nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.BatchNormLayer(network)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2),stride=2,pad=1)\n",
    "    network = lasagne.layers.DropoutLayer(network,p=0.2)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=192, filter_size=(5, 5),pad=2,stride=1,nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.BatchNormLayer(network)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2),stride=2,pad=1)\n",
    "    network = lasagne.layers.DropoutLayer(network,p=0.2)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=192, filter_size=(5, 5),pad=2,stride=1,nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.BatchNormLayer(network)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2),stride=2,pad=1)\n",
    "    network = lasagne.layers.DropoutLayer(network,p=0.2)\n",
    "    \n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=192, filter_size=(5, 5),pad=2,stride=1,nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.BatchNormLayer(network)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2),stride=2,pad=1)\n",
    "    network = lasagne.layers.DropoutLayer(network,p=0.2)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=192, filter_size=(5, 5),pad=2,stride=1,nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.BatchNormLayer(network)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2),stride=2,pad=1)\n",
    "    network = lasagne.layers.DropoutLayer(network,p=0.2)\n",
    "    \n",
    "\n",
    "    network = lasagne.layers.FlattenLayer(network)    \n",
    "    network = lasagne.layers.DenseLayer(network,num_units=3072,nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.DenseLayer(network,num_units=10,nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return network\n",
    "\n",
    "def validateModelSVHN(X_test,y_test,batchSize,epoch,train_batches,train_err,train_acc,val_fn):\n",
    "    #Calculate and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for data, label in zip(batch(X_test, batchSize),batch(y_test, batchSize)):\n",
    "        data = np.transpose(data,(0,3,1,2))\n",
    "        err, acc = val_fn(data, label)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    \n",
    "    \n",
    "    print(\"Final results:\")\n",
    "    print(\" training loss:{:.6f}\".format(train_err / train_batches) \n",
    "          + \",Train accuracy:{:.2f} %\".format(train_acc / train_batches * 100)\n",
    "          + \",test loss:{:.6f}\".format(test_err / test_batches) \n",
    "          + \",test accuracy:{:.2f} %\".format(test_acc / test_batches * 100))\n",
    "\n",
    "    \n",
    "\n",
    "def RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay,fname):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "\n",
    "    X_train, y_train, X_test, y_test = loadDataSVHN(fname,True)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    # prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # build neural network model\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    \n",
    "    net = modelSVHN(input_var)\n",
    "       \n",
    "     # cross-entropy loss for training loss:\n",
    "    prediction = lasagne.layers.get_output(net)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    weightsl2 = lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "    loss += weightDecay * weightsl2\n",
    "    \n",
    "    # SGD\n",
    "    params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "    updates = lasagne.updates.momentum(loss, params, learningRate, momentum)\n",
    "    \n",
    "    test_prediction = lasagne.layers.get_output(net, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "        \n",
    "     # training loss\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    # test accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),dtype=theano.config.floatX)\n",
    "\n",
    "    #validation loss and accuracy\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    # loop over all epochs:\n",
    "    print(\"SVHN Training Started.....\")\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Theano\",\"SVHN\")\n",
    "    start = time.time() \n",
    "    for epoch in range(epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        train_acc = 0\n",
    "        for data, label in zip(batch(X_train, batchSize),batch(y_train, batchSize)):\n",
    "            data = np.transpose(data,(0,3,1,2))\n",
    "            train_err += train_fn(data, label)\n",
    "            _, acc = val_fn(data, label)\n",
    "            train_acc += acc\n",
    "            if train_batches % batchSize == 0:\n",
    "                print('Epoch(',epoch,') Batch(',train_batches+1,\")\",train_err,)\n",
    "            train_batches += 1\n",
    "        validateModelSVHN(X_test,y_test,batchSize,epoch,train_batches,train_err,train_acc,val_fn)\n",
    "    end = time.time()    \n",
    "    print(\"SVHN Training Finished.....\")\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "\n",
    "\n",
    "def modelCIFAR10(input_var=None):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 3, 32, 32),input_var=input_var)\n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=32, filter_size=(3, 3),nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=32, filter_size=(3, 3),nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    network = lasagne.layers.DropoutLayer(network, p=0.25)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=64, filter_size=(3, 3),nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=64, filter_size=(3, 3),nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    network = lasagne.layers.DropoutLayer(network, p=0.25)\n",
    "    network = lasagne.layers.FlattenLayer(network)\n",
    "\n",
    "    network = lasagne.layers.DenseLayer(network,num_units=512,nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.DropoutLayer(network, p=0.5)\n",
    "    network = lasagne.layers.DenseLayer(network,num_units=10,nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return network\n",
    "\n",
    "def RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    # Check if cifar data exists\n",
    "    if not os.path.exists(\"./cifar-10-batches-py\"):\n",
    "        print(\"CIFAR-10 dataset can not be found. Please download the dataset from 'https://www.cs.toronto.edu/~kriz/cifar.html'.\")\n",
    "        return\n",
    "\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    data = loadDataCIFAR10()\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    y_train = data['Y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['Y_test']\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "    \n",
    "    # build neural network model\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    \n",
    "    net = modelCIFAR10(input_var)\n",
    "    \n",
    "    # cross-entropy loss for training loss:\n",
    "    prediction = lasagne.layers.get_output(net)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    weightsl2 = lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "    loss += weightDecay * weightsl2\n",
    "    \n",
    "    # SGD\n",
    "    params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "    updates = lasagne.updates.momentum(loss, params, learningRate, momentum)\n",
    "    \n",
    "    test_prediction = lasagne.layers.get_output(net, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    \n",
    "     # training loss\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    # test accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),dtype=theano.config.floatX)\n",
    "\n",
    "    #validation loss and accuracy\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    # loop over all epochs:\n",
    "    print(\"CIFAR10 Training Started.....\")\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Theano\",\"CIFAR10\")\n",
    "    start = time.time()  \n",
    "    for epoch in range(epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        train_acc = 0\n",
    "        for batch in iterate_minibatches(X_train, y_train, batchSize, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            _, acc = val_fn(inputs, targets)\n",
    "            train_acc += acc\n",
    "            if train_batches % batchSize == 0:\n",
    "                print('Epoch(',epoch,') Batch(',train_batches+1,\")\",train_err,)\n",
    "            train_batches += 1\n",
    "        validateModel(X_test,y_test,batchSize,epoch,train_batches,train_err,train_acc,start,val_fn)\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"CIFAR10 Training Finished.....\")\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "\n",
    "def modelCIFAR100(input_var=None):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 3, 32, 32),input_var=input_var)\n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=128, filter_size=(3, 3),stride=1,pad='same',nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=128, filter_size=(3, 3),stride=1,pad='valid',nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2),stride=2)\n",
    "    network = lasagne.layers.DropoutLayer(network, p=0.1)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=256, filter_size=(3, 3),stride=1,pad='same',nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=256, filter_size=(3, 3),stride=1,pad='valid',nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2),stride=2)\n",
    "    network = lasagne.layers.DropoutLayer(network, p=0.25)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=512, filter_size=(3, 3),stride=1,pad='same',nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=512, filter_size=(3, 3),stride=1,pad='valid',nonlinearity=lasagne.nonlinearities.rectify,W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2),stride=2)\n",
    "    network = lasagne.layers.DropoutLayer(network, p=0.5)\n",
    "\n",
    "    \n",
    "    network = lasagne.layers.FlattenLayer(network)\n",
    "\n",
    "    network = lasagne.layers.DenseLayer(network,num_units=1024,nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.DropoutLayer(network, p=0.5)\n",
    "    network = lasagne.layers.DenseLayer(network,num_units=100,nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return network\n",
    "\n",
    "\n",
    "def RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    # Check if cifar data exists\n",
    "    if not os.path.exists(\"./cifar-100-batches-py\"):\n",
    "        print(\"CIFAR-10 dataset can not be found. Please download the dataset from 'https://www.cs.toronto.edu/~kriz/cifar.html'.\")\n",
    "        return\n",
    "\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    data = loadDataCIFAR100()\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['Y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['Y_test']\n",
    "\n",
    "    \n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "    \n",
    "\n",
    "    \n",
    "    # build neural network model\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    \n",
    "    net = modelCIFAR100(input_var)\n",
    "    \n",
    "   \n",
    "    # cross-entropy loss for training loss:\n",
    "    prediction = lasagne.layers.get_output(net)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    weightsl2 = lasagne.regularization.regularize_network_params(net, lasagne.regularization.l2)\n",
    "    loss += weightDecay * weightsl2\n",
    "    \n",
    "    # SGD\n",
    "    params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "    updates = lasagne.updates.momentum(loss, params, learningRate, momentum)\n",
    "    \n",
    "    test_prediction = lasagne.layers.get_output(net, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    \n",
    "     # training loss\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    # test accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),dtype=theano.config.floatX)\n",
    "\n",
    "\n",
    "    #validation loss and accuracy\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    # loop over all epochs:\n",
    "    print(\"CIFAR100 Training Started.....\")\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Theano\",\"CIFAR100\")\n",
    "    start = time.time()      \n",
    "    for epoch in range(epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        train_acc = 0\n",
    "        for batch in iterate_minibatches(X_train, y_train, batchSize, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            _, acc = val_fn(inputs, targets)\n",
    "            train_acc += acc\n",
    "            if train_batches % batchSize == 0:\n",
    "                print('Epoch(',epoch,') Batch(',train_batches+1,\")\",train_err,)\n",
    "            train_batches += 1\n",
    "        validateModel(X_test,y_test,batchSize,epoch,train_batches,train_err,train_acc,start,val_fn)\n",
    "    end = time.time()\n",
    "    print(\"CIFAR100 Training Finished.....\")\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def runModel(dataset,batchSize=128,numClasses=10,epochs=12,learningRate=0.01,momentum=0.5,weightDecay=1e-6):\n",
    "    if dataset is \"mnist\":\n",
    "        RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar10\":\n",
    "        RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar100\":\n",
    "        RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)  \n",
    "    elif dataset is \"SVHN\":\n",
    "        fname = './%s_32x32.mat'\n",
    "        RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay,fname)    \n",
    "    else:\n",
    "        print(\"Choose cifar10 or mnist\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():    \n",
    "    runModel(\"mnist\",epochs=15)      \n",
    "#    runModel(\"SVHN\",epochs=100)\n",
    "#    runModel(\"cifar10\",epochs=200)\n",
    "#    runModel(\"cifar100\",epochs=200)    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
