{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras import initializers as init\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import LoggerYN as YN\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import time\n",
    "import datetime\n",
    "from data import load_imdb \n",
    "import sys\n",
    "from keras import optimizers as optim\n",
    "from keras import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "import sys\n",
    "from data import load_ptb, load_ptb_vocab\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_imdb(n_epochs):\n",
    "    \n",
    "    np_load_old = np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "    \n",
    "    def imdb_data(train, vocabulary_size, seq_len):\n",
    "        x, y = load_imdb(train, vocabulary_size, seq_len)\n",
    "        x = pad_sequences(x, maxlen=seq_len, padding='post')\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def imdb_lstm(vocabulary_size, embedding_size, seq_len, hidden_size):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocabulary_size, embedding_size, mask_zero=True, input_length=seq_len,\n",
    "                            embeddings_initializer=init.RandomUniform(-1.0, 1.0)))\n",
    "        model.add(LSTM(hidden_size,kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    class ImdbLogCallback(keras.callbacks.Callback):\n",
    "\n",
    "        def __init__(self, test, print_every=50):\n",
    "            self.x_test, self.y_test = test\n",
    "            self.print_every = print_every\n",
    "\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs={}):\n",
    "            self.epoch = epoch\n",
    "\n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            if (batch + 1) % self.print_every == 0:\n",
    "                print('[%d, %5d] train loss: %.3f' % (self.epoch, batch + 1, np.mean(self.losses)))\n",
    "                sys.stdout.flush()\n",
    "                self.losses = []\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            loss, accuracy = self.model.evaluate(self.x_test, self.y_test, verbose=2)\n",
    "            print('[%d] test loss: %.3f accuracy: %.3f' % (self.epoch, loss, accuracy))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def imdb_run(n_epochs, vocabulary_size, seq_len, batch_size, embedding_size, hidden_size):\n",
    "        np.random.seed(1)\n",
    "\n",
    "\n",
    "        x_train, y_train = imdb_data(train=True, vocabulary_size=vocabulary_size, seq_len=seq_len)\n",
    "        x_test, y_test = imdb_data(train=False, vocabulary_size=vocabulary_size, seq_len=seq_len)\n",
    "\n",
    "        model = imdb_lstm(vocabulary_size, embedding_size, seq_len, hidden_size)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"Keras_GPU\",\"IMDB\")\n",
    "        start = time.time()\n",
    "\n",
    "            # checkpoint\n",
    "        filepath=\"/home/wahab/DLBench_Addl/IMDB/weights_GPU_Keras_IMDB.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=False,period=10 )\n",
    "        #callbacks_list = [checkpoint]\n",
    "\n",
    "        model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epochs, \n",
    "                  verbose=2, callbacks=[ImdbLogCallback((x_test, y_test)),checkpoint])\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"total time\", str(datetime.timedelta(seconds=end-start)))\n",
    "        model.save_weights(\"model_GPU_IMDB_Keras.h5\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    imdb_run(n_epochs, vocabulary_size = 5000, seq_len = 500, batch_size = 64, embedding_size = 32, hidden_size = 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_ptb(n_epochs):\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "class PtbGenerator:\n",
    "\n",
    "    def __init__(self, train, batch_size, seq_len, vocabulary_size, skip_step=5):\n",
    "        self.data = load_ptb(train)\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.skip_step = skip_step\n",
    "        self.cur_idx = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.seq_len - 1) // (self.skip_step * self.batch_size)\n",
    "        \n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.seq_len), dtype=np.int32)\n",
    "        y = np.zeros((self.batch_size, self.seq_len, self.vocabulary_size), dtype=np.int32)\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.cur_idx + self.seq_len >= len(self.data):\n",
    "                    self.cur_idx = 0\n",
    "                x[i, :] = self.data[self.cur_idx:self.cur_idx+self.seq_len]\n",
    "                y_batch = self.data[self.cur_idx+1:self.cur_idx+self.seq_len+1]\n",
    "                y[i, :, :] = utils.to_categorical(y_batch, num_classes=self.vocabulary_size)\n",
    "                self.cur_idx += self.skip_step\n",
    "            yield x, y\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def ptb_lstm(vocabulary_size, hidden_size, seq_len, num_layers, dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, hidden_size, input_length=seq_len,\n",
    "                        embeddings_initializer=init.RandomUniform(-1.0, 1.0)))\n",
    "    for _ in range(num_layers):\n",
    "        model.add(LSTM(hidden_size, return_sequences=True,kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "class PtbLogCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, test_generator, print_every=4500):\n",
    "        self.test_generator = test_generator\n",
    "        self.print_every = print_every\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        if (batch + 1) % self.print_every == 0:\n",
    "            print('[%d, %5d] train loss: %.3f' % (self.epoch, batch + 1, np.mean(self.losses)))\n",
    "            self.losses = []\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        loss,accuracy = self.model.evaluate_generator(self.test_generator.generate(), steps=len(self.test_generator), \n",
    "                                             verbose=2)\n",
    "        print('[%d] test loss: %.3f perplexity: %.3f' % (self.epoch, loss, np.exp(loss)))\n",
    "        print('[%d] accuracy: %.3f' % (self.epoch, accuracy))\n",
    "        print(\"\\n\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def ptb_run(n_epochs, hidden_size, batch_size, seq_len, dropout, num_layers):\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ptb_vocab = load_ptb_vocab()\n",
    "    vocabulary_size = len(ptb_vocab)\n",
    "    \n",
    "    train_generator = PtbGenerator(train=True, batch_size=batch_size,\n",
    "                                   seq_len=seq_len, vocabulary_size=vocabulary_size)\n",
    "    test_generator = PtbGenerator(train=False, batch_size=batch_size,\n",
    "                                   seq_len=seq_len, vocabulary_size=vocabulary_size)\n",
    "    \n",
    "    model = ptb_lstm(vocabulary_size, hidden_size, seq_len, num_layers, dropout)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim.Adadelta(lr=1.0, rho=0.95, epsilon=1e-6),metrics=['accuracy'])\n",
    "    \n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Keras_GPU\", \"PTB\")\n",
    "    start = time.time()\n",
    "    filepath=\"/home/wahab/DLBench_Addl/Penn TreeBank/weights_GPU_Keras_ptb.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=False,period=5 )\n",
    "\n",
    "    model.fit_generator(train_generator.generate(), steps_per_epoch=len(train_generator), \n",
    "                        epochs=n_epochs, verbose=2, callbacks=[PtbLogCallback(test_generator)])\n",
    "    \n",
    "    end = time.time()\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "\n",
    "    ptb_run(n_epochs, hidden_size = 200, batch_size = 20, seq_len = 30, dropout = 0.5, num_layers = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_manythings(n_epochs):\n",
    "\n",
    "\n",
    "    # Converts the unicode file to ascii\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "    def preprocess_sentence(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.rstrip().strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "\n",
    "    # 1. Remove the accents\n",
    "    # 2. Clean the sentences\n",
    "    # 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "    def create_dataset(path):\n",
    "        lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "        word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:100000]]\n",
    "\n",
    "        return word_pairs\n",
    "\n",
    "        # This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "    # (e.g., 5 -> \"dad\") for each language,\n",
    "    class LanguageIndex():\n",
    "        def __init__(self, lang):\n",
    "            self.lang = lang\n",
    "            self.word2idx = {}\n",
    "            self.idx2word = {}\n",
    "            self.vocab = set()\n",
    "\n",
    "            self.create_index()\n",
    "\n",
    "        def create_index(self):\n",
    "            for phrase in self.lang:\n",
    "                self.vocab.update(phrase.split(' '))\n",
    "            self.vocab = sorted(self.vocab)\n",
    "            self.word2idx['<pad>'] = 0\n",
    "            for index, word in enumerate(self.vocab):\n",
    "                self.word2idx[word] = index + 1\n",
    "            for word, index in self.word2idx.items():\n",
    "                self.idx2word[index] = word\n",
    "    def max_length(tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "    def load_dataset(path):\n",
    "        # creating cleaned input, output pairs\n",
    "        pairs = create_dataset(path)\n",
    "\n",
    "        # index language using the class defined above    \n",
    "        inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "        targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "\n",
    "        # Vectorize the input and target languages\n",
    "\n",
    "        # Spanish sentences\n",
    "        input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # English sentences\n",
    "        target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # Calculate max_length of input and output tensor\n",
    "        # Here, we'll set those to the longest sentence in the dataset\n",
    "        max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "        # Padding the input and output tensor to the maximum length\n",
    "        input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                     maxlen=max_length_inp,\n",
    "                                                                     padding='post')\n",
    "\n",
    "        target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                      maxlen=max_length_tar, \n",
    "                                                                      padding='post')\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    def create_db(path_to_file):\n",
    "        input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file)\n",
    "        # Creating training and validation sets using an 80-20 split\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2,random_state=42)\n",
    "        vocab_inp_size = len(inp_lang.word2idx)\n",
    "        vocab_tar_size = len(targ_lang.word2idx)\n",
    "        return input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    def generate_batch(X , y,batch_size,max_length_inp,max_length_targ,vocab_tar_size):\n",
    "        ''' Generate a batch of data '''\n",
    "        while True:\n",
    "            for j in range(0, len(X), batch_size):\n",
    "                encoder_input_data = np.zeros((batch_size, max_length_inp),dtype='float32')\n",
    "                decoder_input_data = np.zeros((batch_size, max_length_targ),dtype='float32')\n",
    "                decoder_target_data = np.zeros((batch_size, max_length_targ, vocab_tar_size),dtype='float32')\n",
    "                for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                    for t, word in enumerate(input_text):\n",
    "                        encoder_input_data[i, t] = word # encoder input seq\n",
    "                    for t, word in enumerate(target_text):\n",
    "                        if t<len(target_text)-1:\n",
    "                            decoder_input_data[i, t] = word # decoder input seq\n",
    "                        if t>0:\n",
    "                            # decoder target sequence (one hot encoded)\n",
    "                            # does not include the START_ token\n",
    "                            # Offset by one timestep\n",
    "                            decoder_target_data[i, t - 1, word] = 1.\n",
    "                yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    def perplexity(y_true, y_pred):\n",
    "        return np.power(2,keras.backend.mean(keras.losses.categorical_crossentropy(y_true, y_pred)))\n",
    "    def create_model(embedding_dim,units,vocab_inp_size,vocab_tar_size):\n",
    "        # Encoder\n",
    "        encoder_inputs = Input(shape=(None,))\n",
    "        enc_emb =  Embedding(vocab_inp_size, embedding_dim, mask_zero = True)(encoder_inputs)\n",
    "        encoder_lstm = LSTM(units, return_state=True,kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros')\n",
    "        encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "        encoder_states = [state_h, state_c]\n",
    "        decoder_inputs = Input(shape=(None,))\n",
    "        dec_emb_layer = Embedding(vocab_tar_size, embedding_dim, mask_zero = True)\n",
    "        dec_emb = dec_emb_layer(decoder_inputs)\n",
    "        decoder_lstm = LSTM(units, return_sequences=True, return_state=True,kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros')\n",
    "        decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                             initial_state=encoder_states)\n",
    "        decoder_dense = Dense(vocab_tar_size, activation='softmax')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        return model\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    def run(epochs, BATCH_SIZE,  embedding_dim, units):\n",
    "        path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
    "        path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ = create_db(path_to_file)\n",
    "        BUFFER_SIZE = len(input_tensor_train)\n",
    "        N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "        train_samples = len(input_tensor_train)\n",
    "        val_samples = len(input_tensor_val)\n",
    "        model = create_model(embedding_dim,units,vocab_inp_size,vocab_tar_size)\n",
    "        opt = keras.optimizers.Adam(lr=0.0001)\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"Keras_GPU\",\"Manythings\")\n",
    "\n",
    "        # checkpoint\n",
    "        #checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=False,period=10 )\n",
    "        #callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        model.fit_generator(verbose =2,generator = generate_batch(input_tensor_train, target_tensor_train, BATCH_SIZE,max_length_inp,max_length_targ,vocab_tar_size),\n",
    "                        steps_per_epoch = N_BATCH,epochs=epochs,\n",
    "                        validation_data = generate_batch(input_tensor_val, target_tensor_val,BATCH_SIZE,max_length_inp,max_length_targ,vocab_tar_size),\n",
    "                        validation_steps = val_samples//BATCH_SIZE)\n",
    "        end = time.time()\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"total time\", str(datetime.timedelta(seconds=end-start)))\n",
    "        model.save_weights(\"Keras_GPU_model.h5\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    run(n_epochs,BATCH_SIZE = 128,  embedding_dim = 256, units = 256 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  5/625 [..............................] - ETA: 21:48 - loss: 9.2850 - perplexity: 426.9775 - acc: 0.0061   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-ca6c9f4a6b22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#run_imdb(n_epochs=50)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#run_ptb(n_epochs=50)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrun_manythings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-b47dad48ea10>\u001b[0m in \u001b[0;36mrun_manythings\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0membedding_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-b47dad48ea10>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(epochs, BATCH_SIZE, embedding_dim, units)\u001b[0m\n\u001b[0;32m    170\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN_BATCH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length_targ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_tar_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                         validation_steps = val_samples//BATCH_SIZE)\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mYN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEndLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcpuT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgpuT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run_imdb(n_epochs=50)\n",
    "#run_ptb(n_epochs=50)\n",
    "#run_manythings(n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
