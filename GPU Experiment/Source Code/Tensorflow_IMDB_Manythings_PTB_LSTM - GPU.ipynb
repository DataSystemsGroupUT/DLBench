{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\wahab\\.conda\\envs\\myenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import numpy as np\n",
    "import six\n",
    "import types\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import LoggerYN as YN\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "from data import load_ptb, load_ptb_vocab\n",
    "from data import load_imdb \n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_imdb(n_epochs):\n",
    "\n",
    "    class Net(tf.keras.Model):\n",
    "      \"\"\"A simple linear model.\"\"\"\n",
    "\n",
    "      def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = tf.keras.layers.Dense(5)\n",
    "\n",
    "      def call(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    def imdb_iterator_op(train, batch_size, vocabulary_size, seq_len):\n",
    "        x, y = load_imdb(train, vocabulary_size, seq_len)\n",
    "        lens = np.array([len(xi) for xi in x], dtype=int)\n",
    "        x = np.array([np.pad(xi, (0, seq_len - len(xi)), 'constant') for xi in x], dtype=int)\n",
    "\n",
    "        x_placeholder = tf.placeholder(x.dtype, x.shape)\n",
    "        len_placeholder = tf.placeholder(lens.dtype, lens.shape)\n",
    "        y_placeholder = tf.placeholder(y.dtype, y.shape)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((x_placeholder, len_placeholder, y_placeholder))\n",
    "        if train:\n",
    "            dataset = dataset.shuffle(buffer_size=len(y))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        return iterator, {x_placeholder: x, len_placeholder: lens, y_placeholder: y}\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    class ImdbLstm:\n",
    "\n",
    "        def __init__(self, vocabulary_size, embedding_size, hidden_size):\n",
    "            self.embed = tf.get_variable('embed', (vocabulary_size, embedding_size), \n",
    "                                         initializer=tf.initializers.random_uniform(-1.0, 1.0))\n",
    "            self.lstm = tf.nn.rnn_cell.LSTMCell(hidden_size, initializer=lstm_kernel_initializer)\n",
    "            self.fc = tf.layers.Dense(units=1)\n",
    "\n",
    "        def __call__(self, x, lens):\n",
    "            x = tf.nn.embedding_lookup(self.embed, x)\n",
    "            o, (c, h) = tf.nn.dynamic_rnn(self.lstm, x, sequence_length=lens, dtype=tf.float32)   \n",
    "            p = self.fc(h)\n",
    "            return tf.reshape(p, [-1])\n",
    "\n",
    "\n",
    "    def lstm_kernel_initializer(shape, dtype, partition_info):\n",
    "        hidden_size = shape[1] // 4\n",
    "        input_size = shape[0] - hidden_size\n",
    "        kernel = tf.glorot_uniform_initializer(seed=None)((input_size, shape[1]))\n",
    "        recurrent = tf.orthogonal_initializer(seed=None)((hidden_size, shape[1]))\n",
    "        return tf.concat([kernel, recurrent], axis=0)\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    def imdb_train(model, data_iterator_op, loss_op, optimizer_op, placeholders, epoch, print_every=50):\n",
    "        sess = tf.get_default_session()\n",
    "\n",
    "        data_iterator, data_iterator_feed = data_iterator_op\n",
    "        sess.run(data_iterator.initializer, data_iterator_feed)\n",
    "\n",
    "        losses = []\n",
    "        i = 0\n",
    "        data_op = data_iterator.get_next()\n",
    "        while True:\n",
    "            try:\n",
    "                data = sess.run(data_op)\n",
    "                feed_dict = {placeholder: value for placeholder, value in zip(placeholders, data)}\n",
    "                loss, _ = sess.run([loss_op, optimizer_op], feed_dict)\n",
    "\n",
    "                losses.append(loss)\n",
    "                if (i + 1) % print_every == 0:\n",
    "                    print('[%d, %5d] train loss: %.3f' % (epoch, i + 1, np.mean(losses)))\n",
    "                    sys.stdout.flush()\n",
    "                    losses = []\n",
    "                i += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "\n",
    "    def imdb_test(model, data_iterator_op, loss_op, correct_op, placeholders, epoch):\n",
    "        sess = tf.get_default_session()\n",
    "\n",
    "        data_iterator, data_iterator_feed = data_iterator_op\n",
    "        sess.run(data_iterator.initializer, data_iterator_feed)\n",
    "\n",
    "        losses = []\n",
    "        correct, total = 0, 0\n",
    "        data_op = data_iterator.get_next()\n",
    "        while True:\n",
    "            try:\n",
    "                data = sess.run(data_op)\n",
    "\n",
    "                feed_dict = {placeholder: value for placeholder, value in zip(placeholders, data)}\n",
    "                loss, preds = sess.run([loss_op, correct_op], feed_dict)\n",
    "\n",
    "                losses.append(loss)\n",
    "                correct += preds.sum()\n",
    "                total += preds.shape[0]\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        print('[%d] test loss: %.3f accuracy: %.3f' % (epoch, np.mean(losses), correct / total * 100))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    def imdb_run(n_epochs, vocabulary_size, seq_len, batch_size, embedding_size, hidden_size):\n",
    "        np.random.seed(1)\n",
    "        tf.set_random_seed(1)\n",
    "\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        train_iterator_op = imdb_iterator_op(train=True, batch_size=batch_size, \n",
    "                                             vocabulary_size=vocabulary_size, seq_len=seq_len)\n",
    "        test_iterator_op = imdb_iterator_op(train=False, batch_size=batch_size, \n",
    "                                             vocabulary_size=vocabulary_size, seq_len=seq_len)\n",
    "\n",
    "        inputs = tf.placeholder(tf.int32, [None, seq_len])\n",
    "        lens = tf.placeholder(tf.int32, [None])\n",
    "        labels = tf.placeholder(tf.int32, [None])\n",
    "        placeholders = (inputs, lens, labels)\n",
    "\n",
    "        model = ImdbLstm(vocabulary_size, embedding_size, hidden_size)\n",
    "        outputs = model(inputs, lens)\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, outputs)\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        correct = tf.equal(tf.cast(outputs >= 0.0, tf.int32), labels)\n",
    "\n",
    "        sess = tf.Session()\n",
    "        try: \n",
    "            with tf.device(\"/GPU:0\") as dev:\n",
    "                epoch=0\n",
    "\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                start = time.time()\n",
    "                memT,cpuT,gpuT = YN.StartLogger(\"TensorFlow_GPU\",\"IMDB\")\n",
    "\n",
    "                start = time.time()\n",
    "                current_time = time.time()\n",
    "                time_consumed=current_time-start\n",
    "                epoch=0\n",
    "\n",
    "                with sess.as_default():\n",
    "\n",
    "                    while (time_consumed <= 86400 and epoch <= n_epochs):\n",
    "                        imdb_train(model, train_iterator_op, loss, optimizer, placeholders, epoch)\n",
    "                        imdb_test(model, test_iterator_op, loss, correct, placeholders, epoch)\n",
    "                        epoch += 1\n",
    "                        time_consumed=(time.time())-start\n",
    "                        print(\"Time since beginning \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "                        sys.stdout.flush()\n",
    "                        if epoch % 10 == 0:\n",
    "                            net.save_weights('TensorFlow_GPU_IMDB_LSTM_model')\n",
    "                end = time.time()\n",
    "                YN.EndLogger(memT,cpuT,gpuT)\n",
    "                print(\"\\n\\nTotal Time Consumed \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "                sys.stdout.flush()\n",
    "                net.save_weights('TensorFlow_GPU_IMDB_LSTM_model')\n",
    "        except tf.errors.InvalidArgumentError:\n",
    "            print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    imdb_run(n_epochs, vocabulary_size = 5000, seq_len = 500, batch_size = 64, embedding_size = 32, hidden_size = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_manythings(n_epochs):\n",
    "\n",
    "    # Converts the unicode file to ascii\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "    def preprocess_sentence(w):\n",
    "        w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.rstrip().strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "\n",
    "    # 1. Remove the accents\n",
    "    # 2. Clean the sentences\n",
    "    # 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "    def create_dataset(path):\n",
    "        lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "        word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:100000]]\n",
    "\n",
    "        return word_pairs\n",
    "\n",
    "        # This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "    # (e.g., 5 -> \"dad\") for each language,\n",
    "    class LanguageIndex():\n",
    "        def __init__(self, lang):\n",
    "            self.lang = lang\n",
    "            self.word2idx = {}\n",
    "            self.idx2word = {}\n",
    "            self.vocab = set()\n",
    "\n",
    "            self.create_index()\n",
    "\n",
    "        def create_index(self):\n",
    "            for phrase in self.lang:\n",
    "                self.vocab.update(phrase.split(' '))\n",
    "\n",
    "            self.vocab = sorted(self.vocab)\n",
    "\n",
    "            self.word2idx['<pad>'] = 0\n",
    "            for index, word in enumerate(self.vocab):\n",
    "                self.word2idx[word] = index + 1\n",
    "\n",
    "            for word, index in self.word2idx.items():\n",
    "                self.idx2word[index] = word\n",
    "    def max_length(tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "    def load_dataset(path):\n",
    "        # creating cleaned input, output pairs\n",
    "        pairs = create_dataset(path)\n",
    "\n",
    "        # index language using the class defined above    \n",
    "        inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "        targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "\n",
    "        # Vectorize the input and target languages\n",
    "\n",
    "        # Spanish sentences\n",
    "        input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # English sentences\n",
    "        target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "\n",
    "        # Calculate max_length of input and output tensor\n",
    "        # Here, we'll set those to the longest sentence in the dataset\n",
    "        max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "        # Padding the input and output tensor to the maximum length\n",
    "        input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                     maxlen=max_length_inp,\n",
    "                                                                     padding='post')\n",
    "\n",
    "        target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                      maxlen=max_length_tar, \n",
    "                                                                      padding='post')\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    def create_db(path_to_file):\n",
    "        input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file)\n",
    "        # Creating training and validation sets using an 80-20 split\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2,random_state=42)\n",
    "        vocab_inp_size = len(inp_lang.word2idx)\n",
    "        vocab_tar_size = len(targ_lang.word2idx)\n",
    "        return input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    class Model:\n",
    "        def __init__(self, vocab_inp,vocab_tar, embedding_dim, units, batch_sz):\n",
    "            self.batch_sz = batch_sz\n",
    "            self.units = units\n",
    "            self.embed_enc = tf.get_variable('embed_enc', (vocab_inp, embedding_dim))\n",
    "            self.enc_cell =  tf.nn.rnn_cell.LSTMCell(self.units,initializer=lstm_kernel_initializer)\n",
    "            self.embed_dec = tf.get_variable('embed_dec', (vocab_tar, embedding_dim))\n",
    "            self.dec_cell =  tf.nn.rnn_cell.LSTMCell(self.units,initializer=lstm_kernel_initializer)\n",
    "            self.fc = tf.layers.Dense(units = vocab_tar)\n",
    "\n",
    "        def __call__(self, inp,tar):\n",
    "            enc_emb = tf.nn.embedding_lookup(self.embed_enc, inp)\n",
    "            _,state = tf.nn.dynamic_rnn(self.enc_cell,inputs=enc_emb,dtype=tf.float32)\n",
    "            dec_emb = tf.nn.embedding_lookup(self.embed_dec, tar)\n",
    "            output,_ = tf.nn.dynamic_rnn(self.enc_cell,inputs=dec_emb,dtype=tf.float32)\n",
    "            preds = self.fc(output)\n",
    "            return preds    \n",
    "    def lstm_kernel_initializer(shape, dtype, partition_info):\n",
    "        hidden_size = shape[1] // 4\n",
    "        input_size = shape[0] - hidden_size\n",
    "        kernel = tf.glorot_uniform_initializer(seed=None)((input_size, shape[1]))\n",
    "        recurrent = tf.orthogonal_initializer(seed=None)((hidden_size, shape[1]))\n",
    "        return tf.concat([kernel, recurrent], axis=0)\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    def loss_function(real, pred):\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    def train(sess,input_tensor_train,target_tensor_train,N_BATCH,BATCH_SIZE,optimizer,loss_,inpu,targe):\n",
    "        for batch in range(N_BATCH):\n",
    "                start = BATCH_SIZE*batch\n",
    "                end = (BATCH_SIZE*batch) + BATCH_SIZE\n",
    "                inp = input_tensor_train[start :end]\n",
    "                targ = target_tensor_train[start:end]\n",
    "                loss,_=sess.run([loss_,optimizer],feed_dict={inpu:inp,targe:targ})\n",
    "                if(batch % 300 ==0):\n",
    "                    print(\"Batch \"+str(batch)+\" Loss \"+str(loss))\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "\n",
    "    # In[7]:\n",
    "\n",
    "\n",
    "    def test(sess,input_tensor_train,target_tensor_train,N_BATCH,BATCH_SIZE,loss_,inpu,targe):\n",
    "        t_loss = 0\n",
    "        for batch in range(N_BATCH):\n",
    "            start = BATCH_SIZE*batch\n",
    "            end = (BATCH_SIZE*batch) + BATCH_SIZE\n",
    "            inp = input_tensor_train[start :end]\n",
    "            targ = target_tensor_train[start:end]\n",
    "            loss = sess.run(loss_,feed_dict={inpu:inp,targe:targ})\n",
    "            t_loss += loss\n",
    "        print('Validation Perplexity :{:.4f}'.format(np.power(2,t_loss/batch)))\n",
    "        print('Validation Loss :{:.4f}'.format(t_loss/batch))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "\n",
    "    def acc(acc_,sess,input_tensor_train,target_tensor_train,N_BATCH,BATCH_SIZE,loss_,inpu,targe):\n",
    "        total=0\n",
    "        count=0\n",
    "        for batch in range(N_BATCH):\n",
    "            start = BATCH_SIZE*batch\n",
    "            end = (BATCH_SIZE*batch) + BATCH_SIZE\n",
    "            inp = input_tensor_train[start :end]\n",
    "            targ = target_tensor_train[start:end]\n",
    "            feed_dict={inpu:inp,targe:targ}\n",
    "            acc_tensor = sess.run(acc_,feed_dict)        \n",
    "            accu_i = np.mean(acc_tensor)\n",
    "            total += accu_i\n",
    "            count += 1\n",
    "        print('Accuracy :{:.4f}'.format(total/count))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    # In[9]:\n",
    "\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "        if not K.is_tensor(y_pred):\n",
    "            y_pred = K.constant(y_pred)\n",
    "            y_true = K.constant(y_true)\n",
    "        y_true = K.cast(y_true, y_pred.dtype)\n",
    "        return K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "\n",
    "    def acc_function(real, pred):\n",
    "        prediction=tf.argmax(pred,-1)\n",
    "        acc_tensor=accuracy(real,prediction)\n",
    "        return acc_tensor\n",
    "\n",
    "\n",
    "    # In[10]:\n",
    "\n",
    "\n",
    "    def run(BATCH_SIZE, embedding_dim, units,  epochs):\n",
    "        tf.set_random_seed(1)\n",
    "        path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
    "        path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ = create_db(path_to_file)\n",
    "        # Get parameters\n",
    "        BUFFER_SIZE = len(input_tensor_train)\n",
    "        N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "        N_BATCH_VAL = len(input_tensor_val)//BATCH_SIZE\n",
    "\n",
    "        train_samples = len(input_tensor_train)\n",
    "        val_samples = len(input_tensor_val)\n",
    "        #Loading data    \n",
    "        data_train = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "        data_train = data_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        data_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
    "        data_val = data_val.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        # Model initiation\n",
    "        model = Model(vocab_inp_size,vocab_tar_size,embedding_dim,units,BATCH_SIZE)\n",
    "        inpu = tf.placeholder(tf.int32, [None, max_length_inp])\n",
    "        targe = tf.placeholder(tf.int32, [None, max_length_targ])\n",
    "        predictions = model(inpu,targe[:,:-1])\n",
    "\n",
    "        loss_ = loss_function(targe[:,1:],predictions)\n",
    "        acc_ = acc_function(targe[:,1:],predictions)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss_)\n",
    "        sess = tf.Session()\n",
    "        try: \n",
    "            with tf.device(\"/GPU:0\") as dev:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                start = time.time()\n",
    "                epoch=0\n",
    "                current_time = time.time()\n",
    "                time_consumed=current_time-start\n",
    "                memT,cpuT,gpuT = YN.StartLogger(\"TensorFlow_GPU\",\"Manythings\")\n",
    "                test(sess,input_tensor_train,target_tensor_train,N_BATCH_VAL,BATCH_SIZE,loss_,inpu,targe)\n",
    "                while (time_consumed <= 86400 and epoch <= epochs):\n",
    "                    print(\"Epoch \", epoch) \n",
    "                    train(sess,input_tensor_train,target_tensor_train,N_BATCH,BATCH_SIZE,optimizer,loss_,inpu,targe)\n",
    "                    test(sess,input_tensor_train,target_tensor_train,N_BATCH_VAL,BATCH_SIZE,loss_,inpu,targe)\n",
    "                    acc(acc_,sess,input_tensor_train,target_tensor_train,N_BATCH_VAL,BATCH_SIZE,loss_,inpu,targe)\n",
    "                    epoch += 1\n",
    "                    time_consumed=(time.time())-start\n",
    "                    print(\"Time since beginning: \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "                    sys.stdout.flush()\n",
    "                    #if epoch % 20 == 0:\n",
    "                        #saver = tf.train.Saver()\n",
    "                        #saver.save(sess, './TensorFlow_PTB_LSTM_model_GPU')\n",
    "                YN.EndLogger(memT,cpuT,gpuT)\n",
    "                print(\"\\n\\nTotal Time Consumed: \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "                sys.stdout.flush()\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, './TensorFlow_PTB_LSTM_model_Final_GPU')\n",
    "        except:\n",
    "            print('GPU not available')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "    run(n_epochs,BATCH_SIZE = 128, embedding_dim = 256, units = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ptb(n_epochs):\n",
    "    \n",
    "    class PtbIterator:\n",
    "\n",
    "        def __init__(self, train, batch_size, seq_len, skip_step=5):\n",
    "            self.data = load_ptb(train)\n",
    "            self.batch_size = batch_size\n",
    "            self.seq_len = seq_len\n",
    "            self.skip_step = skip_step\n",
    "            self.reset()\n",
    "\n",
    "        def __iter__(self):\n",
    "            self.reset()\n",
    "            return self\n",
    "\n",
    "        def __next__(self):\n",
    "            x = np.zeros((self.batch_size, self.seq_len), dtype=np.int32)\n",
    "            y = np.zeros((self.batch_size, self.seq_len), dtype=np.int32)\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                if self.cur_idx + self.seq_len >= len(self.data):\n",
    "                    raise StopIteration\n",
    "                x[i, :] = self.data[self.cur_idx:self.cur_idx+self.seq_len]\n",
    "                y[i, :] = self.data[self.cur_idx+1:self.cur_idx+self.seq_len+1]\n",
    "                self.cur_idx += self.skip_step\n",
    "\n",
    "            return x, y.ravel()\n",
    "\n",
    "        def reset(self):\n",
    "            self.cur_idx = 0\n",
    "\n",
    "\n",
    "    # In[15]:\n",
    "\n",
    "\n",
    "    def ptb_lstm(input_var, vocabulary_size, hidden_size, seq_len, num_layers, dropout, batch_size):\n",
    "        l_input = L.InputLayer(shape=(batch_size, seq_len), input_var=input_var)\n",
    "        l_embed = L.EmbeddingLayer(l_input, vocabulary_size, hidden_size,\n",
    "                                   W=init.Uniform(1.0))\n",
    "        l_lstms = []\n",
    "        for i in range(num_layers):\n",
    "            l_lstm = L.LSTMLayer(l_embed if i == 0 else l_lstms[-1], hidden_size,\n",
    "                                 ingate=L.Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal()),\n",
    "                                 forgetgate=L.Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal(), \n",
    "                                                   b=init.Constant(1.0)),\n",
    "                                 cell=L.Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal(), W_cell=None, \n",
    "                                             nonlinearity=lasagne.nonlinearities.tanh), \n",
    "                                 outgate=L.Gate(W_in=init.GlorotUniform(), W_hid=init.Orthogonal()))\n",
    "            l_lstms.append(l_lstm)\n",
    "        l_drop = L.DropoutLayer(l_lstms[-1], dropout)\n",
    "        l_out = L.DenseLayer(l_drop, num_units=vocabulary_size, num_leading_axes=2)\n",
    "        l_out = L.ReshapeLayer(l_out, (l_out.output_shape[0] * l_out.output_shape[1], l_out.output_shape[2]))\n",
    "        l_out = L.NonlinearityLayer(l_out, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "        return l_out\n",
    "\n",
    "\n",
    "    # In[16]:\n",
    "\n",
    "\n",
    "    def acc_function(preds, labels_var):\n",
    "        print(\"preds.shape\",preds.shape)\n",
    "        print(\"labels_var.shape\",labels_var.shape)\n",
    "        return\n",
    "\n",
    "\n",
    "    # In[17]:\n",
    "\n",
    "\n",
    "    def ptb_train(model, data_iter, func, epoch, print_every=500):\n",
    "        losses = []\n",
    "        for i, (inputs, labels) in enumerate(data_iter):\n",
    "            loss = func(inputs, labels)\n",
    "\n",
    "            losses.append(loss)\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('[%d, %5d] train loss: %.3f' % (epoch, i + 1, np.mean(losses)))\n",
    "                losses = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def ptb_test(model, data_iter, func, epoch):\n",
    "        losses = []\n",
    "        t_acc=0\n",
    "        count=0\n",
    "        for inputs, labels in data_iter:\n",
    "            loss, acc = func(inputs, labels)\n",
    "            losses.append(loss)\n",
    "            t_acc += acc\n",
    "            count += 1\n",
    "\n",
    "        print('[%d] test loss: %.3f accuracy: %.3f' % (epoch, np.mean(losses), t_acc/count))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def ptb_test_old(model, data_iter, func, epoch):\n",
    "        losses = []\n",
    "        for inputs, labels in data_iter:\n",
    "            loss,acc = func(inputs, labels)\n",
    "            losses.append(loss)\n",
    "            print(acc)\n",
    "\n",
    "        loss = np.mean(losses)\n",
    "        perplexity = np.exp(loss)\n",
    "        print('[%d] test loss: %.3f perplexity: %.3f' % (epoch, loss, perplexity))\n",
    "\n",
    "\n",
    "    def ptb_run(n_epochs, hidden_size, batch_size, seq_len, dropout, num_layers):\n",
    "        np.random.seed(1)\n",
    "\n",
    "        ptb_vocab = load_ptb_vocab()\n",
    "        vocabulary_size = len(ptb_vocab)\n",
    "\n",
    "        train_iter = PtbIterator(train=True, batch_size=batch_size, seq_len=seq_len)\n",
    "        test_iter = PtbIterator(train=False, batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "        input_var = T.imatrix('inputs')\n",
    "        labels_var = T.ivector('labels')\n",
    "\n",
    "        variables = [input_var, labels_var]\n",
    "        model = ptb_lstm(input_var, vocabulary_size, hidden_size, seq_len, num_layers, dropout, batch_size)\n",
    "\n",
    "        preds = lasagne.layers.get_output(model)\n",
    "        preds_acc = np.argmax(preds,-1) \n",
    "        loss = lasagne.objectives.categorical_crossentropy(preds, labels_var).mean()\n",
    "\n",
    "        test_acc = T.mean(T.eq(T.argmax(preds, axis=1), labels_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "        #acc = acc_function(preds, labels_var)\n",
    "\n",
    "        params = lasagne.layers.get_all_params(model, trainable=True)\n",
    "        updates = lasagne.updates.adadelta(loss, params)\n",
    "        train_func = theano.function(variables, loss, updates=updates)\n",
    "        test_func = theano.function(variables, [loss, test_acc])\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        current_time = time.time()\n",
    "        time_consumed=current_time-start\n",
    "        memT,cpuT,gpuT = YN.StartLogger(\"Chainer_GPU\",\"Manythings\")\n",
    "        epoch=1\n",
    "\n",
    "        while(epoch <= n_epochs and time_consumed <= 86400 ):\n",
    "\n",
    "            ptb_train(model, train_iter, train_func, epoch)\n",
    "            ptb_test(model, test_iter, test_func, epoch)\n",
    "            epoch += 1\n",
    "            time_consumed=(time.time())-start\n",
    "            print(\"Time since beginning \", str(datetime.timedelta(seconds=time_consumed)) )\n",
    "            sys.stdout.flush()\n",
    "            if epoch % 51 == 0:\n",
    "                serializers.save_npz('Chainer_GPU_ManyThings_LSTM_model', model)\n",
    "\n",
    "        end = time.time()\n",
    "        YN.EndLogger(memT,cpuT,gpuT)\n",
    "        print(\"\\n\\nTotal Time Consumed \", str(datetime.timedelta(seconds=time_consumed)))\n",
    "\n",
    "    ptb_run(n_epochs, hidden_size = 200, batch_size = 20, seq_len = 30, dropout = 0.5, num_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-1ba9d021a304>:42: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000024E67B4D808>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000024E67B4D808>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000024E67B4D808>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000024E67B4D808>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000024E5AFBF188>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000024E5AFBF188>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000024E5AFBF188>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000024E5AFBF188>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000024E67B4D188>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000024E67B4D188>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000024E67B4D188>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000024E67B4D188>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From <ipython-input-5-1ba9d021a304>:48: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000024E67B4DC88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000024E67B4DC88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000024E67B4DC88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000024E67B4DC88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "[0,   600] train loss: 6.815\n",
      "[0,  1200] train loss: 6.490\n",
      "[0,  1800] train loss: 6.433\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7314303695e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#run_imdb(n_epochs=50)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#run_manythings(n_epochs=100)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrun_ptb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-1ba9d021a304>\u001b[0m in \u001b[0;36mrun_ptb\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[0mptb_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1ba9d021a304>\u001b[0m in \u001b[0;36mptb_run\u001b[1;34m(n_epochs, hidden_size, batch_size, seq_len, dropout, num_layers)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime_consumed\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m86400\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m                 \u001b[0mptb_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplaceholders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m                 \u001b[0mptb_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplaceholders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mepoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1ba9d021a304>\u001b[0m in \u001b[0;36mptb_train\u001b[1;34m(model, data_iter, loss_op, optimizer_op, placeholders, epoch, print_every)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplaceholders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run_imdb(n_epochs=50)\n",
    "#run_manythings(n_epochs=100)\n",
    "#run_ptb(n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
