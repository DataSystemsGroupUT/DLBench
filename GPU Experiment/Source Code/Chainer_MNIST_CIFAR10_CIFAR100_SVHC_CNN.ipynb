{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import chainer\n",
    "import numpy as np\n",
    "from chainer.backends import cuda\n",
    "from chainer import Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.links import Convolution2D\n",
    "from chainer.training import extensions\n",
    "from chainer.dataset import concat_examples\n",
    "from chainer.backends.cuda import to_cpu\n",
    "import matplotlib.pyplot as plt\n",
    "from chainer.datasets import mnist,svhn,cifar\n",
    "from chainer import Sequential\n",
    "from scipy import io\n",
    "import time\n",
    "import datetime\n",
    "import LoggerYN as YN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    global Dataset    \n",
    "    global pbatchSize\n",
    "    global pnumClasses\n",
    "    global pEpochs\n",
    "    global pLearningRate\n",
    "    global pMomentum\n",
    "    global pWeightDecay\n",
    "    Dataset = dataset\n",
    "    pbatchSize = batchSize\n",
    "    pnumClasses = numClasses\n",
    "    pEpochs = epochs\n",
    "    pLearningRate = learningRate\n",
    "    pMomentum = momentum\n",
    "    pWeightDecay = weightDecay\n",
    "    \n",
    "def loadData(dataset,batchsize):\n",
    "    if(dataset == \"mnist\"):\n",
    "        train, test = mnist.get_mnist(withlabel=True, ndim=1,scale=1.0)\n",
    "        train_iter = iterators.SerialIterator(train, batchsize, shuffle=True)\n",
    "        test_iter = iterators.SerialIterator(test, batchsize,repeat=False, shuffle=False)\n",
    "        return train_iter, test_iter\n",
    "    elif(dataset == \"cifar10\"):\n",
    "        print(\"load cifar10\")\n",
    "        train, test = cifar.get_cifar10(withlabel=True,ndim=1,scale=1.0)\n",
    "        train_iter = iterators.SerialIterator(train, batchsize, shuffle=True)\n",
    "        test_iter = iterators.SerialIterator(test, batchsize,repeat=False, shuffle=False)\n",
    "        return train_iter, test_iter\n",
    "    elif(dataset == \"cifar100\"):\n",
    "        train, test = cifar.get_cifar100(withlabel=True,ndim=1,scale=1.0)\n",
    "        train_iter = iterators.SerialIterator(train, batchsize, shuffle=True)\n",
    "        test_iter = iterators.SerialIterator(test, batchsize,repeat=False, shuffle=False)\n",
    "        return train_iter, test_iter\n",
    "    elif(dataset == \"SVHN\"):\n",
    "        import svhnYN as svv\n",
    "        train, test = svv.get_svhn(withlabel=True,scale=1.0)\n",
    "        train_iter = iterators.SerialIterator(train, batchsize, shuffle=True)\n",
    "        test_iter = iterators.SerialIterator(test, batchsize,repeat=False, shuffle=False)    \n",
    "        return train_iter, test_iter\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "class modelMNIST(Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(modelMNIST, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(1,32,ksize=3,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv2 = L.Convolution2D(32, 64, ksize=3,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.fc1 = L.Linear(None,128)\n",
    "            self.fc2 = L.Linear(128, 10)\n",
    "      \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = x.reshape(-1,1,28,28)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))  \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2))\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = F.flatten(x).reshape(-1,9216)\n",
    "        x= F.relu(self.fc1(x))\n",
    "        x = F.dropout(x,0.5)\n",
    "        if chainer.config.train:\n",
    "            return self.fc2(x)\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "class modelCIFAR10(Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(modelCIFAR10, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(3,32,ksize=3,pad=1,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv2 = L.Convolution2D(32, 32, ksize=3,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv3 = L.Convolution2D(32,64,ksize=3,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv4 = L.Convolution2D(64, 64, ksize=3,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.fc1 = L.Linear(None,512)\n",
    "            self.fc2 = L.Linear(512, 10)\n",
    "      \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = x.reshape(-1,3,32,32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))  \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2))\n",
    "        x = F.dropout(x, 0.25)        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))  \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2))\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = F.flatten(x).reshape(x.shape[0],2304)\n",
    "        x= F.relu(self.fc1(x))\n",
    "        x = F.dropout(x,0.5)\n",
    "        if chainer.config.train:\n",
    "            return self.fc2(x)\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class modelCIFAR100(Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(modelCIFAR100, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(3,128,ksize=3,stride=1,pad=1,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv2 = L.Convolution2D(128, 128, ksize=3,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv3 = L.Convolution2D(128,256,ksize=3,stride=1,pad=1,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv4 = L.Convolution2D(256, 256, ksize=3,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv5 = L.Convolution2D(256,512,ksize=3,stride=1,pad=1,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv6 = L.Convolution2D(512, 512, ksize=3,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.fc1 = L.Linear(2048,1024)\n",
    "            self.fc2 = L.Linear(1024, 100)\n",
    "      \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = x.reshape(-1,3,32,32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))  \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2),stride=2)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))  \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2),stride=2)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))  \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2),stride=4)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        \n",
    "        x = F.flatten(x).reshape(x.shape[0],-1)\n",
    "        x= F.relu(self.fc1(x))\n",
    "        x = F.dropout(x,0.5)\n",
    "        if chainer.config.train:\n",
    "            return self.fc2(x)\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class modelSVHN(Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(modelSVHN, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(None,48,ksize=5,stride=1,pad=2,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv2 = L.Convolution2D(None,64, ksize=5,stride=1,pad=2,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv3 = L.Convolution2D(None,128,ksize=5,stride=1,pad=2,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv4 = L.Convolution2D(None,160, ksize=5,stride=1,pad=2,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv5 = L.Convolution2D(None,192,ksize=5,stride=1,pad=2,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv6 = L.Convolution2D(None,192,ksize=5,stride=1,pad=2,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv7 = L.Convolution2D(None,192,ksize=5,stride=1,pad=2,initialW=chainer.initializers.GlorotUniform())\n",
    "            self.conv8 = L.Convolution2D(None,192, ksize=5,stride=1,pad=2,initialW=chainer.initializers.GlorotUniform())\n",
    "            \n",
    "            self.bN = chainer.links.BatchNormalization(48)\n",
    "            self.bN1 = chainer.links.BatchNormalization(64)\n",
    "            self.bN2 = chainer.links.BatchNormalization(128)\n",
    "            self.bN3 = chainer.links.BatchNormalization(160)\n",
    "            self.bN4 = chainer.links.BatchNormalization(192)\n",
    "            self.bN5 = chainer.links.BatchNormalization(192)\n",
    "            self.bN6 = chainer.links.BatchNormalization(192)\n",
    "            self.bN7 = chainer.links.BatchNormalization(192)\n",
    "            \n",
    "            \n",
    "            self.fc1 = L.Linear(192,3072)\n",
    "            self.fc2 = L.Linear(3072, 10)\n",
    "      \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = x.reshape(-1,3,32,32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.bN(x)\n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2),stride=2)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        \n",
    "        x =self.conv2(x)\n",
    "        x = self.bN1(x)\n",
    "        x = F.relu(x) \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2),stride=2)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        \n",
    "        x =self.conv3(x) \n",
    "        x = self.bN2(x)\n",
    "        x = F.relu(x) \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2),stride=2)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        \n",
    "        x =self.conv4(x) \n",
    "        x = self.bN3(x)\n",
    "        x = F.relu(x)  \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2),stride=2)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        \n",
    "        x =self.conv5(x) \n",
    "        x = self.bN4(x)\n",
    "        x = F.relu(x)  \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2),stride=2)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        \n",
    "        x =self.conv6(x) \n",
    "        x = self.bN5(x)\n",
    "        x = F.relu(x)  \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2),stride=2)\n",
    "        x = F.dropout(x, 0.2)        \n",
    "        \n",
    "        \n",
    "        x =self.conv7(x) \n",
    "        x = self.bN6(x)\n",
    "        x = F.relu(x)  \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2),stride=2)\n",
    "        x = F.dropout(x, 0.2)    \n",
    "        \n",
    "        x =self.conv8(x) \n",
    "        x = self.bN7(x)\n",
    "        x = F.relu(x)  \n",
    "        x = F.max_pooling_2d(x , ksize= (2, 2),stride=2)\n",
    "        x = F.dropout(x, 0.2)    \n",
    "        \n",
    "        \n",
    "        x = F.flatten(x).reshape(x.shape[0],-1)\n",
    "        \n",
    "        x= F.relu(self.fc1(x))\n",
    "        \n",
    "        if chainer.config.train:\n",
    "            return self.fc2(x)\n",
    "        \n",
    "        \n",
    "        x = F.softmax(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "def RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    train_iter, test_iter = loadData(dataset,batchSize)\n",
    "    \n",
    "    model = modelMNIST()\n",
    "\n",
    "    # Choose an optimizer algorithm\n",
    "    optimizer = optimizers.MomentumSGD(lr=learningRate, momentum=momentum)\n",
    "    \n",
    "    # Give the optimizer a reference to the model so that it\n",
    "    # can locate the model's parameters.\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(weightDecay))\n",
    "    \n",
    "    gpu_id = 0  # Set to -1 for CPU, 0  for GPU\n",
    "    if gpu_id >= 0:\n",
    "        model.to_gpu(gpu_id)\n",
    "        \n",
    "    batchId = 0\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Chainer\",\"MNIST\")  \n",
    "    start = time.time()\n",
    "    \n",
    "    train_accuracies = []\n",
    "    while train_iter.epoch < epochs:\n",
    "        \n",
    "        batchId += 1\n",
    "       \n",
    "        # ---------- One iteration of the training loop ----------\n",
    "        train_batch = train_iter.next()\n",
    "        \n",
    "        image_train, target_train = concat_examples(train_batch, gpu_id)\n",
    "        \n",
    "        if ((batchId % batchSize) == 0) or (batchId == 1):\n",
    "            print(\"Epoch: \"+str(train_iter.epoch) , \" Batch (\",batchId,\")\")\n",
    "        # Calculate the prediction of the network\n",
    "        prediction_train = model(image_train)\n",
    "    \n",
    "        # Calculate the loss with softmax_cross_entropy\n",
    "        loss = F.softmax_cross_entropy(prediction_train, target_train)\n",
    "    \n",
    "    \n",
    "         # Calculate the accuracy\n",
    "        accuracyTrain = F.accuracy(prediction_train, target_train)\n",
    "        accuracyTrain.to_cpu()\n",
    "        train_accuracies.append(accuracyTrain.data)\n",
    "        \n",
    "        # Calculate the gradients in the network\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Update all the trainable parameters\n",
    "        optimizer.update()\n",
    "        # --------------------- until here ---------------------\n",
    "    \n",
    "        # Check the validation accuracy of prediction after every epoch\n",
    "        if train_iter.is_new_epoch:  # If this iteration is the final iteration of the current epoch\n",
    "            \n",
    "            batchId = 0\n",
    "            # Display the training loss\n",
    "            print('epoch:{:02d} train_accuracy:{:.04f} train_loss:{:.04f} '.format(train_iter.epoch, np.mean(train_accuracies) ,float(to_cpu(loss.data))), end='')\n",
    "    \n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "            while True:\n",
    "                test_batch = test_iter.next()\n",
    "                image_test, target_test = concat_examples(test_batch, gpu_id)\n",
    "\n",
    "                # Forward the test data\n",
    "                prediction_test = model(image_test)\n",
    "    \n",
    "                # Calculate the loss\n",
    "                loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
    "                test_losses.append(to_cpu(loss_test.data))\n",
    "    \n",
    "                # Calculate the accuracy\n",
    "                accuracy = F.accuracy(prediction_test, target_test)\n",
    "                accuracy.to_cpu()\n",
    "                test_accuracies.append(accuracy.data)\n",
    "    \n",
    "                if test_iter.is_new_epoch:\n",
    "                    test_iter.epoch = 0\n",
    "                    test_iter.current_position = 0\n",
    "                    test_iter.is_new_epoch = False\n",
    "                    test_iter._pushed_position = None\n",
    "                    train_accuracies = []\n",
    "                    break\n",
    "    \n",
    "            print('test_loss:{:.04f} test_accuracy:{:.04f}'.format(np.mean(test_losses), np.mean(test_accuracies)))\n",
    "    end = time.time()\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    train_iter, test_iter = loadData(dataset,batchSize)\n",
    "    \n",
    "    model = modelCIFAR10()\n",
    "\n",
    "    # Choose an optimizer algorithm\n",
    "    optimizer = optimizers.MomentumSGD(lr=learningRate, momentum=momentum)\n",
    "    \n",
    "    # Give the optimizer a reference to the model so that it\n",
    "    # can locate the model's parameters.\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(weightDecay))\n",
    "    gpu_id = 0  # Set to -1 for CPU, 0  for GPU\n",
    "    if gpu_id >= 0:\n",
    "        model.to_gpu(gpu_id)\n",
    "        \n",
    "    batchId = 0\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Chainer\",\"CIFAR10\")  \n",
    "    start = time.time()\n",
    "    train_accuracies = []\n",
    "    while train_iter.epoch < epochs:\n",
    "    \n",
    "        batchId += 1\n",
    "       \n",
    "        # ---------- One iteration of the training loop ----------\n",
    "        train_batch = train_iter.next()\n",
    "        \n",
    "        image_train, target_train = concat_examples(train_batch, gpu_id)\n",
    "        \n",
    "        if ((batchId % batchSize) == 0) or (batchId == 1):\n",
    "            print(\"Epoch: \"+str(train_iter.epoch) , \" Batch (\",batchId,\")\")\n",
    "        # Calculate the prediction of the network\n",
    "        prediction_train = model(image_train)\n",
    "    \n",
    "        # Calculate the loss with softmax_cross_entropy\n",
    "        loss = F.softmax_cross_entropy(prediction_train, target_train)\n",
    "        \n",
    "         # Calculate the accuracy\n",
    "        accuracyTrain = F.accuracy(prediction_train, target_train)\n",
    "        accuracyTrain.to_cpu()\n",
    "        train_accuracies.append(accuracyTrain.data)\n",
    "    \n",
    "        # Calculate the gradients in the network\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Update all the trainable parameters\n",
    "        optimizer.update()\n",
    "        # --------------------- until here ---------------------\n",
    "    \n",
    "        # Check the validation accuracy of prediction after every epoch\n",
    "        if train_iter.is_new_epoch:  # If this iteration is the final iteration of the current epoch\n",
    "            \n",
    "            batchId = 0\n",
    "            # Display the training loss\n",
    "            print('epoch:{:02d} train_accuracy:{:.04f} train_loss:{:.04f} '.format(train_iter.epoch, np.mean(train_accuracies) ,float(to_cpu(loss.data))), end='')\n",
    "    \n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "            while True:\n",
    "                test_batch = test_iter.next()\n",
    "                image_test, target_test = concat_examples(test_batch, gpu_id)\n",
    "\n",
    "                # Forward the test data\n",
    "                prediction_test = model(image_test)\n",
    "    \n",
    "                # Calculate the loss\n",
    "                loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
    "                test_losses.append(to_cpu(loss_test.data))\n",
    "    \n",
    "                # Calculate the accuracy\n",
    "                accuracy = F.accuracy(prediction_test, target_test)\n",
    "                accuracy.to_cpu()\n",
    "                test_accuracies.append(accuracy.data)\n",
    "    \n",
    "                if test_iter.is_new_epoch:\n",
    "                    test_iter.epoch = 0\n",
    "                    test_iter.current_position = 0\n",
    "                    test_iter.is_new_epoch = False\n",
    "                    test_iter._pushed_position = None\n",
    "                    train_accuracies = []\n",
    "                    break\n",
    "    \n",
    "            print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(np.mean(test_losses), np.mean(test_accuracies)))\n",
    "    end = time.time()\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "            \n",
    "def RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    train_iter, test_iter = loadData(dataset,batchSize)\n",
    "    \n",
    "    model = modelCIFAR100()\n",
    "\n",
    "    # Choose an optimizer algorithm\n",
    "    optimizer = optimizers.MomentumSGD(lr=learningRate, momentum=momentum)\n",
    "    \n",
    "    # Give the optimizer a reference to the model so that it\n",
    "    # can locate the model's parameters.\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(weightDecay))\n",
    "    \n",
    "    gpu_id = 0  # Set to -1 for CPU, 0  for GPU\n",
    "    if gpu_id >= 0:\n",
    "        model.to_gpu(gpu_id)\n",
    "        \n",
    "    batchId = 0\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Chainer\",\"CIFAR100\")  \n",
    "    start = time.time()\n",
    "    train_accuracies = []\n",
    "    while train_iter.epoch < epochs:\n",
    "    \n",
    "        batchId += 1\n",
    "       \n",
    "        # ---------- One iteration of the training loop ----------\n",
    "        train_batch = train_iter.next()\n",
    "        \n",
    "        image_train, target_train = concat_examples(train_batch, gpu_id)\n",
    "        \n",
    "        if ((batchId % batchSize) == 0) or (batchId == 1):    \n",
    "            print(\"Epoch: \"+str(train_iter.epoch) , \" Batch (\",batchId,\")\")\n",
    "        # Calculate the prediction of the network\n",
    "        prediction_train = model(image_train)\n",
    "    \n",
    "        # Calculate the loss with softmax_cross_entropy\n",
    "        loss = F.softmax_cross_entropy(prediction_train, target_train)\n",
    "        \n",
    "         # Calculate the accuracy\n",
    "        accuracyTrain = F.accuracy(prediction_train, target_train)\n",
    "        accuracyTrain.to_cpu()\n",
    "        train_accuracies.append(accuracyTrain.data)\n",
    "        \n",
    "        # Calculate the gradients in the network\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Update all the trainable parameters\n",
    "        optimizer.update()\n",
    "        # --------------------- until here ---------------------\n",
    "    \n",
    "        # Check the validation accuracy of prediction after every epoch\n",
    "        if train_iter.is_new_epoch:  # If this iteration is the final iteration of the current epoch\n",
    "            \n",
    "            batchId = 0\n",
    "            # Display the training loss\n",
    "            print('epoch:{:02d} train_accuracy:{:.04f} train_loss:{:.04f} '.format(train_iter.epoch, np.mean(train_accuracies) ,float(to_cpu(loss.data))), end='')\n",
    "    \n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "            while True:\n",
    "                test_batch = test_iter.next()\n",
    "                image_test, target_test = concat_examples(test_batch, gpu_id)\n",
    "\n",
    "                # Forward the test data\n",
    "                prediction_test = model(image_test)\n",
    "    \n",
    "                # Calculate the loss\n",
    "                loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
    "                test_losses.append(to_cpu(loss_test.data))\n",
    "    \n",
    "                # Calculate the accuracy\n",
    "                accuracy = F.accuracy(prediction_test, target_test)\n",
    "                accuracy.to_cpu()\n",
    "                test_accuracies.append(accuracy.data)\n",
    "    \n",
    "                if test_iter.is_new_epoch:\n",
    "                    test_iter.epoch = 0\n",
    "                    test_iter.current_position = 0\n",
    "                    test_iter.is_new_epoch = False\n",
    "                    test_iter._pushed_position = None\n",
    "                    train_accuracies = []\n",
    "                    break\n",
    "    \n",
    "            print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(np.mean(test_losses), np.mean(test_accuracies)))\n",
    "    end = time.time()\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "            \n",
    "def RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    train_iter, test_iter = loadData(dataset,batchSize)\n",
    "    \n",
    "    model = modelSVHN()\n",
    "\n",
    "    # Choose an optimizer algorithm\n",
    "    optimizer = optimizers.MomentumSGD(lr=learningRate, momentum=momentum)\n",
    "    \n",
    "    # Give the optimizer a reference to the model so that it\n",
    "    # can locate the model's parameters.\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(weightDecay))\n",
    "    \n",
    "    gpu_id = 0  # Set to -1 for CPU, 0  for GPU\n",
    "    if gpu_id >= 0:\n",
    "        model.to_gpu(gpu_id)\n",
    "        \n",
    "    batchId = 0\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Chainer\",\"SVHN\")  \n",
    "    start = time.time()\n",
    "    train_accuracies = []\n",
    "    while train_iter.epoch < epochs:\n",
    "    \n",
    "        batchId += 1\n",
    "       \n",
    "        # ---------- One iteration of the training loop ----------\n",
    "        train_batch = train_iter.next()\n",
    "        \n",
    "        image_train, target_train = concat_examples(train_batch, gpu_id)\n",
    "        \n",
    "        if ((batchId % batchSize) == 0) or (batchId == 1):\n",
    "            print(\"Epoch: \"+str(train_iter.epoch) , \" Batch (\",batchId,\")\")\n",
    "        # Calculate the prediction of the network\n",
    "        prediction_train = model(image_train)\n",
    "    \n",
    "        # Calculate the loss with softmax_cross_entropy\n",
    "        loss = F.softmax_cross_entropy(prediction_train, target_train)\n",
    "        \n",
    "         # Calculate the accuracy\n",
    "        accuracyTrain = F.accuracy(prediction_train, target_train)\n",
    "        accuracyTrain.to_cpu()\n",
    "        train_accuracies.append(accuracyTrain.data)\n",
    "        \n",
    "        # Calculate the gradients in the network\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Update all the trainable parameters\n",
    "        optimizer.update()\n",
    "        # --------------------- until here ---------------------\n",
    "    \n",
    "        # Check the validation accuracy of prediction after every epoch\n",
    "        if train_iter.is_new_epoch:  # If this iteration is the final iteration of the current epoch\n",
    "            \n",
    "            batchId = 0\n",
    "            # Display the training loss\n",
    "            print('epoch:{:02d} train_accuracy:{:.04f} train_loss:{:.04f} '.format(train_iter.epoch, np.mean(train_accuracies) ,float(to_cpu(loss.data))), end='')\n",
    "    \n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "            while True:\n",
    "                test_batch = test_iter.next()\n",
    "                image_test, target_test = concat_examples(test_batch, gpu_id)\n",
    "\n",
    "                # Forward the test data\n",
    "                prediction_test = model(image_test)\n",
    "    \n",
    "                # Calculate the loss\n",
    "                loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
    "                test_losses.append(to_cpu(loss_test.data))\n",
    "    \n",
    "                # Calculate the accuracy\n",
    "                accuracy = F.accuracy(prediction_test, target_test)\n",
    "                accuracy.to_cpu()\n",
    "                test_accuracies.append(accuracy.data)\n",
    "    \n",
    "                if test_iter.is_new_epoch:\n",
    "                    test_iter.epoch = 0\n",
    "                    test_iter.current_position = 0\n",
    "                    test_iter.is_new_epoch = False\n",
    "                    test_iter._pushed_position = None\n",
    "                    train_accuracies = []\n",
    "                    break\n",
    "    \n",
    "            print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(np.mean(test_losses), np.mean(test_accuracies)))\n",
    "    end = time.time()\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "    YN.EndLogger(memT,cpuT,gpuT)            \n",
    "\n",
    "def runModel(dataset,batchSize=128,numClasses=10,epochs=12,learningRate=0.01,momentum=0.5,weightDecay=1e-6):\n",
    "    if dataset is \"mnist\":\n",
    "        RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar10\":\n",
    "        RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar100\":\n",
    "        RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"SVHN\":\n",
    "        RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "        #fname = 'SVHN/%s_32x32.mat'\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Choose cifar10 or mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#    runModel(\"SVHN\",epochs=3)\n",
    "    runModel(\"cifar100\",epochs=200)\n",
    "#    runModel(\"cifar10\",epochs=100)\n",
    "#    runModel(\"mnist\",epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
