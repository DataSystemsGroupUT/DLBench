{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import LoggerYN as YN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path):\n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    \n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines]\n",
    "    \n",
    "    return word_pairs\n",
    "\n",
    "    # This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "            \n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    # creating cleaned input, output pairs\n",
    "    pairs = create_dataset(path)\n",
    "\n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "    targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # Spanish sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "    \n",
    "    # English sentences\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.LSTM = nn.LSTM(input_size =embedding_dim, hidden_size = self.enc_units, batch_first=True)\n",
    "        nn.init.xavier_uniform_(self.LSTM.weight_ih_l0)\n",
    "        nn.init.orthogonal_(self.LSTM.weight_hh_l0)\n",
    "        nn.init.constant_(self.LSTM.bias_ih_l0, 0.0)\n",
    "        nn.init.constant_(self.LSTM.bias_ih_l0[self.enc_units:2*self.enc_units], 1.0)\n",
    "        nn.init.constant_(self.LSTM.bias_hh_l0, 0.0)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.LSTM(x) \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return torch.zeros((self.batch_sz,self.enc_units))\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_dim)\n",
    "        self.LSTM = nn.LSTM(input_size = embedding_dim, hidden_size = self.dec_units, batch_first=True)\n",
    "        self.fc = nn.Linear(self.dec_units,vocab_size)\n",
    "        nn.init.xavier_uniform_(self.LSTM.weight_ih_l0)\n",
    "        nn.init.orthogonal_(self.LSTM.weight_hh_l0)\n",
    "        nn.init.constant_(self.LSTM.bias_ih_l0, 0.0)\n",
    "        nn.init.constant_(self.LSTM.bias_ih_l0[self.dec_units:2*self.dec_units], 1.0)\n",
    "        nn.init.constant_(self.LSTM.bias_hh_l0, 0.0)\n",
    "        \n",
    "    def forward(self, x, hidden, enc_output):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.LSTM(x,hidden)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return torch.zeros((self.batch_sz,self.dec_units))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encap(nn.Module):\n",
    "    def __init__(self, encoder,decoder):\n",
    "        super(Encap, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, inp,targ, hidden, BATCH_SIZE,vocab_tar_size):\n",
    "        loss = 0\n",
    "        enc_output, enc_hidden = self.encoder(inp, [hidden,hidden])   \n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = targ[:,:-1]\n",
    "        \n",
    "        predictions, dec_hidden = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "        loss = loss_function(targ[:,1:], predictions,vocab_tar_size)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(path_to_file):\n",
    "    input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file)\n",
    "    # Creating training and validation sets using an 80-20 split\n",
    "    input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2,random_state=42)\n",
    "    vocab_inp_size = len(inp_lang.word2idx)\n",
    "    vocab_tar_size = len(targ_lang.word2idx)\n",
    "    return input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred,vocab):\n",
    "    sfmax = nn.LogSoftmax(dim =1)\n",
    "    cross_ent=F.nll_loss\n",
    "    pred = pred.view(-1,vocab)\n",
    "    pred = sfmax(pred)\n",
    "    real = real.reshape(-1)\n",
    "    loss = cross_ent(pred, real)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,epoch,my_dataloader,hidden,BATCH_SIZE,vocab_tar_size,optimizer):\n",
    "    for (batch, (inp, targ)) in enumerate(my_dataloader):\n",
    "        model.zero_grad()\n",
    "        loss = model(inp,targ,hidden,BATCH_SIZE,vocab_tar_size)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 300 == 0:\n",
    "            print('Batch {} Loss {}'.format(batch,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,val_dataloader,hidden,BATCH_SIZE,vocab_tar_size):\n",
    "    t_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(my_dataloader):\n",
    "        loss = model(inp,targ,hidden,BATCH_SIZE,vocab_tar_size)\n",
    "        t_loss +=loss.data.numpy()\n",
    "    print('Validation Perplexity :{}'.format(np.power(2,t_loss/batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(BATCH_SIZE,  embedding_dim, units, epochs):\n",
    "    path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
    "    path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "    torch.manual_seed(1)\n",
    "    input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val,vocab_inp_size,vocab_tar_size,max_length_inp, max_length_targ = create_db(path_to_file)\n",
    "    # Get parameters\n",
    "    BUFFER_SIZE = len(input_tensor_train)\n",
    "    N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "\n",
    "    train_samples = len(input_tensor_train)\n",
    "    val_samples = len(input_tensor_val)\n",
    "    #data pre-pre processing\n",
    "    input_tensor_train = np.array(input_tensor_train,dtype = 'int')\n",
    "    target_tensor_train = np.array(target_tensor_train,dtype = 'int')\n",
    "    input_tensor_val = np.array(input_tensor_val,dtype = 'int')\n",
    "    target_tensor_val = np.array(target_tensor_val,dtype = 'int') \n",
    "    tensor_x = torch.stack([torch.from_numpy(i).cuda() for i in input_tensor_train]).cuda() \n",
    "    tensor_y = torch.stack([torch.from_numpy(i).cuda() for i in target_tensor_train]).cuda()\n",
    "    val_x = torch.stack([torch.from_numpy(i) for i in input_tensor_val])\n",
    "    val_y = torch.stack([torch.from_numpy(i) for i in target_tensor_val])\n",
    "    my_dataset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "    my_dataloader = utils.DataLoader(my_dataset,batch_size=BATCH_SIZE)\n",
    "    val_dataset = utils.TensorDataset(val_x,val_y) # create your datset\n",
    "    val_dataloader = utils.DataLoader(val_dataset,batch_size=BATCH_SIZE)\n",
    "    #create model\n",
    "    encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "    decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "    model = Encap(encoder,decoder)\n",
    "    model.cuda()\n",
    "    cudnn.benchmark = True\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr=0.0001)  \n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"PyTorch\",\"Manythings\")\n",
    "    start = time.time()\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    test(model,val_dataloader,hidden,BATCH_SIZE,vocab_tar_size)\n",
    "    for epoch in range(epochs):\n",
    "        train(model,epoch,my_dataloader,hidden,BATCH_SIZE,vocab_tar_size,optimizer)\n",
    "        test(model,val_dataloader,hidden,BATCH_SIZE,vocab_tar_size)\n",
    "    end = time.time()\n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "    print(str(datetime.timedelta(seconds=end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(BATCH_SIZE = 128,  embedding_dim = 256, units = 256, epochs = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
