{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "import os\n",
    "import platform\n",
    "import gzip\n",
    "import LoggerYN as YN\n",
    "import scipy.io as sio\n",
    "import utilsYN as uYN\n",
    "import warnings\n",
    "import time\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\", message=\"Reloaded modules: <module_name>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    global Dataset    \n",
    "    global pbatchSize\n",
    "    global pnumClasses\n",
    "    global pEpochs\n",
    "    global pLearningRate\n",
    "    global pMomentum\n",
    "    global pWeightDecay\n",
    "    Dataset = dataset\n",
    "    pbatchSize = batchSize\n",
    "    pnumClasses = numClasses\n",
    "    pEpochs = epochs\n",
    "    pLearningRate = learningRate\n",
    "    pMomentum = momentum\n",
    "    pWeightDecay = weightDecay\n",
    "    \n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def NormalizeData(x_train,x_test):\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "        return x_train, x_test\n",
    "    \n",
    "\n",
    "\n",
    "# CIFAR 10 ###############################################################\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_CIFAR10_data(num_training=50000, num_validation=1000, num_test=10000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = loadDataCIFAR10_temp(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "\n",
    "def get_CIFAR100_data():\n",
    "        # Check if cifar data exists\n",
    "    if not os.path.exists(\"./cifar-100-batches-py\"):\n",
    "        print(\"CIFAR-10 dataset can not be found. Please download the dataset from 'https://www.cs.toronto.edu/~kriz/cifar.html'.\")\n",
    "        return\n",
    "\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    data = loadDataCIFAR100_temp()\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    y_train = data['Y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['Y_test']\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    num_training = 50000\n",
    "    num_test=10000\n",
    "    \n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def loadDataCIFAR10_temp(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)/np.float32(255)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    Xte = Xte / np.float32(255)\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "\n",
    "def loadDataCIFAR100_temp():\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for j in range(1):\n",
    "      d = unpickle('cifar-100-batches-py/train')\n",
    "      x = d['data']\n",
    "      y = d['fine_labels']\n",
    "      xs.append(x)\n",
    "      ys.append(y)\n",
    "    d = unpickle('cifar-100-batches-py/test')\n",
    "    xs.append(d['data'])\n",
    "    ys.append(d['fine_labels'])\n",
    "    x = np.concatenate(xs)/np.float32(255)\n",
    "    y = np.concatenate(ys)\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3))\n",
    "\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "\n",
    "    X_test = x[50000:,:,:,:]\n",
    "    Y_test = y[50000:]\n",
    "\n",
    "    return dict(X_train=X_train,Y_train=Y_train.astype('int32'),X_test = X_test,Y_test = Y_test.astype('int32'),)   \n",
    "\n",
    "\n",
    "\n",
    "def loadDataCIFAR10():\n",
    "    X_train, y_train, X_test, y_test = get_CIFAR10_data()\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def loadDataCIFAR100():\n",
    "    X_train, y_train, X_test, y_test = get_CIFAR100_data()\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def loadDataMNIST():\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        return data / np.float32(255)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "#    print(X_train.shape)\n",
    "    global imgRows\n",
    "    global imgCols\n",
    "    global imgRGB_Dimensions\n",
    "    global inputShape\n",
    "    \n",
    "    imgRGB_Dimensions = X_train.shape[1]\n",
    "    imgRows = X_train.shape[2]\n",
    "    imgCols = X_train.shape[3]\n",
    "    \n",
    "    \n",
    "    inputShape = (imgRows, imgCols, imgRGB_Dimensions)\n",
    "    \n",
    "    X_train = X_train.transpose(0,2,3,1).astype(\"float\")\n",
    "    X_test = X_test.transpose(0,2,3,1).astype(\"float\")\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def loadDataSVHN(fname,extra=False):\n",
    "\n",
    "    def load_mat(fname):\n",
    "        data = sio.loadmat(fname)\n",
    "        X = data['X'].transpose(3, 0, 1, 2)\n",
    "        y = data['y'] % 10  # map label \"10\" --> \"0\"\n",
    "        return X, y\n",
    "\n",
    "    data = uYN.Dataset()\n",
    "    data.classes = np.arange(10)\n",
    "\n",
    "\n",
    "    X, y = load_mat(fname % 'train')\n",
    "    data.train_images = X\n",
    "    data.train_labels = y.reshape(-1)\n",
    "\n",
    "    X, y = load_mat(fname % 'test')\n",
    "    data.test_images = X\n",
    "    data.test_labels = y.reshape(-1)\n",
    "\n",
    "    new_x = data.train_images\n",
    "    new_y = data.train_labels\n",
    "    \n",
    "    if extra:\n",
    "        X, y = load_mat(fname % 'extra')\n",
    "        data.extra_images = X\n",
    "        data.extra_labels = y.reshape(-1)\n",
    "    \n",
    "        # Use extra dataset\n",
    "        new_x = data.extra_images#np.concatenate((data.extra_images, data.train_images), axis=0)\n",
    "        new_y = data.extra_labels#np.concatenate((data.extra_labels, data.train_labels), axis=0)\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test)  = (new_x,new_y),(data.test_images,data.test_labels)\n",
    "    \n",
    "    global imgRows\n",
    "    global imgCols\n",
    "    global imgRGB_Dimensions\n",
    "    global inputShape\n",
    "    \n",
    "    imgRows = x_train.shape[1]\n",
    "    imgCols = x_train.shape[2]\n",
    "\n",
    "    try:\n",
    "        imgRGB_Dimensions = x_train.shape[3]\n",
    "    except Exception:\n",
    "        imgRGB_Dimensions = 1 #For Gray Scale Images\n",
    "\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_test = x_test.reshape(x_test.shape[0], imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train, x_test = NormalizeData(x_train, x_test)\n",
    "    inputShape = (imgRows, imgCols, imgRGB_Dimensions)\n",
    "    \n",
    "    \n",
    "    x_train = x_train.reshape(len(y_train), imgRows, imgCols, imgRGB_Dimensions)\n",
    "    x_test = x_test.reshape(len(y_test), imgRows, imgCols, imgRGB_Dimensions)\n",
    "    \n",
    "    num_training= x_train.shape[0]\n",
    "    mask = range(num_training)\n",
    "    x_train = x_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    \n",
    "    \n",
    "    return x_train,y_train,x_test,y_test    \n",
    "    \n",
    "\n",
    "\n",
    "### Models\n",
    "    \n",
    "class ModelMNIST():\n",
    "    def __init__(self):\n",
    "       \n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "        self.Wconv1 = tf.get_variable(\"Wconv1\", shape=[3, 3, 1, 32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x64 = 50176\n",
    "        self.Wconv2 = tf.get_variable(\"Wconv2\", shape=[3, 3, 32, 64],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv2 = tf.get_variable(\"bconv2\", shape=[64])\n",
    "   \n",
    "        # affine layer with 1024\n",
    "        self.W1 = tf.get_variable(\"W1\", shape=[9216, 128],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b1 = tf.get_variable(\"b1\", shape=[128])\n",
    "        # affine layer with 10\n",
    "        self.W2 = tf.get_variable(\"W2\", shape=[128, 10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b2 = tf.get_variable(\"b2\", shape=[10])        \n",
    "        \n",
    "    def forward(self, X, y, is_training):\n",
    "        conv1 = tf.nn.conv2d(X, self.Wconv1, strides=[1, 1, 1, 1], padding='VALID') + self.bconv1\n",
    "        relu1 = tf.nn.relu(conv1)\n",
    "        \n",
    "        # Conv\n",
    "        conv2 = tf.nn.conv2d(relu1, self.Wconv2, strides=[1, 1, 1, 1], padding='VALID') + self.bconv2\n",
    "        relu2 = tf.nn.relu(conv2)\n",
    "        \n",
    "        maxpool = tf.layers.max_pooling2d(relu2, pool_size=(2,2),strides=2)\n",
    "        drop1 = tf.layers.dropout(inputs=maxpool, training=is_training,rate=0.25)\n",
    "        \n",
    "        maxpool_flat = tf.reshape(drop1,[-1,9216])\n",
    "\n",
    "        \n",
    "        affine1 = tf.matmul(maxpool_flat, self.W1) + self.b1\n",
    "        \n",
    "     \n",
    "        # ReLU Activation Layer\n",
    "        relu2 = tf.nn.relu(affine1)\n",
    "        \n",
    "        # dropout\n",
    "        drop2 = tf.layers.dropout(inputs=relu2, training=is_training, rate = 0.5)\n",
    "        \n",
    "        # Affine layer from 1024 input units to 10 outputs\n",
    "        affine2 = tf.matmul(drop2, self.W2) + self.b2\n",
    "        \n",
    "        self.predict = affine2\n",
    "    \n",
    "        return self.predict\n",
    "    \n",
    "    def run(self, session, loss_val, Xd, yd,Xt,yt,epochs=1, batch_size=64, print_every=100,training=None, plot_losses=False, isSoftMax=False):\n",
    "        # have tensorflow compute accuracy\n",
    "            \n",
    "        correct_prediction = tf.equal(tf.argmax(self.predict,1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # shuffle indicies\n",
    "        train_indicies = np.arange(Xd.shape[0])\n",
    "        np.random.shuffle(train_indicies)\n",
    "\n",
    "        # setting up variables we want to compute (and optimizing)\n",
    "        # if we have a training function, add that to things we compute\n",
    "        variables = [mean_loss, correct_prediction, accuracy]\n",
    "        variables[-1] = training\n",
    "\n",
    "        # counter \n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            # keep track of losses and accuracy\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            # make sure we iterate over the dataset once\n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xd[idx,:],\n",
    "                             y: yd[idx],\n",
    "                             is_training: True }\n",
    "                # get batch size\n",
    "                actual_batch_size = yd[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "\n",
    "                # aggregate performance stats\n",
    "                losses.append(loss*actual_batch_size)\n",
    "                correct += np.sum(corr)\n",
    "\n",
    "                # print every now and then\n",
    "                if  (iter_cnt % print_every) == 0:\n",
    "                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                iter_cnt += 1\n",
    "\n",
    "            test_indicies = np.arange(Xt.shape[0])\n",
    "            np.random.shuffle(test_indicies)    \n",
    "            correctTest = 0\n",
    "            lossesTest = []\n",
    "            correct_prediction_test = tf.equal(tf.argmax(self.predict,1), y)#tf.nn.softmax(self.predict)\n",
    "            accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32))\n",
    "            variables_test = [mean_loss, correct_prediction_test, accuracy_test]\n",
    "            for i in range(int(math.ceil(Xt.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xt.shape[0]\n",
    "                idx = test_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xt[idx,:],\n",
    "                             y: yt[idx],\n",
    "                             is_training: False }\n",
    "                # get batch size\n",
    "                actual_batch_size = yt[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables_test,feed_dict=feed_dict)\n",
    "            \n",
    "\n",
    "                # aggregate performance stats\n",
    "                lossesTest.append(loss*actual_batch_size)\n",
    "                correctTest += np.sum(corr)\n",
    "\n",
    "                \n",
    "                \n",
    "            total_correct = correct/Xd.shape[0]\n",
    "            total_loss = np.sum(losses)/Xd.shape[0]\n",
    "            total_correct_test = correctTest/Xt.shape[0]\n",
    "            total_loss_test = np.sum(lossesTest)/Xt.shape[0]\n",
    "            print((\"Epoch {2}, Train loss: {0:.3g} and Train accuracy of {1:.3g}\"\n",
    "                  + \", Test loss: {3:.3g} and Test accuracy of {4:.3g}\").format(total_loss,total_correct,e+1,total_loss_test,total_correct_test))                \n",
    "                \n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch loss')\n",
    "                plt.show()\n",
    "        return total_loss, total_correct\n",
    "\n",
    "    \n",
    "    \n",
    "class ModelSVHN():\n",
    "    def __init__(self):\n",
    "       \n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "        self.Wconv1 = tf.get_variable(\"Wconv1\", shape=[5, 5, 3, 48],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv1 = tf.get_variable(\"bconv1\", shape=[48])\n",
    "    \n",
    "    \n",
    "        self.Wconv2 = tf.get_variable(\"Wconv2\", shape=[5, 5, 48, 64],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv2 = tf.get_variable(\"bconv2\", shape=[64])\n",
    "        \n",
    "        self.Wconv3 = tf.get_variable(\"Wconv3\", shape=[5, 5, 64, 128],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv3 = tf.get_variable(\"bconv3\", shape=[128])\n",
    "        \n",
    "        self.Wconv4 = tf.get_variable(\"Wconv4\", shape=[5, 5, 128, 160],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv4 = tf.get_variable(\"bconv4\", shape=[160])\n",
    "     \n",
    "        self.Wconv5 = tf.get_variable(\"Wconv5\", shape=[5, 5, 160, 192],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv5 = tf.get_variable(\"bconv5\", shape=[192])\n",
    "    \n",
    "        self.Wconv6 = tf.get_variable(\"Wconv6\", shape=[5, 5, 192, 192],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv6 = tf.get_variable(\"bconv6\", shape=[192])\n",
    "\n",
    "        self.Wconv7 = tf.get_variable(\"Wconv7\", shape=[5, 5, 192, 192],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv7 = tf.get_variable(\"bconv7\", shape=[192])\n",
    "\n",
    "        self.Wconv8 = tf.get_variable(\"Wconv8\", shape=[5, 5, 192, 192],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv8 = tf.get_variable(\"bconv8\", shape=[192])        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # affine layer with 1024\n",
    "        self.W1 = tf.get_variable(\"W1\", shape=[192, 3072],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b1 = tf.get_variable(\"b1\", shape=[3072])\n",
    "        # affine layer with 10\n",
    "        self.W2 = tf.get_variable(\"W2\", shape=[3072, 10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b2 = tf.get_variable(\"b2\", shape=[10])        \n",
    "        \n",
    "    def forward(self, X, y, is_training):\n",
    "\n",
    "        conv1 = tf.nn.conv2d(X, self.Wconv1, strides=[1, 1, 1, 1], padding='SAME') + self.bconv1\n",
    "        relu1 = tf.nn.relu(conv1)\n",
    "        bN1 = tf.layers.batch_normalization(relu1, training=is_training)\n",
    "        maxpool = tf.layers.max_pooling2d(bN1, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop1 = tf.layers.dropout(inputs=maxpool,rate=0.2, training=is_training)\n",
    "\n",
    "        conv2 = tf.nn.conv2d(drop1, self.Wconv2, strides=[1, 1, 1, 1], padding='SAME') + self.bconv2\n",
    "        relu2 = tf.nn.relu(conv2)\n",
    "        bN2 = tf.layers.batch_normalization(relu2, training=is_training)\n",
    "        maxpool2 = tf.layers.max_pooling2d(bN2, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop2 = tf.layers.dropout(inputs=maxpool2,rate=0.2, training=is_training)\n",
    "\n",
    "        conv3 = tf.nn.conv2d(drop2, self.Wconv3, strides=[1, 1, 1, 1], padding='SAME') + self.bconv3\n",
    "        relu3 = tf.nn.relu(conv3)\n",
    "        bN3 = tf.layers.batch_normalization(relu3, training=is_training)\n",
    "        maxpool3 = tf.layers.max_pooling2d(bN3, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop3 = tf.layers.dropout(inputs=maxpool3,rate=0.2, training=is_training)\n",
    "\n",
    "        conv4 = tf.nn.conv2d(drop3, self.Wconv4, strides=[1, 1, 1, 1], padding='SAME') + self.bconv4\n",
    "        relu4 = tf.nn.relu(conv4)\n",
    "        bN4 = tf.layers.batch_normalization(relu4, training=is_training)\n",
    "        maxpool4 = tf.layers.max_pooling2d(bN4, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop4 = tf.layers.dropout(inputs=maxpool4,rate=0.2, training=is_training)\n",
    "\n",
    "        conv5 = tf.nn.conv2d(drop4, self.Wconv5, strides=[1, 1, 1, 1], padding='SAME') + self.bconv5\n",
    "        relu5 = tf.nn.relu(conv5)\n",
    "        bN5 = tf.layers.batch_normalization(relu5, training=is_training)\n",
    "        maxpool5 = tf.layers.max_pooling2d(bN5, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop5 = tf.layers.dropout(inputs=maxpool5,rate=0.2, training=is_training)\n",
    "\n",
    "        conv6 = tf.nn.conv2d(drop5, self.Wconv6, strides=[1, 1, 1, 1], padding='SAME') + self.bconv6\n",
    "        relu6 = tf.nn.relu(conv6)\n",
    "        bN6 = tf.layers.batch_normalization(relu6, training=is_training)\n",
    "        maxpool6 = tf.layers.max_pooling2d(bN6, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop6 = tf.layers.dropout(inputs=maxpool6,rate=0.2, training=is_training)\n",
    "\n",
    "        conv7 = tf.nn.conv2d(drop6, self.Wconv7, strides=[1, 1, 1, 1], padding='SAME') + self.bconv7\n",
    "        relu7 = tf.nn.relu(conv7)\n",
    "        bN7 = tf.layers.batch_normalization(relu7, training=is_training)\n",
    "        maxpool7 = tf.layers.max_pooling2d(bN7, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop7 = tf.layers.dropout(inputs=maxpool7,rate=0.2, training=is_training)\n",
    "\n",
    "        conv8 = tf.nn.conv2d(drop7, self.Wconv8, strides=[1, 1, 1, 1], padding='SAME') + self.bconv8\n",
    "        relu8 = tf.nn.relu(conv8)\n",
    "        bN8 = tf.layers.batch_normalization(relu8, training=is_training)\n",
    "        maxpool8 = tf.layers.max_pooling2d(bN8, pool_size=(2,2),strides=2,padding='SAME')\n",
    "        drop8 = tf.layers.dropout(inputs=maxpool8,rate=0.2, training=is_training)\n",
    "\n",
    "        maxpool_flat = tf.reshape(drop8,[-1,192])\n",
    "\n",
    "        affine1 = tf.matmul(maxpool_flat, self.W1) + self.b1\n",
    "\n",
    "\n",
    "        # ReLU Activation Layer\n",
    "        relu9= tf.nn.relu(affine1)\n",
    "\n",
    "        # dropout\n",
    "        drop9 = tf.layers.dropout(inputs=relu9, training=is_training)\n",
    "\n",
    "        # Affine layer from 3072 input units to 10 outputs\n",
    "        affine2 = tf.matmul(drop9, self.W2) + self.b2\n",
    "\n",
    "        self.predict = affine2\n",
    "\n",
    "        return self.predict\n",
    "\n",
    "    def run(self, session, loss_val, Xd, yd,Xt,yt,epochs=1, batch_size=64, print_every=100,training=None, plot_losses=False, isSoftMax=False):\n",
    "        # have tensorflow compute accuracy\n",
    "            \n",
    "        correct_prediction = tf.equal(tf.argmax(self.predict,1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # shuffle indicies\n",
    "        train_indicies = np.arange(Xd.shape[0])\n",
    "        np.random.shuffle(train_indicies)\n",
    "\n",
    "        # setting up variables we want to compute (and optimizing)\n",
    "        # if we have a training function, add that to things we compute\n",
    "        variables = [mean_loss, correct_prediction, accuracy]\n",
    "        variables[-1] = training\n",
    "\n",
    "        # counter \n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            # keep track of losses and accuracy\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            # make sure we iterate over the dataset once\n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xd[idx,:],\n",
    "                             y: yd[idx],\n",
    "                             is_training: True }\n",
    "                # get batch size\n",
    "                actual_batch_size = yd[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "\n",
    "                # aggregate performance stats\n",
    "                losses.append(loss*actual_batch_size)\n",
    "                correct += np.sum(corr)\n",
    "\n",
    "                # print every now and then\n",
    "                if  (iter_cnt % print_every) == 0:\n",
    "                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                iter_cnt += 1\n",
    "\n",
    "            test_indicies = np.arange(Xt.shape[0])\n",
    "            np.random.shuffle(test_indicies)    \n",
    "            correctTest = 0\n",
    "            lossesTest = []\n",
    "            correct_prediction_test = tf.equal(tf.argmax(self.predict,1), y)#tf.nn.softmax(self.predict)\n",
    "            accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32))\n",
    "            variables_test = [mean_loss, correct_prediction_test, accuracy_test]\n",
    "            for i in range(int(math.ceil(Xt.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xt.shape[0]\n",
    "                idx = test_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xt[idx,:],\n",
    "                             y: yt[idx],\n",
    "                             is_training: False }\n",
    "                # get batch size\n",
    "                actual_batch_size = yt[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables_test,feed_dict=feed_dict)\n",
    "            \n",
    "\n",
    "                # aggregate performance stats\n",
    "                lossesTest.append(loss*actual_batch_size)\n",
    "                correctTest += np.sum(corr)\n",
    "\n",
    "                \n",
    "                \n",
    "            total_correct = correct/Xd.shape[0]\n",
    "            total_loss = np.sum(losses)/Xd.shape[0]\n",
    "            total_correct_test = correctTest/Xt.shape[0]\n",
    "            total_loss_test = np.sum(lossesTest)/Xt.shape[0]\n",
    "            print((\"Epoch {2}, Train loss: {0:.3g} and Train accuracy of {1:.3g}\"\n",
    "                  + \", Test loss: {3:.3g} and Test accuracy of {4:.3g}\").format(total_loss,total_correct,e+1,total_loss_test,total_correct_test))                \n",
    "                \n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch loss')\n",
    "                plt.show()\n",
    "        return total_loss, total_correct\n",
    "\n",
    "\n",
    "\n",
    "class ModelCIFAR10():\n",
    "    def __init__(self):\n",
    "       \n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "        self.Wconv1 = tf.get_variable(\"Wconv1\", shape=[3, 3, 3, 32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x64 = 50176\n",
    "        self.Wconv2 = tf.get_variable(\"Wconv2\", shape=[3, 3, 32, 32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv2 = tf.get_variable(\"bconv2\", shape=[32])\n",
    "        \n",
    "        self.Wconv3 = tf.get_variable(\"Wconv3\", shape=[3, 3, 32, 64],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv3 = tf.get_variable(\"bconv3\", shape=[64])\n",
    "        \n",
    "        self.Wconv4 = tf.get_variable(\"Wconv4\", shape=[3, 3, 64, 64],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv4 = tf.get_variable(\"bconv4\", shape=[64])\n",
    "        \n",
    "        # affine layer with 1024\n",
    "        self.W1 = tf.get_variable(\"W1\", shape=[2304, 512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b1 = tf.get_variable(\"b1\", shape=[512])\n",
    "        # affine layer with 10\n",
    "        self.W2 = tf.get_variable(\"W2\", shape=[512, 10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b2 = tf.get_variable(\"b2\", shape=[10])        \n",
    "        \n",
    "    def forward(self, X, y, is_training):\n",
    "        \n",
    "        conv1 = tf.nn.conv2d(X, self.Wconv1, strides=[1, 1, 1, 1], padding='SAME') + self.bconv1\n",
    "        relu1 = tf.nn.relu(conv1)\n",
    "        # Conv\n",
    "        conv2 = tf.nn.conv2d(relu1, self.Wconv2, strides=[1, 1, 1, 1], padding='VALID') + self.bconv2\n",
    "        relu2 = tf.nn.relu(conv2)\n",
    "        \n",
    "        maxpool = tf.layers.max_pooling2d(relu2, pool_size=(2,2),strides=2)\n",
    "        \n",
    "        \n",
    "        drop1 = tf.layers.dropout(inputs=maxpool,rate=0.25, training=is_training)\n",
    "        \n",
    "        conv3 = tf.nn.conv2d(drop1, self.Wconv3, strides=[1, 1, 1, 1], padding='SAME') + self.bconv3\n",
    "        relu3 = tf.nn.relu(conv3)\n",
    "        \n",
    "        conv4 = tf.nn.conv2d(relu3, self.Wconv4, strides=[1, 1, 1, 1], padding='VALID') + self.bconv4\n",
    "        relu4 = tf.nn.relu(conv4)\n",
    "        \n",
    "        maxpool2 = tf.layers.max_pooling2d(relu4, pool_size=(2,2),strides=2,padding=\"VALID\")\n",
    "        \n",
    "        \n",
    "        drop2 = tf.layers.dropout(inputs=maxpool2,rate=0.25, training=is_training)\n",
    "        \n",
    "        maxpool_flat = tf.reshape(drop2,[-1,2304])\n",
    "        affine1 = tf.matmul(maxpool_flat, self.W1) + self.b1\n",
    "        \n",
    "     \n",
    "        # ReLU Activation Layer\n",
    "        relu2 = tf.nn.relu(affine1)\n",
    "        \n",
    "        # dropout\n",
    "        drop1 = tf.layers.dropout(inputs=relu2,rate=0.5, training=is_training)\n",
    "        \n",
    "        # Affine layer from 1024 input units to 10 outputs\n",
    "        affine2 = tf.matmul(drop1, self.W2) + self.b2\n",
    "           \n",
    "        self.predict = affine2\n",
    "        return self.predict\n",
    "    \n",
    "    def run(self, session, loss_val, Xd, yd,Xt,yt,epochs=1, batch_size=64, print_every=100,training=None, plot_losses=False, isSoftMax=False):\n",
    "        # have tensorflow compute accuracy\n",
    "            \n",
    "        correct_prediction = tf.equal(tf.argmax(self.predict,1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # shuffle indicies\n",
    "        train_indicies = np.arange(Xd.shape[0])\n",
    "        np.random.shuffle(train_indicies)\n",
    "\n",
    "        # setting up variables we want to compute (and optimizing)\n",
    "        # if we have a training function, add that to things we compute\n",
    "        variables = [mean_loss, correct_prediction, accuracy]\n",
    "        variables[-1] = training\n",
    "\n",
    "        # counter \n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            # keep track of losses and accuracy\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            # make sure we iterate over the dataset once\n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xd[idx,:],\n",
    "                             y: yd[idx],\n",
    "                             is_training: True }\n",
    "                # get batch size\n",
    "                actual_batch_size = yd[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "\n",
    "                # aggregate performance stats\n",
    "                losses.append(loss*actual_batch_size)\n",
    "                correct += np.sum(corr)\n",
    "\n",
    "                # print every now and then\n",
    "                if  (iter_cnt % print_every) == 0:\n",
    "                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                iter_cnt += 1\n",
    "\n",
    "            test_indicies = np.arange(Xt.shape[0])\n",
    "            np.random.shuffle(test_indicies)    \n",
    "            correctTest = 0\n",
    "            lossesTest = []\n",
    "            correct_prediction_test = tf.equal(tf.argmax(self.predict,1), y)#tf.nn.softmax(self.predict)\n",
    "            accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32))\n",
    "            variables_test = [mean_loss, correct_prediction_test, accuracy_test]\n",
    "            for i in range(int(math.ceil(Xt.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xt.shape[0]\n",
    "                idx = test_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xt[idx,:],\n",
    "                             y: yt[idx],\n",
    "                             is_training: False }\n",
    "                # get batch size\n",
    "                actual_batch_size = yt[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables_test,feed_dict=feed_dict)\n",
    "            \n",
    "\n",
    "                # aggregate performance stats\n",
    "                lossesTest.append(loss*actual_batch_size)\n",
    "                correctTest += np.sum(corr)\n",
    "\n",
    "                \n",
    "                \n",
    "            total_correct = correct/Xd.shape[0]\n",
    "            total_loss = np.sum(losses)/Xd.shape[0]\n",
    "            total_correct_test = correctTest/Xt.shape[0]\n",
    "            total_loss_test = np.sum(lossesTest)/Xt.shape[0]\n",
    "            print((\"Epoch {2}, Train loss: {0:.3g} and Train accuracy of {1:.3g}\"\n",
    "                  + \", Test loss: {3:.3g} and Test accuracy of {4:.3g}\").format(total_loss,total_correct,e+1,total_loss_test,total_correct_test))                \n",
    "                \n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch loss')\n",
    "                plt.show()\n",
    "        return total_loss, total_correct\n",
    "\n",
    "class ModelCIFAR100():\n",
    "    def __init__(self):\n",
    "       \n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "        self.Wconv1 = tf.get_variable(\"Wconv1\", shape=[3, 3, 3, 128],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv1 = tf.get_variable(\"bconv1\", shape=[128])\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x64 = 50176\n",
    "        self.Wconv2 = tf.get_variable(\"Wconv2\", shape=[3, 3, 128, 128],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv2 = tf.get_variable(\"bconv2\", shape=[128])\n",
    "        \n",
    "        self.Wconv3 = tf.get_variable(\"Wconv3\", shape=[3, 3, 128, 256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv3 = tf.get_variable(\"bconv3\", shape=[256])\n",
    "        \n",
    "        self.Wconv4 = tf.get_variable(\"Wconv4\", shape=[3, 3, 256, 256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv4 = tf.get_variable(\"bconv4\", shape=[256])\n",
    "        \n",
    "        \n",
    "        self.Wconv5 = tf.get_variable(\"Wconv5\", shape=[3, 3, 256, 512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv5 = tf.get_variable(\"bconv5\", shape=[512])\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.Wconv6 = tf.get_variable(\"Wconv6\", shape=[3, 3, 512, 512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.bconv6 = tf.get_variable(\"bconv6\", shape=[512])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # affine layer with 1024\n",
    "        self.W1 = tf.get_variable(\"W1\", shape=[2048, 1024],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b1 = tf.get_variable(\"b1\", shape=[1024])\n",
    "        # affine layer with 10\n",
    "        self.W2 = tf.get_variable(\"W2\", shape=[1024, 100],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.b2 = tf.get_variable(\"b2\", shape=[100])        \n",
    "        \n",
    "    def forward(self, X, y, is_training):\n",
    "\n",
    "        conv1 = tf.nn.conv2d(X, self.Wconv1, strides=[1, 1, 1, 1], padding='SAME') + self.bconv1\n",
    "        relu1 = tf.nn.relu(conv1)\n",
    "        print(relu1.shape)\n",
    "        # Conv\n",
    "        conv2 = tf.nn.conv2d(relu1, self.Wconv2, strides=[1, 1, 1, 1], padding='VALID') + self.bconv2\n",
    "        relu2 = tf.nn.relu(conv2)\n",
    "        print(relu2.shape)\n",
    "        \n",
    "        maxpool = tf.layers.max_pooling2d(relu2, pool_size=(2,2),strides=2)\n",
    "        drop1 = tf.layers.dropout(inputs=maxpool, training=is_training,rate=0.1)\n",
    "        print(maxpool.shape)\n",
    "        \n",
    "        conv3 = tf.nn.conv2d(drop1, self.Wconv3, strides=[1, 1, 1, 1], padding='SAME') + self.bconv3\n",
    "        relu3 = tf.nn.relu(conv3)\n",
    "        print(relu3.shape)\n",
    "        \n",
    "        conv4 = tf.nn.conv2d(relu3, self.Wconv4, strides=[1, 1, 1, 1], padding='VALID') + self.bconv4\n",
    "        relu4 = tf.nn.relu(conv4)\n",
    "        print(relu4.shape)\n",
    "        \n",
    "        maxpool2 = tf.layers.max_pooling2d(relu4, pool_size=(2,2),strides=2)\n",
    "        drop2 = tf.layers.dropout(inputs=maxpool2, training=is_training,rate=0.25)\n",
    "        print(maxpool2.shape)\n",
    "        \n",
    "        \n",
    "        conv5 = tf.nn.conv2d(drop2, self.Wconv5, strides=[1, 1, 1, 1], padding='SAME') + self.bconv5\n",
    "        relu5 = tf.nn.relu(conv5)\n",
    "        print(relu5.shape)\n",
    "        \n",
    "        conv6 = tf.nn.conv2d(relu5, self.Wconv6, strides=[1, 1, 1, 1], padding='VALID') + self.bconv6\n",
    "        relu6 = tf.nn.relu(conv6)\n",
    "        print(relu6.shape)\n",
    "        \n",
    "        \n",
    "        maxpool3 = tf.layers.max_pooling2d(relu6, pool_size=(2,2),strides=2)\n",
    "        drop3 = tf.layers.dropout(inputs=maxpool3, training=is_training,rate=0.5)\n",
    "        print(maxpool3.shape)\n",
    "        \n",
    "        \n",
    "        maxpool_flat = tf.reshape(drop3,[-1,2048])\n",
    "\n",
    "        affine1 = tf.matmul(maxpool_flat, self.W1) + self.b1\n",
    "        \n",
    "     \n",
    "        # ReLU Activation Layer\n",
    "        relu7 = tf.nn.relu(affine1)\n",
    "        \n",
    "        # dropout\n",
    "        drop4 = tf.layers.dropout(inputs=relu7, training=is_training,rate=0.5)\n",
    "        \n",
    "        # Affine layer from 1024 input units to 10 outputs\n",
    "        affine2 = tf.matmul(drop4, self.W2) + self.b2\n",
    "        \n",
    "   \n",
    "        \n",
    "        self.predict = affine2#tf.layers.batch_normalization(inputs=affine2, center=True, scale=True, training=is_training)\n",
    "        \n",
    "        return self.predict\n",
    "    \n",
    "    def run(self, session, loss_val, Xd, yd,Xt,yt,epochs=1, batch_size=64, print_every=100,training=None, plot_losses=False, isSoftMax=False):\n",
    "        # have tensorflow compute accuracy\n",
    "            \n",
    "        correct_prediction = tf.equal(tf.argmax(self.predict,1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # shuffle indicies\n",
    "        train_indicies = np.arange(Xd.shape[0])\n",
    "        np.random.shuffle(train_indicies)\n",
    "\n",
    "        # setting up variables we want to compute (and optimizing)\n",
    "        # if we have a training function, add that to things we compute\n",
    "        variables = [mean_loss, correct_prediction, accuracy]\n",
    "        variables[-1] = training\n",
    "\n",
    "        # counter \n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            # keep track of losses and accuracy\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            # make sure we iterate over the dataset once\n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xd[idx,:],\n",
    "                             y: yd[idx],\n",
    "                             is_training: True }\n",
    "                # get batch size\n",
    "                actual_batch_size = yd[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "\n",
    "                # aggregate performance stats\n",
    "                losses.append(loss*actual_batch_size)\n",
    "                correct += np.sum(corr)\n",
    "\n",
    "                # print every now and then\n",
    "                if  (iter_cnt % print_every) == 0:\n",
    "                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                iter_cnt += 1\n",
    "\n",
    "            test_indicies = np.arange(Xt.shape[0])\n",
    "            np.random.shuffle(test_indicies)    \n",
    "            correctTest = 0\n",
    "            lossesTest = []\n",
    "            correct_prediction_test = tf.equal(tf.argmax(self.predict,1), y)#tf.nn.softmax(self.predict)\n",
    "            accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32))\n",
    "            variables_test = [mean_loss, correct_prediction_test, accuracy_test]\n",
    "            for i in range(int(math.ceil(Xt.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xt.shape[0]\n",
    "                idx = test_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xt[idx,:],\n",
    "                             y: yt[idx],\n",
    "                             is_training: False }\n",
    "                # get batch size\n",
    "                actual_batch_size = yt[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables_test,feed_dict=feed_dict)\n",
    "            \n",
    "\n",
    "                # aggregate performance stats\n",
    "                lossesTest.append(loss*actual_batch_size)\n",
    "                correctTest += np.sum(corr)\n",
    "\n",
    "                \n",
    "                \n",
    "            total_correct = correct/Xd.shape[0]\n",
    "            total_loss = np.sum(losses)/Xd.shape[0]\n",
    "            total_correct_test = correctTest/Xt.shape[0]\n",
    "            total_loss_test = np.sum(lossesTest)/Xt.shape[0]\n",
    "            print((\"Epoch {2}, Train loss: {0:.3g} and Train accuracy of {1:.3g}\"\n",
    "                  + \", Test loss: {3:.3g} and Test accuracy of {4:.3g}\").format(total_loss,total_correct,e+1,total_loss_test,total_correct_test))                \n",
    "                \n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch loss')\n",
    "                plt.show()\n",
    "        return total_loss, total_correct\n",
    "    \n",
    "\n",
    "def RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = loadDataMNIST()\n",
    "    tf.reset_default_graph()\n",
    "    global X\n",
    "    global y\n",
    "    global mean_loss\n",
    "    global is_training\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    net = ModelMNIST()\n",
    "    net.forward(X,y,is_training)\n",
    "    \n",
    "    \n",
    "    # Annealing the learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Feel free to play with this cell\n",
    "    mean_loss = None\n",
    "    optimizer = None\n",
    "    \n",
    "    # define our loss\n",
    "    cross_entr_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=net.predict)\n",
    "    l2_loss = weightDecay * tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()])\n",
    "    mean_loss = tf.reduce_mean(cross_entr_loss + l2_loss)\n",
    "    \n",
    "    # define our optimizer\n",
    "    optimizer = tf.train.MomentumOptimizer(learningRate,momentum=momentum)\n",
    "    \n",
    "    \n",
    "    # batch normalization in tensorflow requires this extra dependency\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_step = optimizer.minimize(mean_loss, global_step=global_step)\n",
    "        \n",
    "        \n",
    "    # train with 10 epochs\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Tensorflow\",\"MNIST\")    \n",
    "    start = time.time()\n",
    "    try:\n",
    "        with tf.device(\"/GPU:0\") as dev:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Training')\n",
    "            net.run(sess, mean_loss, X_train, y_train,X_test, y_test, epochs, batchSize, batchSize, train_step, False)\n",
    "            \n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    YN.EndLogger(memT,cpuT,gpuT)   \n",
    "\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    X_train,y_train ,X_test,y_test = loadDataCIFAR10()\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    global X\n",
    "    global y\n",
    "    global mean_loss\n",
    "    global is_training\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    net = ModelCIFAR10()\n",
    "    net.forward(X,y,is_training)\n",
    "    \n",
    "    \n",
    "    # Annealing the learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Feel free to play with this cell\n",
    "    mean_loss = None\n",
    "    optimizer = None\n",
    "    \n",
    "    # define our loss\n",
    "    cross_entr_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=net.predict)\n",
    "    l2_loss = weightDecay * tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()])\n",
    "    mean_loss = tf.reduce_mean(cross_entr_loss + l2_loss)\n",
    "    \n",
    "    # define our optimizer\n",
    "    optimizer = tf.train.MomentumOptimizer(learningRate,momentum=momentum)\n",
    "    \n",
    "    \n",
    "    # batch normalization in tensorflow requires this extra dependency\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_step = optimizer.minimize(mean_loss, global_step=global_step)\n",
    "        \n",
    "        \n",
    "    # train with 10 epochs\n",
    "    sess = tf.Session()\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Tensorflow\",\"CIFAR10\")\n",
    "    start = time.time()    \n",
    "    try:\n",
    "        with tf.device(\"/GPU:0\") as dev:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Training')\n",
    "            net.run(sess, mean_loss, X_train, y_train,X_test, y_test, epochs, batchSize, batchSize, train_step, False)\n",
    "            \n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")\n",
    "    end = time.time()\n",
    "    \n",
    "    YN.EndLogger(memT,cpuT,gpuT)   \n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "def RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    X_train,y_train ,X_test,y_test =  loadDataCIFAR100()\n",
    "    print(\"####################\")\n",
    "    tf.reset_default_graph()\n",
    "    global X\n",
    "    global y\n",
    "    global mean_loss\n",
    "    global is_training\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    net = ModelCIFAR100()\n",
    "    net.forward(X,y,is_training)\n",
    "    \n",
    "    \n",
    "    # Annealing the learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Feel free to play with this cell\n",
    "    mean_loss = None\n",
    "    optimizer = None\n",
    "    \n",
    "    # define our loss\n",
    "    cross_entr_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=net.predict)\n",
    "    l2_loss = weightDecay * tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()])\n",
    "    mean_loss = tf.reduce_mean(cross_entr_loss + l2_loss)\n",
    "    \n",
    "    # define our optimizer\n",
    "    optimizer = tf.train.MomentumOptimizer(learningRate,momentum=momentum)\n",
    "    \n",
    "    \n",
    "    # batch normalization in tensorflow requires this extra dependency\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_step = optimizer.minimize(mean_loss, global_step=global_step)\n",
    "        \n",
    "        \n",
    "    # train with 100 epochs\n",
    "    sess = tf.Session()\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Tensorflow\",\"CIFAR100\")\n",
    "    start = time.time()    \n",
    "    try:\n",
    "        with tf.device(\"/GPU:0\") as dev:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Training')\n",
    "            net.run(sess, mean_loss, X_train, y_train,X_test, y_test, epochs, batchSize, batchSize, train_step, False)\n",
    "            \n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")\n",
    "    end = time.time()\n",
    "    YN.EndLogger(memT,cpuT,gpuT)   \n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "\n",
    "def RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay,fname):\n",
    "    \n",
    "    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    \n",
    "    X_train,y_train,X_test,y_test = loadDataSVHN(fname,True)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    global X\n",
    "    global y\n",
    "    global mean_loss\n",
    "    global is_training\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    net = ModelSVHN()\n",
    "    net.forward(X,y,is_training)\n",
    "    \n",
    "    \n",
    "    # Annealing the learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Feel free to play with this cell\n",
    "    mean_loss = None\n",
    "    optimizer = None\n",
    "    \n",
    "    # define our loss\n",
    "    cross_entr_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=net.predict)\n",
    "    l2_loss = weightDecay * tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()])\n",
    "    mean_loss = tf.reduce_mean(cross_entr_loss + l2_loss)\n",
    "    \n",
    "    # define our optimizer\n",
    "    optimizer = tf.train.MomentumOptimizer(learningRate,momentum=momentum)\n",
    "    \n",
    "    \n",
    "    # batch normalization in tensorflow requires this extra dependency\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_step = optimizer.minimize(mean_loss, global_step=global_step)\n",
    "        \n",
    "        \n",
    "    # train with 10 epochs\n",
    "    sess = tf.Session()\n",
    "    memT,cpuT,gpuT = YN.StartLogger(\"Tensorflow\",\"SVHN\")\n",
    "    start = time.time()    \n",
    "    try:\n",
    "        with tf.device(\"/GPU:0\") as dev:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('Training')\n",
    "            net.run(sess, mean_loss, X_train, y_train,X_test, y_test, epochs, batchSize, batchSize, train_step, False)\n",
    "            \n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")\n",
    "    end = time.time()\n",
    "    \n",
    "    YN.EndLogger(memT,cpuT,gpuT)\n",
    "    print(str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "def runModel(dataset,batchSize=128,numClasses=10,epochs=12,learningRate=0.01,momentum=0.5,weightDecay=1e-6):\n",
    "    if dataset is \"mnist\":\n",
    "        RunMNIST(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar10\":\n",
    "        RunCIFAR10(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"cifar100\":\n",
    "        RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)\n",
    "    elif dataset is \"SVHN\":\n",
    "        fname = './%s_32x32.mat'\n",
    "        RunSVHN(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay,fname) \n",
    "    else:\n",
    "        print(\"Choose cifar10 or mnist\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#    runModel(\"mnist\",epochs=15)\n",
    "#    runModel(\"cifar10\",epochs=100)\n",
    "#   runModel(\"SVHN\",epochs=100)\n",
    "   runModel(\"cifar100\",epochs=200)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
